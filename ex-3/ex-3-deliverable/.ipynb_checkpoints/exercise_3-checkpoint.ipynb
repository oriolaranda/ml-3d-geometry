{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Exercise 3: Shape Reconstruction\n",
    "\n",
    "**Submission Deadline**: 07.12.2022, 23:55\n",
    "\n",
    "We will take a look at two major approaches for 3D shape reconstruction in this last exercise.\n",
    "\n",
    "Note that training reconstruction methods generally takes relatively long, even for simple shape completion. Training the generalization will take a few hours. **Thus, please make sure to start training well before the submission deadline.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0. Running this notebook\n",
    "We recommend running this notebook on a CUDA compatible local gpu. You can also run training on cpu, it will just take longer.\n",
    "\n",
    "You have three options for running this exercise on a GPU, choose one of them and start the exercise below in section \"Imports\":\n",
    "1. Locally on your own GPU\n",
    "2. On our dedicated compute cluster\n",
    "3. On Google Colab\n",
    "\n",
    "We describe every option in more detail below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### (a) Local Execution\n",
    "\n",
    "If you run this notebook locally, you have to first install the python dependiencies again. They are the same as for exercise 1 so you can re-use the environment you used last time. If you use [poetry](https://python-poetry.org), you can also simply re-install everything (`poetry install`) and then run this notebook via `poetry run jupyter notebook`.\n",
    "\n",
    "In case you are working with a RTX 3000-series GPU, you need to install a patched version of pytorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu113\n",
      "Requirement already satisfied: torch in /home/oriol/.cache/pypoetry/virtualenvs/machine_learning_for_3d_geometry-_2t5S_p_-py3.10/lib/python3.10/site-packages (1.13.0)\n",
      "Requirement already satisfied: torchvision in /home/oriol/.cache/pypoetry/virtualenvs/machine_learning_for_3d_geometry-_2t5S_p_-py3.10/lib/python3.10/site-packages (0.14.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/oriol/.cache/pypoetry/virtualenvs/machine_learning_for_3d_geometry-_2t5S_p_-py3.10/lib/python3.10/site-packages (from torch) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/oriol/.cache/pypoetry/virtualenvs/machine_learning_for_3d_geometry-_2t5S_p_-py3.10/lib/python3.10/site-packages (from torch) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/oriol/.cache/pypoetry/virtualenvs/machine_learning_for_3d_geometry-_2t5S_p_-py3.10/lib/python3.10/site-packages (from torch) (8.5.0.96)\n",
      "Requirement already satisfied: typing-extensions in /home/oriol/.cache/pypoetry/virtualenvs/machine_learning_for_3d_geometry-_2t5S_p_-py3.10/lib/python3.10/site-packages (from torch) (4.4.0)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/oriol/.cache/pypoetry/virtualenvs/machine_learning_for_3d_geometry-_2t5S_p_-py3.10/lib/python3.10/site-packages (from torch) (11.7.99)\n",
      "Requirement already satisfied: wheel in /home/oriol/.cache/pypoetry/virtualenvs/machine_learning_for_3d_geometry-_2t5S_p_-py3.10/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (0.38.4)\n",
      "Requirement already satisfied: setuptools in /home/oriol/.cache/pypoetry/virtualenvs/machine_learning_for_3d_geometry-_2t5S_p_-py3.10/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (65.6.3)\n",
      "Requirement already satisfied: numpy in /home/oriol/.cache/pypoetry/virtualenvs/machine_learning_for_3d_geometry-_2t5S_p_-py3.10/lib/python3.10/site-packages (from torchvision) (1.23.5)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/oriol/.cache/pypoetry/virtualenvs/machine_learning_for_3d_geometry-_2t5S_p_-py3.10/lib/python3.10/site-packages (from torchvision) (7.2.0)\n",
      "Requirement already satisfied: requests in /home/oriol/.cache/pypoetry/virtualenvs/machine_learning_for_3d_geometry-_2t5S_p_-py3.10/lib/python3.10/site-packages (from torchvision) (2.28.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/oriol/.cache/pypoetry/virtualenvs/machine_learning_for_3d_geometry-_2t5S_p_-py3.10/lib/python3.10/site-packages (from requests->torchvision) (1.26.13)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/oriol/.cache/pypoetry/virtualenvs/machine_learning_for_3d_geometry-_2t5S_p_-py3.10/lib/python3.10/site-packages (from requests->torchvision) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/oriol/.cache/pypoetry/virtualenvs/machine_learning_for_3d_geometry-_2t5S_p_-py3.10/lib/python3.10/site-packages (from requests->torchvision) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/oriol/.cache/pypoetry/virtualenvs/machine_learning_for_3d_geometry-_2t5S_p_-py3.10/lib/python3.10/site-packages (from requests->torchvision) (2022.9.24)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m22.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision --extra-index-url https://download.pytorch.org/whl/cu113"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Compute Cluster\n",
    "\n",
    "We provide access to a small compute cluster for the exercises and projects, consisting of a login node and 4 compute nodes with one dedicated RTX 3090 GPU each.\n",
    "Please send us a short email with your name and preferred username so we can add you as a user.\n",
    "\n",
    "We uploaded a PDF to Moodle with detailed information on how to access and use the cluster.\n",
    "\n",
    "Since the cluster contains RTX 3000-series GPUs, you will need to install a patched version of pytorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision --extra-index-url https://download.pytorch.org/whl/cu113"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) Google Colab\n",
    "\n",
    "If you don't have access to a GPU and don't want to use our cluster, you can also use Google Colab. However, we experienced the issue that inline visualization of shapes or inline images didn't work on colab, so just keep that in mind.\n",
    "What you can also do is only train networks on colab, download the checkpoint, and visualize inference locally.\n",
    "\n",
    "In case you're using Google Colab, you can upload the exercise folder (containing `exercise_2.ipynb`, directory `exercise_2` and the file `requirements.txt`) as `3d-machine-learning` to google drive (make sure you don't upload extracted datasets files).\n",
    "Additionally you'd need to open the notebook `exercise_2.ipynb` in Colab using `File > Open Notebook > Upload`.\n",
    "\n",
    "Next you'll need to run these two cells for setting up the environment. Before you do that make sure your instance has a GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "\n",
    "# We assume you uploaded the exercise folder in root Google Drive folder\n",
    "\n",
    "!cp -r /content/drive/MyDrive/3d-machine-learning 3d-machine-learning/\n",
    "os.chdir('/content/3d-machine-learning/')\n",
    "print('Installing requirements')\n",
    "!pip install -r requirements.txt\n",
    "\n",
    "# Make sure you restart runtime when directed by Colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this cell after restarting your colab runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "os.chdir('/content/3d-machine-learning/')\n",
    "sys.path.insert(1, \"/content/3d-machine-learning/\")\n",
    "print('CUDA availability:', torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Imports\n",
    "\n",
    "The following imports should work regardless of whether you are using Colab or local execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import k3d\n",
    "import trimesh\n",
    "import torch\n",
    "import skimage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the next cell to test whether a GPU was detected by pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Shape Reconstruction from 3D SDF grids with 3D-EPN\n",
    "\n",
    "In the first part of this exercise, we will take a look at shape complation using [3D-EPN](https://arxiv.org/abs/1612.00101). This approach was also introduced in the lecture.\n",
    "\n",
    "The visualization below shows an overview of the method: From an incomplete shape observation (which you would get when scanning an object with a depth sensor for example), we use a 3D encoder-predictor network that first encodes the incomplete shapes into a common latent space using several 3D convolution layers and then decodes them again using multiple 3D transpose convolutions.\n",
    "\n",
    "This way, we get from a 32^3 SDF voxel grid to a 32^3 DF (unsigned) voxel grid that represents the completed shape. We only focus on this part here; in the original implementation, this 32^3 completed prediction would then be further improved (in an offline step after inference) by sampling parts from a shape database to get the final resolution to 128^3.\n",
    "\n",
    "<img src=\"exercise_3/images/3depn_teaser.png\" alt=\"3D-EPN Teaser\" style=\"width: 800px;\"/>\n",
    "\n",
    "The next steps will follow the structure we established in exercise 2: Taking a look at the dataset structure and downloading the data; then, implementing dataset, model, and training loop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Downloading the data\n",
    "We will use the original dataset used in the official implementation. It consists of SDF and DF grids (representing incomplete input data and complete target data) with a resolution of 32^3 each. Each input-target pair is generated from a ShapeNet shape.\n",
    "\n",
    "The incomplete SDF data are generated by sampling virtual camera trajectories around every object. Each trajectory is assigned an ID which is part of the file names (see below). The camera views for each trajectory are combined into a common SDF grid by volumetric fusion. It is easy to generate an SDF here since we know both camera location and object surface: Everything between camera and surface is known free space and outside the object, leading to a positive SDF sign. Everything behind the surface has a negative sign. For the complete shapes, however, deciding whether a voxel in the DF grid is inside or outside an object is not a trivial problem. This is why we use unsigned distance fields as target and prediction representation instead. This still encodes the distance to the closest surface but does not contain explicit information about the inside/outside location.\n",
    "\n",
    "In terms of dataset layout, we follow the ShapeNet directory structure as seen in the last exercise:\n",
    "Each folder in the `exercise_3/data/shapenet_dim32_sdf` and `exercise_3/data/shapenet_dim32_df` directories contains one shape category represented by a number, e.g. `02691156`.\n",
    "We provide the mapping between these numbers and the corresponding names in `exercise_3/data/shape_info.json`. Each of these shape category folders contains lots of shapes in sdf or df format. In addition to that, every shape now also contains multiple trajectories: 0 to 7, encoded as `__0__` to `__7__`. These 8 files are just different input representations, meaning they vary in the level of completeness and location of missing parts; they all map to the `.df` file with corresponding shape ID and `__0__` at the end.\n",
    "\n",
    "```\n",
    "# contents of exercise_2/data/shapenet_dim32_sdf\n",
    "02691156/                                           # Shape category folder with all its shapes\n",
    "    ├── 10155655850468db78d106ce0a280f87__0__.sdf   # Trajectory 0 for a shape of the category\n",
    "    ├── 10155655850468db78d106ce0a280f87__1__.sdf   # Trajectory 1 for the same shape\n",
    "    ├── :                                      \n",
    "    ├── 10155655850468db78d106ce0a280f87__7__.sdf   # Trajectory 7 for the same shape\n",
    "    ├── 10155655850468db78d106ce0a280f87__0__.sdf   # Trajectory 0 for another shape\n",
    "    ├── :                                           # And so on ...\n",
    "02933112/                                           # Another shape category folder\n",
    "02958343/                                           # In total you should have 8 shape category folders\n",
    ":\n",
    "\n",
    "# contents of exercise_2/data/shapenet_dim32_df\n",
    "02691156/                                           # Shape category folder with all its shapes\n",
    "    ├── 10155655850468db78d106ce0a280f87__0__.df    # A single shape of the category\n",
    "    ├── 1021a0914a7207aff927ed529ad90a11__0__.df    # Another shape of the category\n",
    "    ├── :                                           # And so on ...\n",
    "02933112/                                           # Another shape category folder\n",
    "02958343/                                           # In total you should have 55 shape category folders\n",
    ":\n",
    "```\n",
    "\n",
    "Download and extract the data with the code cell below.\n",
    "\n",
    "**Note**: If you are training on Google Colab and are running out of disk space, you can do the following:\n",
    "- Only download the zip files below without extracting them (comment out all lines after `print('Extracting ...')`)\n",
    "- Change `from exercise_3.data.shapenet import ShapeNet` to `from exercise_3.data.shapenet_zip import ShapeNet`\n",
    "- Implement your dataset in `shapenet_zip.py`. This implementation extracts the data on-the-fly without taking up any additional disk space. Your training will therefore run a bit slower.\n",
    "- Make sure you uncomment the lines setting the worker_init_fn in `train_3depn.py` (marked with TODOs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading ...\n",
      "--2022-11-29 15:54:39--  http://kaldir.vc.in.tum.de/adai/CNNComplete/shapenet_dim32_sdf.zip\n",
      "Resolving kaldir.vc.in.tum.de (kaldir.vc.in.tum.de)... 131.159.98.128\n",
      "Connecting to kaldir.vc.in.tum.de (kaldir.vc.in.tum.de)|131.159.98.128|:80... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: https://kaldir.vc.in.tum.de:443/adai/CNNComplete/shapenet_dim32_sdf.zip [following]\n",
      "--2022-11-29 15:54:39--  https://kaldir.vc.in.tum.de/adai/CNNComplete/shapenet_dim32_sdf.zip\n",
      "Connecting to kaldir.vc.in.tum.de (kaldir.vc.in.tum.de)|131.159.98.128|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 11741813909 (11G) [application/zip]\n",
      "Saving to: ‘exercise_3/data/shapenet_dim32_sdf.zip’\n",
      "\n",
      "shapenet_dim32_sdf. 100%[===================>]  10,93G  21,3MB/s    in 9m 43s  \n",
      "\n",
      "2022-11-29 16:04:22 (19,2 MB/s) - ‘exercise_3/data/shapenet_dim32_sdf.zip’ saved [11741813909/11741813909]\n",
      "\n",
      "--2022-11-29 16:04:22--  http://kaldir.vc.in.tum.de/adai/CNNComplete/shapenet_dim32_df.zip\n",
      "Resolving kaldir.vc.in.tum.de (kaldir.vc.in.tum.de)... 131.159.98.128\n",
      "Connecting to kaldir.vc.in.tum.de (kaldir.vc.in.tum.de)|131.159.98.128|:80... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: https://kaldir.vc.in.tum.de:443/adai/CNNComplete/shapenet_dim32_df.zip [following]\n",
      "--2022-11-29 16:04:22--  https://kaldir.vc.in.tum.de/adai/CNNComplete/shapenet_dim32_df.zip\n",
      "Connecting to kaldir.vc.in.tum.de (kaldir.vc.in.tum.de)|131.159.98.128|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4054495236 (3,8G) [application/zip]\n",
      "Saving to: ‘exercise_3/data/shapenet_dim32_df.zip’\n",
      "\n",
      "shapenet_dim32_df.z 100%[===================>]   3,78G  18,5MB/s    in 3m 29s  \n",
      "\n",
      "2022-11-29 16:07:51 (18,5 MB/s) - ‘exercise_3/data/shapenet_dim32_df.zip’ saved [4054495236/4054495236]\n",
      "\n",
      "Extracting ...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "print('Downloading ...')\n",
    "# File sizes: 11GB for shapenet_dim32_sdf.zip (incomplete scans), 4GB for shapenet_dim32_df.zip (target shapes)\n",
    "!wget http://kaldir.vc.in.tum.de/adai/CNNComplete/shapenet_dim32_sdf.zip -P exercise_3/data\n",
    "!wget http://kaldir.vc.in.tum.de/adai/CNNComplete/shapenet_dim32_df.zip -P exercise_3/data\n",
    "print('Extracting ...')\n",
    "!unzip -q exercise_3/data/shapenet_dim32_sdf.zip -d exercise_3/data\n",
    "!unzip -q exercise_3/data/shapenet_dim32_df.zip -d exercise_3/data\n",
    "!rm exercise_3/data/shapenet_dim32_sdf.zip\n",
    "!rm exercise_3/data/shapenet_dim32_df.zip\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Dataset\n",
    "\n",
    "The dataset implementation follows the same general structure as in exercise 2. We prepared an initial implementation already in `exercise_3/data/shapenet.py`; your task is to resolve all TODOs there.\n",
    "\n",
    "The data for SDFs and DFs in `.sdf`/`.df` files are stored in binary form as follows:\n",
    "```\n",
    "dimX    #uint64 \n",
    "dimY    #uint64 \n",
    "dimZ    #uint64 \n",
    "data    #(dimX*dimY*dimZ) floats for sdf/df values\n",
    "```\n",
    "The SDF values stored per-voxel represent the distance to the closest surface *in voxels*.\n",
    "\n",
    "You have to take care of three important steps before returning the SDF and DF for the corresponding `index` in `__getitem__`:\n",
    "1. **Truncation**: 3D-EPN uses a truncated SDF which means that for each voxel, the distance to the closest surface will be clamped to a max absolute value. This is helpful since we do not care about longer distances (Marching Cubes only cares about distances close to the surface). It allows us to focus our predictions on the voxels near the surface. We use a `truncation_distance` of 3 (voxels) which means we expect to get an SDF with values between -3 and 3 as input to the model.\n",
    "2. **Separation** of distances and sign: 3D-EPN uses as input a 2x32x32x32 SDF grid, with absolute distance values of the SDF in channel 0 and the signs (-1 or 1) in channel 1.\n",
    "3. **Log** scaling: We scale targets and prediction with a log operation to further guide predictions to focus on the surface voxels. Therefore, you should return target DFs as `log(df + 1)`.\n",
    "\n",
    "**Hint**: An easy way to load the data from `.sdf` and `.df` files is to use `np.fromfile`. First, load the dimensions, then the data, then reshape everything into the shape you loaded in the beginning. Make sure you get the datatypes and byte offsets right! If you are using the zip version of the dataset as explained above, you should use `np.frombuffer` instead of `np.fromfile` to load from the `data`-buffer. The syntax is identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train set: 153540\n",
      "Length of val set: 32304\n",
      "Length of overfit set: 64\n"
     ]
    }
   ],
   "source": [
    "from exercise_3.data.shapenet import ShapeNet\n",
    "\n",
    "# Create a dataset with train split\n",
    "train_dataset = ShapeNet('train')\n",
    "val_dataset = ShapeNet('val')\n",
    "overfit_dataset = ShapeNet('overfit')\n",
    "\n",
    "# Get length, which is a call to __len__ function\n",
    "print(f'Length of train set: {len(train_dataset)}')  # expected output: 153540\n",
    "# Get length, which is a call to __len__ function\n",
    "print(f'Length of val set: {len(val_dataset)}')  # expected output: 32304\n",
    "# Get length, which is a call to __len__ function\n",
    "print(f'Length of overfit set: {len(overfit_dataset)}')  # expected output: 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: 03001627/798a46965d9e0edfcea003eff0268278__3__-03001627/798a46965d9e0edfcea003eff0268278__0__\n",
      "Input SDF: (2, 32, 32, 32)\n",
      "Target DF: (32, 32, 32)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0949396660cf4154ba63d77fa1835e79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize some shapes\n",
    "from exercise_3.util.visualization import visualize_mesh\n",
    "from skimage.measure import marching_cubes\n",
    "\n",
    "train_sample = train_dataset[1]\n",
    "print(f'Name: {train_sample[\"name\"]}')  # expected output: 03001627/798a46965d9e0edfcea003eff0268278__3__-03001627/798a46965d9e0edfcea003eff0268278__0__\n",
    "print(f'Input SDF: {train_sample[\"input_sdf\"].shape}')  # expected output: (2, 32, 32, 32)\n",
    "print(f'Target DF: {train_sample[\"target_df\"].shape}')  # expected output: (32, 32, 32)\n",
    "\n",
    "input_mesh = marching_cubes(train_sample['input_sdf'][0], level=1)\n",
    "visualize_mesh(input_mesh[0], input_mesh[1], flip_axes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: 04379243/a1be21c9a71d133dc5beea20858a99d5__5__-04379243/a1be21c9a71d133dc5beea20858a99d5__0__\n",
      "Input SDF: (2, 32, 32, 32)\n",
      "Target DF: (32, 32, 32)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "591df9ea75214b2694969fbc1beec1c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_sample = train_dataset[223]\n",
    "print(f'Name: {train_sample[\"name\"]}')  # expected output: 04379243/a1be21c9a71d133dc5beea20858a99d5__5__-04379243/a1be21c9a71d133dc5beea20858a99d5__0__\n",
    "print(f'Input SDF: {train_sample[\"input_sdf\"].shape}')  # expected output: (2, 32, 32, 32)\n",
    "print(f'Target DF: {train_sample[\"target_df\"].shape}')  # expected output: (32, 32, 32)\n",
    "\n",
    "input_mesh = marching_cubes(train_sample['input_sdf'][0], level=1)\n",
    "visualize_mesh(input_mesh[0], input_mesh[1], flip_axes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: 03636649/3889631e42a84b0f51f77a6d7299806__2__-03636649/3889631e42a84b0f51f77a6d7299806__0__\n",
      "Input SDF: (2, 32, 32, 32)\n",
      "Target DF: (32, 32, 32)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1ef99097917467b88e4537ce83bdbc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_sample = train_dataset[95]\n",
    "print(f'Name: {train_sample[\"name\"]}')  # expected output: 03636649/3889631e42a84b0f51f77a6d7299806__2__-03636649/3889631e42a84b0f51f77a6d7299806__0__\n",
    "print(f'Input SDF: {train_sample[\"input_sdf\"].shape}')  # expected output: (2, 32, 32, 32)\n",
    "print(f'Target DF: {train_sample[\"target_df\"].shape}')  # expected output: (32, 32, 32)\n",
    "\n",
    "input_mesh = marching_cubes(train_sample['input_sdf'][0], level=1)\n",
    "visualize_mesh(input_mesh[0], input_mesh[1], flip_axes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) Model\n",
    "\n",
    "The model architecture of 3D-EPN is visualized below:\n",
    "\n",
    "<img src=\"exercise_3/images/3depn.png\" alt=\"3D-EPN Architecture\" style=\"width: 800px;\"/>\n",
    "\n",
    "For this exercise, we simplify the model by omitting the classification part - this will not have a big impact since most of the shape completion performance comes from the 3D encoder-decoder unet.\n",
    "\n",
    "The model consists of three parts: The encoder, the bottleneck, and the decoder. Encoder and decoder are constructed with the same architecture, just mirrored.\n",
    "\n",
    "The details of each part are:\n",
    "- **Encoder**: 4 layers, each one containing a 3D convolution (with kernel size 4, as seen in the visualization), a 3D batch norm (except the very first layer), and a leaky ReLU with a negative slope of 0.2. Our goal is to reduce the spatial dimension from 32x32x32 to 1x1x1 and to get the feature dimension from 2 (absolute values and sign) to `num_features * 8`. We do this by using a stride of 2 and padding of 1 for all convolutions except for the last one where we use a stride of 1 and no padding. The feature channels are increased from 2 to `num_features` in the first layer and then doubled with every subsequent layer.\n",
    "- **Decoder**: Same architecture as encoder, just mirrored: Going from `num_features * 8 * 2` (the 2 will be explained later) to 1 (the DF values). The spatial dimensions go from 1x1x1 to 32x32x32. Each layer use a 3D Transpose convolution now, together with 3D batch norm and ReLU (no leaky ReLUs anymore). Note that the last layer uses neither Batch Norms nor a ReLU since we do not want to constrain the range of possible values for the prediction.\n",
    "- **Bottleneck**: This is realized with 2 fully connected layers, each one going from a vector of size 640 (which is `num_features * 8`) to a vector of size 640. Each such layer is followed by a ReLU activation.\n",
    "\n",
    "Some minor details:\n",
    "- **Skip connections** allow the decoder to use information from the encoder and also improve gradient flow. We use it here to connect the output of encoder layer 1 to decoder layer 4, the output of encoder layer 2 to decoder layer 3, and so on. This means that the input to a decoder layer is the concatenation of the previous decoder output with the corresponding encoder output, along the feature dimension. Hence, the number of input features for each decoder layer are twice those of the encoder layers, as mentioned above.\n",
    "- **Log scaling**: You also need to scale the final outputs of the network logarithmically: `out = log(abs(out) + 1)`. This is the same transformation you applied to the target shapes in the dataloader before and ensures that prediction and target volumes are comparable.\n",
    "\n",
    "With this in mind, implement the network architecture and `forward()` function in `exercise_3/model/threedepn.py`. You can check your architecture with the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   | Name         | Type            | Params  \n",
      "----------------------------------------------------\n",
      "0  | conv1        | Conv3d          | 10320   \n",
      "1  | conv2        | Conv3d          | 819360  \n",
      "2  | enc_bn2      | BatchNorm3d     | 320     \n",
      "3  | conv3        | Conv3d          | 3277120 \n",
      "4  | enc_bn3      | BatchNorm3d     | 640     \n",
      "5  | conv4        | Conv3d          | 13107840\n",
      "6  | enc_bn4      | BatchNorm3d     | 1280    \n",
      "7  | leaky_relu   | LeakyReLU       | 0       \n",
      "8  | fcn1         | Linear          | 410240  \n",
      "9  | fcn2         | Linear          | 410240  \n",
      "10 | bottleneck   | Sequential      | 820480  \n",
      "11 | bottleneck.1 | ReLU            | 0       \n",
      "12 | bottleneck.3 | ReLU            | 0       \n",
      "13 | deconv1      | ConvTranspose3d | 26214720\n",
      "14 | dec_bn1      | BatchNorm3d     | 640     \n",
      "15 | deconv2      | ConvTranspose3d | 6553760 \n",
      "16 | dec_bn2      | BatchNorm3d     | 320     \n",
      "17 | deconv3      | ConvTranspose3d | 1638480 \n",
      "18 | dec_bn3      | BatchNorm3d     | 160     \n",
      "19 | deconv4      | ConvTranspose3d | 10241   \n",
      "20 | relu         | ReLU            | 0       \n",
      "21 | TOTAL        | ThreeDEPN       | 52455681\n",
      "Output tensor shape:  torch.Size([4, 32, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "from exercise_3.model.threedepn import ThreeDEPN\n",
    "from exercise_3.util.model import summarize_model\n",
    "\n",
    "threedepn = ThreeDEPN()\n",
    "print(summarize_model(threedepn))  # Expected: Rows 0-34 and TOTAL = 52455681\n",
    "\n",
    "sdf = torch.randn(4, 1, 32, 32, 32) * 2. - 1.\n",
    "input_tensor = torch.cat([torch.abs(sdf), torch.sign(sdf)], dim=1)\n",
    "predictions = threedepn(input_tensor)\n",
    "\n",
    "print('Output tensor shape: ', predictions.shape)  # Expected: torch.Size([4, 32, 32, 32])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (d) Training script and overfitting to a single shape reconstruction\n",
    "\n",
    "You can now go to the train script in `exercise_3/training/train_3depn.py` and fill in the missing pieces as you did for exercise 2. Then, verify that your training work by overfitting to a few samples below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "[004/00001] train_loss: 0.087482\n",
      "[009/00001] train_loss: 0.023595\n",
      "[012/00000] val_loss: 0.548603 | best_loss_val: 0.548603\n",
      "[014/00001] train_loss: 0.014106\n",
      "[019/00001] train_loss: 0.009392\n",
      "[024/00001] train_loss: 0.007127\n",
      "[024/00001] val_loss: 0.223460 | best_loss_val: 0.223460\n",
      "[029/00001] train_loss: 0.006056\n",
      "[034/00001] train_loss: 0.005249\n",
      "[037/00000] val_loss: 0.182895 | best_loss_val: 0.182895\n",
      "[039/00001] train_loss: 0.004624\n",
      "[044/00001] train_loss: 0.004199\n",
      "[049/00001] train_loss: 0.003924\n",
      "[049/00001] val_loss: 0.161660 | best_loss_val: 0.161660\n",
      "[054/00001] train_loss: 0.003683\n",
      "[059/00001] train_loss: 0.003470\n",
      "[062/00000] val_loss: 0.148215 | best_loss_val: 0.148215\n",
      "[064/00001] train_loss: 0.003292\n",
      "[069/00001] train_loss: 0.003196\n",
      "[074/00001] train_loss: 0.003113\n",
      "[074/00001] val_loss: 0.140536 | best_loss_val: 0.140536\n",
      "[079/00001] train_loss: 0.003006\n",
      "[084/00001] train_loss: 0.002933\n",
      "[087/00000] val_loss: 0.135461 | best_loss_val: 0.135461\n",
      "[089/00001] train_loss: 0.002871\n",
      "[094/00001] train_loss: 0.002834\n",
      "[099/00001] train_loss: 0.002778\n",
      "[099/00001] val_loss: 0.131962 | best_loss_val: 0.131962\n",
      "[104/00001] train_loss: 0.002731\n",
      "[109/00001] train_loss: 0.002722\n",
      "[112/00000] val_loss: 0.130033 | best_loss_val: 0.130033\n",
      "[114/00001] train_loss: 0.002701\n",
      "[119/00001] train_loss: 0.002659\n",
      "[124/00001] train_loss: 0.002643\n",
      "[124/00001] val_loss: 0.128595 | best_loss_val: 0.128595\n",
      "[129/00001] train_loss: 0.002655\n",
      "[134/00001] train_loss: 0.002633\n",
      "[137/00000] val_loss: 0.127625 | best_loss_val: 0.127625\n",
      "[139/00001] train_loss: 0.002609\n",
      "[144/00001] train_loss: 0.002596\n",
      "[149/00001] train_loss: 0.002588\n",
      "[149/00001] val_loss: 0.127060 | best_loss_val: 0.127060\n",
      "[154/00001] train_loss: 0.002590\n",
      "[159/00001] train_loss: 0.002594\n",
      "[162/00000] val_loss: 0.126582 | best_loss_val: 0.126582\n",
      "[164/00001] train_loss: 0.002574\n",
      "[169/00001] train_loss: 0.002589\n",
      "[174/00001] train_loss: 0.002573\n",
      "[174/00001] val_loss: 0.126317 | best_loss_val: 0.126317\n",
      "[179/00001] train_loss: 0.002570\n",
      "[184/00001] train_loss: 0.002579\n",
      "[187/00000] val_loss: 0.126117 | best_loss_val: 0.126117\n",
      "[189/00001] train_loss: 0.002560\n",
      "[194/00001] train_loss: 0.002562\n",
      "[199/00001] train_loss: 0.002548\n",
      "[199/00001] val_loss: 0.126015 | best_loss_val: 0.126015\n",
      "[204/00001] train_loss: 0.002552\n",
      "[209/00001] train_loss: 0.002544\n",
      "[212/00000] val_loss: 0.125910 | best_loss_val: 0.125910\n",
      "[214/00001] train_loss: 0.002567\n",
      "[219/00001] train_loss: 0.002546\n",
      "[224/00001] train_loss: 0.002543\n",
      "[224/00001] val_loss: 0.125889 | best_loss_val: 0.125889\n",
      "[229/00001] train_loss: 0.002557\n",
      "[234/00001] train_loss: 0.002544\n",
      "[237/00000] val_loss: 0.125857 | best_loss_val: 0.125857\n",
      "[239/00001] train_loss: 0.002545\n",
      "[244/00001] train_loss: 0.002554\n",
      "[249/00001] train_loss: 0.002574\n",
      "[249/00001] val_loss: 0.125798 | best_loss_val: 0.125798\n"
     ]
    }
   ],
   "source": [
    "from exercise_3.training import train_3depn\n",
    "config = {\n",
    "    'experiment_name': '3_1_3depn_overfitting',\n",
    "    'device': 'cuda:0',  # change this to cpu if you do not have a GPU\n",
    "    'is_overfit': True,\n",
    "    'batch_size': 32,\n",
    "    'resume_ckpt': None,\n",
    "    'learning_rate': 0.001,\n",
    "    'max_epochs': 250,\n",
    "    'print_every_n': 10,\n",
    "    'validate_every_n': 25,\n",
    "}\n",
    "train_3depn.main(config)  # should be able to get <0.0025 train_loss and <0.13 val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (e) Training over the entire training set\n",
    "If the overfitting works, we can go ahead with training on the entire dataset.\n",
    "\n",
    "**Note**: As is the case with most reconstruction networks and considering the size of the model (> 50M parameters), this training will take a few hours on a GPU. *Please make sure to start training early enough before the submission deadline.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "[000/00049] train_loss: 0.034083\n",
      "[000/00099] train_loss: 0.013804\n",
      "[000/00149] train_loss: 0.011750\n",
      "[000/00199] train_loss: 0.010041\n",
      "[000/00249] train_loss: 0.009256\n",
      "[000/00299] train_loss: 0.008867\n",
      "[000/00349] train_loss: 0.008533\n",
      "[000/00399] train_loss: 0.008060\n",
      "[000/00449] train_loss: 0.007990\n",
      "[000/00499] train_loss: 0.007227\n",
      "[000/00549] train_loss: 0.007631\n",
      "[000/00599] train_loss: 0.007385\n",
      "[000/00649] train_loss: 0.007276\n",
      "[000/00699] train_loss: 0.006797\n",
      "[000/00749] train_loss: 0.007098\n",
      "[000/00799] train_loss: 0.006921\n",
      "[000/00849] train_loss: 0.006802\n",
      "[000/00899] train_loss: 0.006602\n",
      "[000/00949] train_loss: 0.006846\n",
      "[000/00999] train_loss: 0.006873\n",
      "[000/00999] val_loss: 0.161904 | best_loss_val: 0.161904\n",
      "[000/01049] train_loss: 0.006218\n",
      "[000/01099] train_loss: 0.006852\n",
      "[000/01149] train_loss: 0.006200\n",
      "[000/01199] train_loss: 0.006148\n",
      "[000/01249] train_loss: 0.005730\n",
      "[000/01299] train_loss: 0.006069\n",
      "[000/01349] train_loss: 0.005720\n",
      "[000/01399] train_loss: 0.005952\n",
      "[000/01449] train_loss: 0.006028\n",
      "[000/01499] train_loss: 0.006079\n",
      "[000/01549] train_loss: 0.005968\n",
      "[000/01599] train_loss: 0.005742\n",
      "[000/01649] train_loss: 0.005659\n",
      "[000/01699] train_loss: 0.005833\n",
      "[000/01749] train_loss: 0.006018\n",
      "[000/01799] train_loss: 0.005448\n",
      "[000/01849] train_loss: 0.005685\n",
      "[000/01899] train_loss: 0.005588\n",
      "[000/01949] train_loss: 0.005498\n",
      "[000/01999] train_loss: 0.005600\n",
      "[000/01999] val_loss: 0.213476 | best_loss_val: 0.161904\n",
      "[000/02049] train_loss: 0.005267\n",
      "[000/02099] train_loss: 0.005128\n",
      "[000/02149] train_loss: 0.005491\n",
      "[000/02199] train_loss: 0.005133\n",
      "[000/02249] train_loss: 0.005211\n",
      "[000/02299] train_loss: 0.005375\n",
      "[000/02349] train_loss: 0.005108\n",
      "[000/02399] train_loss: 0.005047\n",
      "[000/02449] train_loss: 0.005344\n",
      "[000/02499] train_loss: 0.005281\n",
      "[000/02549] train_loss: 0.004960\n",
      "[000/02599] train_loss: 0.004994\n",
      "[000/02649] train_loss: 0.004879\n",
      "[000/02699] train_loss: 0.004715\n",
      "[000/02749] train_loss: 0.004950\n",
      "[000/02799] train_loss: 0.004794\n",
      "[000/02849] train_loss: 0.005063\n",
      "[000/02899] train_loss: 0.005040\n",
      "[000/02949] train_loss: 0.005157\n",
      "[000/02999] train_loss: 0.005130\n",
      "[000/02999] val_loss: 0.244565 | best_loss_val: 0.161904\n",
      "[000/03049] train_loss: 0.005177\n",
      "[000/03099] train_loss: 0.004781\n",
      "[000/03149] train_loss: 0.004805\n",
      "[000/03199] train_loss: 0.005142\n",
      "[000/03249] train_loss: 0.004688\n",
      "[000/03299] train_loss: 0.005297\n",
      "[000/03349] train_loss: 0.004713\n",
      "[000/03399] train_loss: 0.004871\n",
      "[000/03449] train_loss: 0.004532\n",
      "[000/03499] train_loss: 0.004916\n",
      "[000/03549] train_loss: 0.004490\n",
      "[000/03599] train_loss: 0.004927\n",
      "[000/03649] train_loss: 0.004824\n",
      "[000/03699] train_loss: 0.004894\n",
      "[000/03749] train_loss: 0.004650\n",
      "[000/03799] train_loss: 0.004487\n",
      "[000/03849] train_loss: 0.004562\n",
      "[000/03899] train_loss: 0.004907\n",
      "[000/03949] train_loss: 0.004654\n",
      "[000/03999] train_loss: 0.004519\n",
      "[000/03999] val_loss: 0.143128 | best_loss_val: 0.143128\n",
      "[000/04049] train_loss: 0.004397\n",
      "[000/04099] train_loss: 0.004734\n",
      "[000/04149] train_loss: 0.004611\n",
      "[000/04199] train_loss: 0.004474\n",
      "[000/04249] train_loss: 0.004433\n",
      "[000/04299] train_loss: 0.004576\n",
      "[000/04349] train_loss: 0.004446\n",
      "[000/04399] train_loss: 0.004571\n",
      "[000/04449] train_loss: 0.004485\n",
      "[000/04499] train_loss: 0.004264\n",
      "[000/04549] train_loss: 0.004374\n",
      "[000/04599] train_loss: 0.004325\n",
      "[000/04649] train_loss: 0.004683\n",
      "[000/04699] train_loss: 0.004267\n",
      "[000/04749] train_loss: 0.004471\n",
      "[001/00000] train_loss: 0.004415\n",
      "[001/00050] train_loss: 0.004263\n",
      "[001/00100] train_loss: 0.004207\n",
      "[001/00150] train_loss: 0.004713\n",
      "[001/00200] train_loss: 0.004118\n",
      "[001/00200] val_loss: 0.114089 | best_loss_val: 0.114089\n",
      "[001/00250] train_loss: 0.004366\n",
      "[001/00300] train_loss: 0.004331\n",
      "[001/00350] train_loss: 0.004372\n",
      "[001/00400] train_loss: 0.004304\n",
      "[001/00450] train_loss: 0.004108\n",
      "[001/00500] train_loss: 0.004107\n",
      "[001/00550] train_loss: 0.004290\n",
      "[001/00600] train_loss: 0.004333\n",
      "[001/00650] train_loss: 0.004514\n",
      "[001/00700] train_loss: 0.004272\n",
      "[001/00750] train_loss: 0.004334\n",
      "[001/00800] train_loss: 0.004352\n",
      "[001/00850] train_loss: 0.004209\n",
      "[001/00900] train_loss: 0.003973\n",
      "[001/00950] train_loss: 0.003942\n",
      "[001/01000] train_loss: 0.004066\n",
      "[001/01050] train_loss: 0.003926\n",
      "[001/01100] train_loss: 0.004145\n",
      "[001/01150] train_loss: 0.004330\n",
      "[001/01200] train_loss: 0.003773\n",
      "[001/01200] val_loss: 0.138447 | best_loss_val: 0.114089\n",
      "[001/01250] train_loss: 0.004142\n",
      "[001/01300] train_loss: 0.004175\n",
      "[001/01350] train_loss: 0.003897\n",
      "[001/01400] train_loss: 0.004083\n",
      "[001/01450] train_loss: 0.004103\n",
      "[001/01500] train_loss: 0.004235\n",
      "[001/01550] train_loss: 0.004152\n",
      "[001/01600] train_loss: 0.004494\n",
      "[001/01650] train_loss: 0.004072\n",
      "[001/01700] train_loss: 0.004040\n",
      "[001/01750] train_loss: 0.003910\n",
      "[001/01800] train_loss: 0.004167\n",
      "[001/01850] train_loss: 0.003896\n",
      "[001/01900] train_loss: 0.004184\n",
      "[001/01950] train_loss: 0.003897\n",
      "[001/02000] train_loss: 0.004019\n",
      "[001/02050] train_loss: 0.004293\n",
      "[001/02100] train_loss: 0.003906\n",
      "[001/02150] train_loss: 0.003976\n",
      "[001/02200] train_loss: 0.003741\n",
      "[001/02200] val_loss: 0.104484 | best_loss_val: 0.104484\n",
      "[001/02250] train_loss: 0.003998\n",
      "[001/02300] train_loss: 0.003921\n",
      "[001/02350] train_loss: 0.004176\n",
      "[001/02400] train_loss: 0.004057\n",
      "[001/02450] train_loss: 0.003920\n",
      "[001/02500] train_loss: 0.003975\n",
      "[001/02550] train_loss: 0.003825\n",
      "[001/02600] train_loss: 0.003779\n",
      "[001/02650] train_loss: 0.004161\n",
      "[001/02700] train_loss: 0.004133\n",
      "[001/02750] train_loss: 0.003826\n",
      "[001/02800] train_loss: 0.004156\n",
      "[001/02850] train_loss: 0.004171\n",
      "[001/02900] train_loss: 0.004019\n",
      "[001/02950] train_loss: 0.004074\n",
      "[001/03000] train_loss: 0.004023\n",
      "[001/03050] train_loss: 0.003893\n",
      "[001/03100] train_loss: 0.003936\n",
      "[001/03150] train_loss: 0.003973\n",
      "[001/03200] train_loss: 0.003872\n",
      "[001/03200] val_loss: 0.232966 | best_loss_val: 0.104484\n",
      "[001/03250] train_loss: 0.004105\n",
      "[001/03300] train_loss: 0.003812\n",
      "[001/03350] train_loss: 0.003856\n",
      "[001/03400] train_loss: 0.003787\n",
      "[001/03450] train_loss: 0.003748\n",
      "[001/03500] train_loss: 0.004085\n",
      "[001/03550] train_loss: 0.003778\n",
      "[001/03600] train_loss: 0.003829\n",
      "[001/03650] train_loss: 0.003913\n",
      "[001/03700] train_loss: 0.003954\n",
      "[001/03750] train_loss: 0.003876\n",
      "[001/03800] train_loss: 0.003925\n",
      "[001/03850] train_loss: 0.003896\n",
      "[001/03900] train_loss: 0.003767\n",
      "[001/03950] train_loss: 0.004090\n",
      "[001/04000] train_loss: 0.003938\n",
      "[001/04050] train_loss: 0.003867\n",
      "[001/04100] train_loss: 0.003757\n",
      "[001/04150] train_loss: 0.003770\n",
      "[001/04200] train_loss: 0.003859\n",
      "[001/04200] val_loss: 0.211787 | best_loss_val: 0.104484\n",
      "[001/04250] train_loss: 0.003734\n",
      "[001/04300] train_loss: 0.003610\n",
      "[001/04350] train_loss: 0.003803\n",
      "[001/04400] train_loss: 0.003975\n",
      "[001/04450] train_loss: 0.003772\n",
      "[001/04500] train_loss: 0.003645\n",
      "[001/04550] train_loss: 0.003693\n",
      "[001/04600] train_loss: 0.003609\n",
      "[001/04650] train_loss: 0.003748\n",
      "[001/04700] train_loss: 0.003469\n",
      "[001/04750] train_loss: 0.003656\n",
      "[002/00001] train_loss: 0.003746\n",
      "[002/00051] train_loss: 0.003885\n",
      "[002/00101] train_loss: 0.003609\n",
      "[002/00151] train_loss: 0.003596\n",
      "[002/00201] train_loss: 0.003365\n",
      "[002/00251] train_loss: 0.003644\n",
      "[002/00301] train_loss: 0.003293\n",
      "[002/00351] train_loss: 0.003521\n",
      "[002/00401] train_loss: 0.003632\n",
      "[002/00401] val_loss: 0.188052 | best_loss_val: 0.104484\n",
      "[002/00451] train_loss: 0.003362\n",
      "[002/00501] train_loss: 0.003780\n",
      "[002/00551] train_loss: 0.003568\n",
      "[002/00601] train_loss: 0.003495\n",
      "[002/00651] train_loss: 0.003549\n",
      "[002/00701] train_loss: 0.003478\n",
      "[002/00751] train_loss: 0.003708\n",
      "[002/00801] train_loss: 0.003566\n",
      "[002/00851] train_loss: 0.003488\n",
      "[002/00901] train_loss: 0.003554\n",
      "[002/00951] train_loss: 0.003486\n",
      "[002/01001] train_loss: 0.003390\n",
      "[002/01051] train_loss: 0.003572\n",
      "[002/01101] train_loss: 0.003342\n",
      "[002/01151] train_loss: 0.003359\n",
      "[002/01201] train_loss: 0.003795\n",
      "[002/01251] train_loss: 0.003505\n",
      "[002/01301] train_loss: 0.003505\n",
      "[002/01351] train_loss: 0.003597\n",
      "[002/01401] train_loss: 0.003580\n",
      "[002/01401] val_loss: 0.193063 | best_loss_val: 0.104484\n",
      "[002/01451] train_loss: 0.003615\n",
      "[002/01501] train_loss: 0.003681\n",
      "[002/01551] train_loss: 0.003659\n",
      "[002/01601] train_loss: 0.003758\n",
      "[002/01651] train_loss: 0.003732\n",
      "[002/01701] train_loss: 0.003570\n",
      "[002/01751] train_loss: 0.003256\n",
      "[002/01801] train_loss: 0.003302\n",
      "[002/01851] train_loss: 0.003380\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[002/01901] train_loss: 0.003313\n",
      "[002/01951] train_loss: 0.003427\n",
      "[002/02001] train_loss: 0.003337\n",
      "[002/02051] train_loss: 0.003326\n",
      "[002/02101] train_loss: 0.003715\n",
      "[002/02151] train_loss: 0.003379\n",
      "[002/02201] train_loss: 0.003546\n",
      "[002/02251] train_loss: 0.003157\n",
      "[002/02301] train_loss: 0.003301\n",
      "[002/02351] train_loss: 0.003489\n",
      "[002/02401] train_loss: 0.003386\n",
      "[002/02401] val_loss: 0.100280 | best_loss_val: 0.100280\n",
      "[002/02451] train_loss: 0.003214\n",
      "[002/02501] train_loss: 0.003332\n",
      "[002/02551] train_loss: 0.003507\n",
      "[002/02601] train_loss: 0.003726\n",
      "[002/02651] train_loss: 0.003222\n",
      "[002/02701] train_loss: 0.003444\n",
      "[002/02751] train_loss: 0.003453\n",
      "[002/02801] train_loss: 0.003583\n",
      "[002/02851] train_loss: 0.003528\n",
      "[002/02901] train_loss: 0.003414\n",
      "[002/02951] train_loss: 0.003328\n",
      "[002/03001] train_loss: 0.003448\n",
      "[002/03051] train_loss: 0.003244\n",
      "[002/03101] train_loss: 0.003063\n",
      "[002/03151] train_loss: 0.003418\n",
      "[002/03201] train_loss: 0.002994\n",
      "[002/03251] train_loss: 0.003468\n",
      "[002/03301] train_loss: 0.003544\n",
      "[002/03351] train_loss: 0.003560\n",
      "[002/03401] train_loss: 0.003313\n",
      "[002/03401] val_loss: 0.107854 | best_loss_val: 0.100280\n",
      "[002/03451] train_loss: 0.003337\n",
      "[002/03501] train_loss: 0.003307\n",
      "[002/03551] train_loss: 0.003292\n",
      "[002/03601] train_loss: 0.003267\n",
      "[002/03651] train_loss: 0.003381\n",
      "[002/03701] train_loss: 0.003590\n",
      "[002/03751] train_loss: 0.003282\n",
      "[002/03801] train_loss: 0.003444\n",
      "[002/03851] train_loss: 0.003510\n",
      "[002/03901] train_loss: 0.003267\n",
      "[002/03951] train_loss: 0.003430\n",
      "[002/04001] train_loss: 0.003270\n",
      "[002/04051] train_loss: 0.003271\n",
      "[002/04101] train_loss: 0.003367\n",
      "[002/04151] train_loss: 0.003395\n",
      "[002/04201] train_loss: 0.003202\n",
      "[002/04251] train_loss: 0.003072\n",
      "[002/04301] train_loss: 0.003349\n",
      "[002/04351] train_loss: 0.003276\n",
      "[002/04401] train_loss: 0.003317\n",
      "[002/04401] val_loss: 0.264654 | best_loss_val: 0.100280\n",
      "[002/04451] train_loss: 0.003263\n",
      "[002/04501] train_loss: 0.003334\n",
      "[002/04551] train_loss: 0.003226\n",
      "[002/04601] train_loss: 0.003447\n",
      "[002/04651] train_loss: 0.003267\n",
      "[002/04701] train_loss: 0.003294\n",
      "[002/04751] train_loss: 0.003309\n",
      "[003/00002] train_loss: 0.003142\n",
      "[003/00052] train_loss: 0.002896\n",
      "[003/00102] train_loss: 0.002912\n",
      "[003/00152] train_loss: 0.003261\n",
      "[003/00202] train_loss: 0.003156\n",
      "[003/00252] train_loss: 0.003005\n",
      "[003/00302] train_loss: 0.003000\n",
      "[003/00352] train_loss: 0.002978\n",
      "[003/00402] train_loss: 0.003045\n",
      "[003/00452] train_loss: 0.003176\n",
      "[003/00502] train_loss: 0.003178\n",
      "[003/00552] train_loss: 0.002938\n",
      "[003/00602] train_loss: 0.002857\n",
      "[003/00602] val_loss: 0.093851 | best_loss_val: 0.093851\n",
      "[003/00652] train_loss: 0.003138\n",
      "[003/00702] train_loss: 0.002842\n",
      "[003/00752] train_loss: 0.003033\n",
      "[003/00802] train_loss: 0.003163\n",
      "[003/00852] train_loss: 0.003294\n",
      "[003/00902] train_loss: 0.003107\n",
      "[003/00952] train_loss: 0.003273\n",
      "[003/01002] train_loss: 0.003247\n",
      "[003/01052] train_loss: 0.003043\n",
      "[003/01102] train_loss: 0.002930\n",
      "[003/01152] train_loss: 0.003167\n",
      "[003/01202] train_loss: 0.003037\n",
      "[003/01252] train_loss: 0.003051\n",
      "[003/01302] train_loss: 0.003414\n",
      "[003/01352] train_loss: 0.003185\n",
      "[003/01402] train_loss: 0.003278\n",
      "[003/01452] train_loss: 0.003080\n",
      "[003/01502] train_loss: 0.003063\n",
      "[003/01552] train_loss: 0.003291\n",
      "[003/01602] train_loss: 0.003133\n",
      "[003/01602] val_loss: 0.145048 | best_loss_val: 0.093851\n",
      "[003/01652] train_loss: 0.003208\n",
      "[003/01702] train_loss: 0.003046\n",
      "[003/01752] train_loss: 0.003106\n",
      "[003/01802] train_loss: 0.003094\n",
      "[003/01852] train_loss: 0.003019\n",
      "[003/01902] train_loss: 0.003109\n",
      "[003/01952] train_loss: 0.003088\n",
      "[003/02002] train_loss: 0.002970\n",
      "[003/02052] train_loss: 0.003270\n",
      "[003/02102] train_loss: 0.003115\n",
      "[003/02152] train_loss: 0.003123\n",
      "[003/02202] train_loss: 0.003015\n",
      "[003/02252] train_loss: 0.002872\n",
      "[003/02302] train_loss: 0.002914\n",
      "[003/02352] train_loss: 0.002967\n",
      "[003/02402] train_loss: 0.003059\n",
      "[003/02452] train_loss: 0.003163\n",
      "[003/02502] train_loss: 0.002948\n",
      "[003/02552] train_loss: 0.002885\n",
      "[003/02602] train_loss: 0.002969\n",
      "[003/02602] val_loss: 0.110123 | best_loss_val: 0.093851\n",
      "[003/02652] train_loss: 0.003048\n",
      "[003/02702] train_loss: 0.003032\n",
      "[003/02752] train_loss: 0.003090\n",
      "[003/02802] train_loss: 0.002801\n",
      "[003/02852] train_loss: 0.002821\n",
      "[003/02902] train_loss: 0.002950\n",
      "[003/02952] train_loss: 0.002927\n",
      "[003/03002] train_loss: 0.002913\n",
      "[003/03052] train_loss: 0.002894\n",
      "[003/03102] train_loss: 0.003002\n",
      "[003/03152] train_loss: 0.002957\n",
      "[003/03202] train_loss: 0.003029\n",
      "[003/03252] train_loss: 0.002817\n",
      "[003/03302] train_loss: 0.002997\n",
      "[003/03352] train_loss: 0.003153\n",
      "[003/03402] train_loss: 0.003123\n",
      "[003/03452] train_loss: 0.002966\n",
      "[003/03502] train_loss: 0.002848\n",
      "[003/03552] train_loss: 0.002945\n",
      "[003/03602] train_loss: 0.002958\n",
      "[003/03602] val_loss: 0.094683 | best_loss_val: 0.093851\n",
      "[003/03652] train_loss: 0.003037\n",
      "[003/03702] train_loss: 0.002925\n",
      "[003/03752] train_loss: 0.002889\n",
      "[003/03802] train_loss: 0.003266\n",
      "[003/03852] train_loss: 0.002939\n",
      "[003/03902] train_loss: 0.002964\n",
      "[003/03952] train_loss: 0.002912\n",
      "[003/04002] train_loss: 0.003114\n",
      "[003/04052] train_loss: 0.003091\n",
      "[003/04102] train_loss: 0.003261\n",
      "[003/04152] train_loss: 0.002787\n",
      "[003/04202] train_loss: 0.003063\n",
      "[003/04252] train_loss: 0.002938\n",
      "[003/04302] train_loss: 0.003132\n",
      "[003/04352] train_loss: 0.002995\n",
      "[003/04402] train_loss: 0.003070\n",
      "[003/04452] train_loss: 0.002940\n",
      "[003/04502] train_loss: 0.002908\n",
      "[003/04552] train_loss: 0.002903\n",
      "[003/04602] train_loss: 0.003018\n",
      "[003/04602] val_loss: 0.092974 | best_loss_val: 0.092974\n",
      "[003/04652] train_loss: 0.003056\n",
      "[003/04702] train_loss: 0.002648\n",
      "[003/04752] train_loss: 0.003007\n",
      "[004/00003] train_loss: 0.003100\n",
      "[004/00053] train_loss: 0.002736\n",
      "[004/00103] train_loss: 0.002797\n",
      "[004/00153] train_loss: 0.002797\n",
      "[004/00203] train_loss: 0.002850\n",
      "[004/00253] train_loss: 0.002832\n",
      "[004/00303] train_loss: 0.002694\n",
      "[004/00353] train_loss: 0.002708\n",
      "[004/00403] train_loss: 0.002766\n",
      "[004/00453] train_loss: 0.002828\n",
      "[004/00503] train_loss: 0.002885\n",
      "[004/00553] train_loss: 0.002673\n",
      "[004/00603] train_loss: 0.002621\n",
      "[004/00653] train_loss: 0.002643\n",
      "[004/00703] train_loss: 0.002761\n",
      "[004/00753] train_loss: 0.002738\n",
      "[004/00803] train_loss: 0.002721\n",
      "[004/00803] val_loss: 0.464735 | best_loss_val: 0.092974\n",
      "[004/00853] train_loss: 0.002881\n",
      "[004/00903] train_loss: 0.002666\n",
      "[004/00953] train_loss: 0.002770\n",
      "[004/01003] train_loss: 0.002741\n",
      "[004/01053] train_loss: 0.002915\n",
      "[004/01103] train_loss: 0.002653\n",
      "[004/01153] train_loss: 0.002695\n",
      "[004/01203] train_loss: 0.002696\n",
      "[004/01253] train_loss: 0.002684\n",
      "[004/01303] train_loss: 0.002822\n",
      "[004/01353] train_loss: 0.002723\n",
      "[004/01403] train_loss: 0.002630\n",
      "[004/01453] train_loss: 0.002634\n",
      "[004/01503] train_loss: 0.002818\n",
      "[004/01553] train_loss: 0.002746\n",
      "[004/01603] train_loss: 0.002907\n",
      "[004/01653] train_loss: 0.002830\n",
      "[004/01703] train_loss: 0.002619\n",
      "[004/01753] train_loss: 0.002724\n",
      "[004/01803] train_loss: 0.002829\n",
      "[004/01803] val_loss: 0.290621 | best_loss_val: 0.092974\n",
      "[004/01853] train_loss: 0.002691\n",
      "[004/01903] train_loss: 0.002787\n",
      "[004/01953] train_loss: 0.002682\n",
      "[004/02003] train_loss: 0.002820\n",
      "[004/02053] train_loss: 0.002831\n",
      "[004/02103] train_loss: 0.002659\n",
      "[004/02153] train_loss: 0.002639\n",
      "[004/02203] train_loss: 0.002876\n",
      "[004/02253] train_loss: 0.002773\n",
      "[004/02303] train_loss: 0.002804\n",
      "[004/02353] train_loss: 0.002557\n",
      "[004/02403] train_loss: 0.002759\n",
      "[004/02453] train_loss: 0.002841\n",
      "[004/02503] train_loss: 0.002797\n",
      "[004/02553] train_loss: 0.002604\n",
      "[004/02603] train_loss: 0.002598\n",
      "[004/02653] train_loss: 0.002767\n",
      "[004/02703] train_loss: 0.002703\n",
      "[004/02753] train_loss: 0.002648\n",
      "[004/02803] train_loss: 0.002840\n",
      "[004/02803] val_loss: 0.256516 | best_loss_val: 0.092974\n",
      "[004/02853] train_loss: 0.002930\n",
      "[004/02903] train_loss: 0.002628\n",
      "[004/02953] train_loss: 0.002825\n",
      "[004/03003] train_loss: 0.002670\n",
      "[004/03053] train_loss: 0.002789\n",
      "[004/03103] train_loss: 0.002711\n",
      "[004/03153] train_loss: 0.002655\n",
      "[004/03203] train_loss: 0.002624\n",
      "[004/03253] train_loss: 0.002726\n",
      "[004/03303] train_loss: 0.002834\n",
      "[004/03353] train_loss: 0.002755\n",
      "[004/03403] train_loss: 0.002949\n",
      "[004/03453] train_loss: 0.002713\n",
      "[004/03503] train_loss: 0.002686\n",
      "[004/03553] train_loss: 0.002764\n",
      "[004/03603] train_loss: 0.002646\n",
      "[004/03653] train_loss: 0.002507\n",
      "[004/03703] train_loss: 0.002763\n",
      "[004/03753] train_loss: 0.002842\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[004/03803] train_loss: 0.002563\n",
      "[004/03803] val_loss: 0.178973 | best_loss_val: 0.092974\n",
      "[004/03853] train_loss: 0.002728\n",
      "[004/03903] train_loss: 0.002529\n",
      "[004/03953] train_loss: 0.002789\n",
      "[004/04003] train_loss: 0.002606\n",
      "[004/04053] train_loss: 0.002730\n",
      "[004/04103] train_loss: 0.002811\n",
      "[004/04153] train_loss: 0.002543\n",
      "[004/04203] train_loss: 0.002741\n",
      "[004/04253] train_loss: 0.002624\n",
      "[004/04303] train_loss: 0.002593\n",
      "[004/04353] train_loss: 0.002743\n",
      "[004/04403] train_loss: 0.002736\n",
      "[004/04453] train_loss: 0.002905\n",
      "[004/04503] train_loss: 0.002675\n",
      "[004/04553] train_loss: 0.002875\n",
      "[004/04603] train_loss: 0.002827\n",
      "[004/04653] train_loss: 0.002810\n",
      "[004/04703] train_loss: 0.002824\n",
      "[004/04753] train_loss: 0.002753\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    'experiment_name': '3_1_3depn_generalization',\n",
    "    'device': 'cuda:0',  # change this to cpu if you do not have a GPU\n",
    "    'is_overfit': False,\n",
    "    'batch_size': 32,\n",
    "    'resume_ckpt': None,\n",
    "    'learning_rate': 0.001,\n",
    "    'max_epochs': 5,\n",
    "    'print_every_n': 50,\n",
    "    'validate_every_n': 1000,\n",
    "}\n",
    "train_3depn.main(config)  # should be able to get best_loss_val < 0.1 after a few hours and 5 epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (f) Inference\n",
    "\n",
    "Implement the missing bits in `exercise_3/inference/infer_3depn.py`. You should then be able to see your reconstructions below.\n",
    "\n",
    "The outputs of our provided visualization functions are, from left to right:\n",
    "- Input, partial shape\n",
    "- Predicted completion\n",
    "- Target shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exercise_3.util.visualization import visualize_meshes\n",
    "from exercise_3.inference.infer_3depn import InferenceHandler3DEPN\n",
    "\n",
    "# create a handler for inference using a trained checkpoint\n",
    "inferer = InferenceHandler3DEPN('exercise_3/runs/3_1_3depn_generalization/model_best.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ad088e23908445f802c87d36ddf7a59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "input_sdf = ShapeNet.get_shape_sdf('03636649/b286c9c136784db2af1744fdb1fbe7df__0__')\n",
    "target_df = ShapeNet.get_shape_df('03636649/b286c9c136784db2af1744fdb1fbe7df__0__')\n",
    "\n",
    "input_mesh, reconstructed_mesh, target_mesh = inferer.infer_single(input_sdf, target_df)\n",
    "visualize_meshes([input_mesh, reconstructed_mesh, target_mesh], flip_axes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a67eebfd96784a00adcf05cb7c0246eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "input_sdf = ShapeNet.get_shape_sdf('03636649/23eaba9bdd51a5b0dfe9cab879fd37e8__1__')\n",
    "target_df = ShapeNet.get_shape_df('03636649/23eaba9bdd51a5b0dfe9cab879fd37e8__0__')\n",
    "\n",
    "input_mesh, reconstructed_mesh, target_mesh = inferer.infer_single(input_sdf, target_df)\n",
    "visualize_meshes([input_mesh, reconstructed_mesh, target_mesh], flip_axes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09beb050f5aa4165950004cc985e18aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "input_sdf = ShapeNet.get_shape_sdf('02691156/5de2cc606b65b960e0b6546e08902f28__0__')\n",
    "target_df = ShapeNet.get_shape_df('02691156/5de2cc606b65b960e0b6546e08902f28__0__')\n",
    "\n",
    "input_mesh, reconstructed_mesh, target_mesh = inferer.infer_single(input_sdf, target_df)\n",
    "visualize_meshes([input_mesh, reconstructed_mesh, target_mesh], flip_axes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 DeepSDF\n",
    "\n",
    "\n",
    "Here, we will take a look at 3D-reconstruction using [DeepSDF](https://arxiv.org/abs/1901.05103). We recommend reading the paper before attempting the exercise.\n",
    "\n",
    "DeepSDF is an auto-decoder based approach that learns a continuous SDF representation for a class of shapes. Once trained, it can be used for shape representation, interpolation and shape completion. We'll look at each of these\n",
    "applications.\n",
    "\n",
    "<img src=\"exercise_3/images/deepsdf_teaser.png\" alt=\"deepsdf_teaser\" style=\"width: 800px;\"/>\n",
    "\n",
    "During training, the autodecoder optimizes both the network parameters and the latent codes representing each of the training shapes. Once trained, to reconstruct a shape given its SDF observations, a latent code is\n",
    "optimized keeping the network parameters fixed, such that the optimized latent code gives the lowest error with observed SDF values.\n",
    "\n",
    "An advantage that implicit representations have over voxel/grid based approaches is that they are not tied to a particular grid resolution, and can be evaluated at any resolution once trained.\n",
    "\n",
    "Similar to previous exercise, we'll first download the processed dataset, look at the implementation of the dataset, the model and the trainer, try out overfitting and generalization over the entire dataset, and finally inference on unseen samples.\n",
    "\n",
    "### (a) Downloading the data\n",
    "\n",
    "Whereas volumetric models output entire 3d shape representations, implicit models like DeepSDF work on per point basis. The network takes in a 3D-coordinate (and additionally the latent vector) and outputs the SDF value at the queried point. To train such a model,\n",
    "we therefore need, for each of the training shapes, a bunch of points with their corresponding SDF values for supervision. Points are sampled more aggressively near the surface of the object as we want to capture a more detailed SDF near the surface. For those curious,\n",
    "data preparation is decribed in more detail in section 5 of the paper.\n",
    "\n",
    "We'll be using the ShapeNet Sofa class for the experiments in this exercise. We've already prepared this data, so that you don't need to deal with the preprocessing. For each shape, the following files are provided:\n",
    "- `mesh.obj` representing the mesh representation of the shape\n",
    "- `sdf.npz` file containing large number of points sampled on and around the mesh and their sdf values; contains numpy arrays under keys \"pos\" and \"neg\", containing points with positive and negative sdf values respectively\n",
    "\n",
    "```\n",
    "# contents of exercise_3/data/sdf_sofas\n",
    "1faa4c299b93a3e5593ebeeedbff73b/                    # shape 0\n",
    "    ├── mesh.obj                                    # shape 0 mesh\n",
    "    ├── sdf.npz                                     # shape 0 sdf\n",
    "    ├── surface.obj                                 # shape 0 surface\n",
    "1fde48d83065ef5877a929f61fea4d0/                    # shape 1\n",
    "1fe1411b6c8097acf008d8a3590fb522/                   # shape 2\n",
    ":\n",
    "```\n",
    "Download and extract the data with the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading ...\n",
      "--2022-12-03 16:30:13--  https://www.dropbox.com/s/4k5pw126nzus8ef/sdf_sofas.zip?dl=0\n",
      "Resolving www.dropbox.com (www.dropbox.com)... 2620:100:6022:18::a27d:4212, 162.125.66.18\n",
      "Connecting to www.dropbox.com (www.dropbox.com)|2620:100:6022:18::a27d:4212|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: /s/raw/4k5pw126nzus8ef/sdf_sofas.zip [following]\n",
      "--2022-12-03 16:30:13--  https://www.dropbox.com/s/raw/4k5pw126nzus8ef/sdf_sofas.zip\n",
      "Reusing existing connection to [www.dropbox.com]:443.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://uc0db6c4138716c198b3ad943bc7.dl.dropboxusercontent.com/cd/0/inline/Bx58xZBS2HEe46hvn_v0KopLF4mvpL1TZO7N0NwgjZqR7Da6MUa0MRJ1OXFgvGJZrVqX1T9oXvUKw35fyrsUfqVgtIwrFesguLJDKBgS2e5_PhjyQwJkVCDycmZ5e6jPaXegCFOLpYlKbX71I-XaIBBhG8djzhLeBNRTobB4FTk7_g/file# [following]\n",
      "--2022-12-03 16:30:14--  https://uc0db6c4138716c198b3ad943bc7.dl.dropboxusercontent.com/cd/0/inline/Bx58xZBS2HEe46hvn_v0KopLF4mvpL1TZO7N0NwgjZqR7Da6MUa0MRJ1OXFgvGJZrVqX1T9oXvUKw35fyrsUfqVgtIwrFesguLJDKBgS2e5_PhjyQwJkVCDycmZ5e6jPaXegCFOLpYlKbX71I-XaIBBhG8djzhLeBNRTobB4FTk7_g/file\n",
      "Resolving uc0db6c4138716c198b3ad943bc7.dl.dropboxusercontent.com (uc0db6c4138716c198b3ad943bc7.dl.dropboxusercontent.com)... 2620:100:6027:15::a27d:480f, 162.125.72.15\n",
      "Connecting to uc0db6c4138716c198b3ad943bc7.dl.dropboxusercontent.com (uc0db6c4138716c198b3ad943bc7.dl.dropboxusercontent.com)|2620:100:6027:15::a27d:480f|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: /cd/0/inline2/Bx5roayUS3MJSZhx-DOdZwINWVRjxbqfCx2uE5Mr4Z0rYj58CeR9U0PcFMsDa2snB8PXUKNcMdVYHWtRgI2X4iIrluCx2n7Ub1p2oj1QsVxyhuKLWgFSPV8iIJTp3BDRS-kLlP_d1dnz6F_uC8BaqAaCPqhR4xu-4UDT6NzGUFZ01yX9J2gIOXH4zpo0L7we3VEhmFQPikG1hhaH5vKWFO9OeY0P8segPrxhm0gu2KMagxmH7veD0AxIUeBsrxOJw9P94hogpY-M2y4gf-dAOIlh_Cm7DOyDTzy-N5JG7ECwfrGdW5VwhLgusK_Rd09EYQ2Nw_vAv9n30QjkIgwXrA0QGWGht1M222r2_Dtx8hqGA-z9ds8tjkPQCLd8Ttpv0N9ayU1KkPckAiQGmh5PElc2yN-d3_vwM0jZOmbRxGDpqA/file [following]\n",
      "--2022-12-03 16:30:15--  https://uc0db6c4138716c198b3ad943bc7.dl.dropboxusercontent.com/cd/0/inline2/Bx5roayUS3MJSZhx-DOdZwINWVRjxbqfCx2uE5Mr4Z0rYj58CeR9U0PcFMsDa2snB8PXUKNcMdVYHWtRgI2X4iIrluCx2n7Ub1p2oj1QsVxyhuKLWgFSPV8iIJTp3BDRS-kLlP_d1dnz6F_uC8BaqAaCPqhR4xu-4UDT6NzGUFZ01yX9J2gIOXH4zpo0L7we3VEhmFQPikG1hhaH5vKWFO9OeY0P8segPrxhm0gu2KMagxmH7veD0AxIUeBsrxOJw9P94hogpY-M2y4gf-dAOIlh_Cm7DOyDTzy-N5JG7ECwfrGdW5VwhLgusK_Rd09EYQ2Nw_vAv9n30QjkIgwXrA0QGWGht1M222r2_Dtx8hqGA-z9ds8tjkPQCLd8Ttpv0N9ayU1KkPckAiQGmh5PElc2yN-d3_vwM0jZOmbRxGDpqA/file\n",
      "Reusing existing connection to [uc0db6c4138716c198b3ad943bc7.dl.dropboxusercontent.com]:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 10138669216 (9,4G) [application/zip]\n",
      "Saving to: ‘exercise_3/data/sdf_sofas.zip’\n",
      "\n",
      "sdf_sofas.zip        76%[==============>     ]   7,26G  3,70MB/s    in 59m 59s \n",
      "\n",
      "2022-12-03 17:30:16 (2,06 MB/s) - Connection closed at byte 7791851903. Retrying.\n",
      "\n",
      "--2022-12-03 17:30:17--  (try: 2)  https://uc0db6c4138716c198b3ad943bc7.dl.dropboxusercontent.com/cd/0/inline2/Bx5roayUS3MJSZhx-DOdZwINWVRjxbqfCx2uE5Mr4Z0rYj58CeR9U0PcFMsDa2snB8PXUKNcMdVYHWtRgI2X4iIrluCx2n7Ub1p2oj1QsVxyhuKLWgFSPV8iIJTp3BDRS-kLlP_d1dnz6F_uC8BaqAaCPqhR4xu-4UDT6NzGUFZ01yX9J2gIOXH4zpo0L7we3VEhmFQPikG1hhaH5vKWFO9OeY0P8segPrxhm0gu2KMagxmH7veD0AxIUeBsrxOJw9P94hogpY-M2y4gf-dAOIlh_Cm7DOyDTzy-N5JG7ECwfrGdW5VwhLgusK_Rd09EYQ2Nw_vAv9n30QjkIgwXrA0QGWGht1M222r2_Dtx8hqGA-z9ds8tjkPQCLd8Ttpv0N9ayU1KkPckAiQGmh5PElc2yN-d3_vwM0jZOmbRxGDpqA/file\n",
      "Connecting to uc0db6c4138716c198b3ad943bc7.dl.dropboxusercontent.com (uc0db6c4138716c198b3ad943bc7.dl.dropboxusercontent.com)|2620:100:6027:15::a27d:480f|:443... connected.\n",
      "HTTP request sent, awaiting response... 206 Partial Content\n",
      "Length: 10138669216 (9,4G), 2346817313 (2,2G) remaining [application/zip]\n",
      "Saving to: ‘exercise_3/data/sdf_sofas.zip’\n",
      "\n",
      "sdf_sofas.zip       100%[+++++++++++++++====>]   9,44G  4,13MB/s    in 14m 7s  \n",
      "\n",
      "2022-12-03 17:44:25 (2,64 MB/s) - ‘exercise_3/data/sdf_sofas.zip’ saved [10138669216/10138669216]\n",
      "\n",
      "Extracting ...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "print('Downloading ...')\n",
    "# File sizes: ~10GB\n",
    "!wget https://www.dropbox.com/s/4k5pw126nzus8ef/sdf_sofas.zip\\?dl\\=0 -O exercise_3/data/sdf_sofas.zip -P exercise_3/data\n",
    "\n",
    "print('Extracting ...')\n",
    "!unzip -q exercise_3/data/sdf_sofas.zip -d exercise_3/data\n",
    "!rm exercise_3/data/sdf_sofas.zip\n",
    "\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Dataset\n",
    "\n",
    "We provide a partial implementation of the dataset in `exercise_3/data/shape_implicit.py`.\n",
    "Your task is to complete the `#TODOs` so that the dataset works as specified by the docstrings.\n",
    "\n",
    "Once done, you can try running the following code blocks as sanity checks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train set: 1226\n",
      "Length of val set: 137\n",
      "Length of overfit set: 1\n"
     ]
    }
   ],
   "source": [
    "from exercise_3.data.shape_implicit import ShapeImplicit\n",
    "\n",
    "num_points_to_samples = 40000\n",
    "train_dataset = ShapeImplicit(num_points_to_samples, \"train\")\n",
    "val_dataset = ShapeImplicit(num_points_to_samples, \"val\")\n",
    "overfit_dataset = ShapeImplicit(num_points_to_samples, \"overfit\")\n",
    "\n",
    "# Get length, which is a call to __len__ function\n",
    "print(f'Length of train set: {len(train_dataset)}')  # expected output: 1226\n",
    "# Get length, which is a call to __len__ function\n",
    "print(f'Length of val set: {len(val_dataset)}')  # expected output: 137\n",
    "# Get length, which is a call to __len__ function\n",
    "print(f'Length of overfit set: {len(overfit_dataset)}')  # expected output: 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's take a look at the points sampled for a particular shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from exercise_3.util.visualization import visualize_mesh, visualize_pointcloud\n",
    "\n",
    "shape_id = train_dataset[0]['name']\n",
    "points = train_dataset[0]['points']\n",
    "sdf = train_dataset[0]['sdf']\n",
    "\n",
    "# sampled points inside the shape\n",
    "inside_points = points[sdf[:, 0] < 0, :].numpy()\n",
    "\n",
    "# sampled points outside the shape\n",
    "outside_points = points[sdf[:, 0] > 0, :].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mesh\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/oriol/.cache/pypoetry/virtualenvs/machine_learning_for_3d_geometry-_2t5S_p_-py3.10/lib/python3.10/site-packages/traittypes/traittypes.py:97: UserWarning: Given trait value dtype \"uint32\" does not match required type \"uint32\". A coerced copy has been created.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "609be19dab494f4094f1f6fbb4dfafe4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mesh = ShapeImplicit.get_mesh(shape_id)\n",
    "print('Mesh')\n",
    "visualize_mesh(mesh.vertices, mesh.faces, flip_axes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled points with negative SDF (inside)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e1f6c9d948b4d4f920cdf952e518ded",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('Sampled points with negative SDF (inside)')\n",
    "visualize_pointcloud(inside_points, 0.025, flip_axes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled points with positive SDF (outside)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cfb882da03546beb667c7ae22785504",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('Sampled points with positive SDF (outside)')\n",
    "visualize_pointcloud(outside_points, 0.025, flip_axes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice that more points are sampled close to the surface rather than away from the surface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### (c) Model\n",
    "\n",
    "The DeepSDF auto-decoder architecture is visualized below:\n",
    "\n",
    "<img src=\"exercise_3/images/deepsdf_architecture.png\" alt=\"deepsdf_arch\" style=\"width: 640px;\"/>\n",
    "\n",
    "Things to note:\n",
    "\n",
    "- The network takes in the latent code for a shape concatenated with the query 3d coordinate, making up a 259 length vector (assuming latent code length is 256).\n",
    "- The network consist of a sequence of weight-normed linear layers, each followed by a ReLU and a dropout. For weight norming a layer, check out `torch.nn.utils.weight_norm`. Each of these linear layers outputs a 512 dimensional vector, except the 4th layer which outputs a 253 dimensional vector.\n",
    "- The output of the 4th layer is concatenated with the input, making the input to the 5th layer a 512 dimensional vector.\n",
    "- The final layer is a simple linear layer without any norm, dropout or non-linearity, with a single dimensional output representing the SDF value.\n",
    "\n",
    "Implement this architecture in file `exercise_3/model/deepsdf.py`.\n",
    "\n",
    "Here are some basic sanity tests once you're done with your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   | Name    | Type           | Params \n",
      "---------------------------------------------\n",
      "0  | wnl1    | Linear         | 133632 \n",
      "1  | wnl2    | Linear         | 263168 \n",
      "2  | wnl3    | Linear         | 263168 \n",
      "3  | wnl4    | Linear         | 130042 \n",
      "4  | wnl5    | Linear         | 263168 \n",
      "5  | wnl6    | Linear         | 263168 \n",
      "6  | wnl7    | Linear         | 263168 \n",
      "7  | wnl8    | Linear         | 263168 \n",
      "8  | fcn     | Linear         | 513    \n",
      "9  | relu    | ReLU           | 0      \n",
      "10 | dropout | Dropout        | 0      \n",
      "11 | TOTAL   | DeepSDFDecoder | 1843195\n",
      "\n",
      "Output tensor shape:  torch.Size([4096, 1])\n",
      "\n",
      "Number of traininable params: 1.84M\n"
     ]
    }
   ],
   "source": [
    "from exercise_3.model.deepsdf import DeepSDFDecoder\n",
    "from exercise_3.util.model import summarize_model\n",
    "\n",
    "deepsdf = DeepSDFDecoder(latent_size=256)\n",
    "print(summarize_model(deepsdf))\n",
    "\n",
    "# input to the network is a concatenation of point coordinates (3) and the latent code (256 in this example);\n",
    "# here we use a batch of 4096 points\n",
    "input_tensor = torch.randn(4096, 3 + 256)\n",
    "predictions = deepsdf(input_tensor)\n",
    "\n",
    "print('\\nOutput tensor shape: ', predictions.shape)  # expected output: 4096, 1\n",
    "\n",
    "num_trainable_params = sum(p.numel() for p in deepsdf.parameters() if p.requires_grad) / 1e6\n",
    "print(f'\\nNumber of traininable params: {num_trainable_params:.2f}M')  # expected output: ~1.8M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (d) Training script and overfitting to a single shape\n",
    "\n",
    "Fill in the train script in `exercise_3/training/train_deepsdf.py`, and verify that your training work by overfitting to a few samples below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "[049/00000] train_loss: 0.035413\n",
      "[099/00000] train_loss: 0.024070\n",
      "[149/00000] train_loss: 0.018119\n",
      "[199/00000] train_loss: 0.014501\n",
      "[249/00000] train_loss: 0.012590\n",
      "[299/00000] train_loss: 0.011702\n",
      "[349/00000] train_loss: 0.010594\n",
      "[399/00000] train_loss: 0.009976\n",
      "[449/00000] train_loss: 0.009626\n",
      "[499/00000] train_loss: 0.009212\n",
      "[549/00000] train_loss: 0.008450\n",
      "[599/00000] train_loss: 0.008251\n",
      "[649/00000] train_loss: 0.008085\n",
      "[699/00000] train_loss: 0.007972\n",
      "[749/00000] train_loss: 0.007816\n",
      "[799/00000] train_loss: 0.007676\n",
      "[849/00000] train_loss: 0.007663\n",
      "[899/00000] train_loss: 0.007493\n",
      "[949/00000] train_loss: 0.007357\n",
      "[999/00000] train_loss: 0.007314\n",
      "[1049/00000] train_loss: 0.007058\n",
      "[1099/00000] train_loss: 0.007008\n",
      "[1149/00000] train_loss: 0.006990\n",
      "[1199/00000] train_loss: 0.006900\n",
      "[1249/00000] train_loss: 0.006893\n",
      "[1299/00000] train_loss: 0.006812\n",
      "[1349/00000] train_loss: 0.006765\n",
      "[1399/00000] train_loss: 0.006694\n",
      "[1449/00000] train_loss: 0.006659\n",
      "[1499/00000] train_loss: 0.006631\n",
      "[1549/00000] train_loss: 0.006540\n",
      "[1599/00000] train_loss: 0.006482\n",
      "[1649/00000] train_loss: 0.006488\n",
      "[1699/00000] train_loss: 0.006439\n",
      "[1749/00000] train_loss: 0.006429\n",
      "[1799/00000] train_loss: 0.006407\n",
      "[1849/00000] train_loss: 0.006392\n",
      "[1899/00000] train_loss: 0.006342\n",
      "[1949/00000] train_loss: 0.006359\n",
      "[1999/00000] train_loss: 0.006313\n"
     ]
    }
   ],
   "source": [
    "from exercise_3.training import train_deepsdf\n",
    "\n",
    "overfit_config = {\n",
    "    'experiment_name': '3_2_deepsdf_overfit',\n",
    "    'device': 'cuda:0',  # change this to cpu if you do not have a GPU\n",
    "    'is_overfit': True,\n",
    "    'num_sample_points': 4096,\n",
    "    'latent_code_length': 256,\n",
    "    'batch_size': 1,\n",
    "    'resume_ckpt': None,\n",
    "    'learning_rate_model': 0.0005,\n",
    "    'learning_rate_code': 0.001,\n",
    "    'lambda_code_regularization': 0.0001,\n",
    "    'max_epochs': 2000,\n",
    "    'print_every_n': 50,\n",
    "    'visualize_every_n': 250,\n",
    "}\n",
    "\n",
    "train_deepsdf.main(overfit_config)  # expected loss around 0.0062"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the overfitted shape reconstruction to check if it looks reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GT\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33423e0dfdac40df841b786d855ac058",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overfit\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40d68fe470714f63933a26ca0f3fc268",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load and visualize GT mesh of the overfit sample\n",
    "gt_mesh = ShapeImplicit.get_mesh('7e728818848f191bee7d178666aae23d')\n",
    "print('GT')\n",
    "visualize_mesh(gt_mesh.vertices, gt_mesh.faces, flip_axes=True)\n",
    "\n",
    "# Load and visualize reconstructed overfit sample; it's okay if they don't look visually exact, since we don't run \n",
    "# the training too long and have a learning rate decay while training \n",
    "mesh_path = \"exercise_3/runs/3_2_deepsdf_overfit/meshes/01999_000.obj\"\n",
    "overfit_output = trimesh.load(mesh_path)\n",
    "print('Overfit')\n",
    "visualize_mesh(overfit_output.vertices, overfit_output.faces, flip_axes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (e) Training over entire train set\n",
    "\n",
    "Once overfitting works, we can train on the entire train set.\n",
    "\n",
    "Note: This training will take a few hours on a GPU (took ~3 hrs for 500 epochs on our 2080Ti, which already gave decent results). Please make sure to start training early enough before the submission deadline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "[000/00049] train_loss: 0.037374\n",
      "[000/00099] train_loss: 0.033836\n",
      "[000/00149] train_loss: 0.033300\n",
      "[000/00199] train_loss: 0.033588\n",
      "[000/00249] train_loss: 0.034959\n",
      "[000/00299] train_loss: 0.032489\n",
      "[000/00349] train_loss: 0.033075\n",
      "[000/00399] train_loss: 0.031810\n",
      "[000/00449] train_loss: 0.033007\n",
      "[000/00499] train_loss: 0.032857\n",
      "[000/00549] train_loss: 0.031932\n",
      "[000/00599] train_loss: 0.032695\n",
      "[000/00649] train_loss: 0.031765\n",
      "[000/00699] train_loss: 0.031937\n",
      "[000/00749] train_loss: 0.032509\n",
      "[000/00799] train_loss: 0.031831\n",
      "[000/00849] train_loss: 0.031820\n",
      "[000/00899] train_loss: 0.032367\n",
      "[000/00949] train_loss: 0.031795\n",
      "[000/00999] train_loss: 0.030276\n",
      "[000/01049] train_loss: 0.033185\n",
      "[000/01099] train_loss: 0.032054\n",
      "[000/01149] train_loss: 0.030535\n",
      "[000/01199] train_loss: 0.030892\n",
      "[001/00023] train_loss: 0.032519\n",
      "[001/00073] train_loss: 0.031907\n",
      "[001/00123] train_loss: 0.030956\n",
      "[001/00173] train_loss: 0.031553\n",
      "[001/00223] train_loss: 0.030168\n",
      "[001/00273] train_loss: 0.030307\n",
      "[001/00323] train_loss: 0.031393\n",
      "[001/00373] train_loss: 0.030462\n",
      "[001/00423] train_loss: 0.030577\n",
      "[001/00473] train_loss: 0.030957\n",
      "[001/00523] train_loss: 0.031438\n",
      "[001/00573] train_loss: 0.031522\n",
      "[001/00623] train_loss: 0.030562\n",
      "[001/00673] train_loss: 0.031031\n",
      "[001/00723] train_loss: 0.029776\n",
      "[001/00773] train_loss: 0.030624\n",
      "[001/00823] train_loss: 0.030060\n",
      "[001/00873] train_loss: 0.030538\n",
      "[001/00923] train_loss: 0.029967\n",
      "[001/00973] train_loss: 0.029974\n",
      "[001/01023] train_loss: 0.030855\n",
      "[001/01073] train_loss: 0.031118\n",
      "[001/01123] train_loss: 0.030833\n",
      "[001/01173] train_loss: 0.030307\n",
      "[001/01223] train_loss: 0.031293\n",
      "[002/00047] train_loss: 0.030754\n",
      "[002/00097] train_loss: 0.029604\n",
      "[002/00147] train_loss: 0.030577\n",
      "[002/00197] train_loss: 0.028707\n",
      "[002/00247] train_loss: 0.030698\n",
      "[002/00297] train_loss: 0.029653\n",
      "[002/00347] train_loss: 0.029364\n",
      "[002/00397] train_loss: 0.028689\n",
      "[002/00447] train_loss: 0.029474\n",
      "[002/00497] train_loss: 0.029401\n",
      "[002/00547] train_loss: 0.028941\n",
      "[002/00597] train_loss: 0.028516\n",
      "[002/00647] train_loss: 0.029199\n",
      "[002/00697] train_loss: 0.029687\n",
      "[002/00747] train_loss: 0.029094\n",
      "[002/00797] train_loss: 0.028888\n",
      "[002/00847] train_loss: 0.029015\n",
      "[002/00897] train_loss: 0.030012\n",
      "[002/00947] train_loss: 0.029411\n",
      "[002/00997] train_loss: 0.029893\n",
      "[002/01047] train_loss: 0.028212\n",
      "[002/01097] train_loss: 0.028873\n",
      "[002/01147] train_loss: 0.028568\n",
      "[002/01197] train_loss: 0.028430\n",
      "[003/00021] train_loss: 0.030598\n",
      "[003/00071] train_loss: 0.029008\n",
      "[003/00121] train_loss: 0.028336\n",
      "[003/00171] train_loss: 0.028252\n",
      "[003/00221] train_loss: 0.027780\n",
      "[003/00271] train_loss: 0.027908\n",
      "[003/00321] train_loss: 0.028099\n",
      "[003/00371] train_loss: 0.027423\n",
      "[003/00421] train_loss: 0.028160\n",
      "[003/00471] train_loss: 0.026831\n",
      "[003/00521] train_loss: 0.026451\n",
      "[003/00571] train_loss: 0.027336\n",
      "[003/00621] train_loss: 0.028164\n",
      "[003/00671] train_loss: 0.027502\n",
      "[003/00721] train_loss: 0.028034\n",
      "[003/00771] train_loss: 0.026770\n",
      "[003/00821] train_loss: 0.028577\n",
      "[003/00871] train_loss: 0.028137\n",
      "[003/00921] train_loss: 0.027212\n",
      "[003/00971] train_loss: 0.027340\n",
      "[003/01021] train_loss: 0.027218\n",
      "[003/01071] train_loss: 0.027419\n",
      "[003/01121] train_loss: 0.026954\n",
      "[003/01171] train_loss: 0.027004\n",
      "[003/01221] train_loss: 0.026808\n",
      "[004/00045] train_loss: 0.028661\n",
      "[004/00095] train_loss: 0.027416\n",
      "[004/00145] train_loss: 0.026625\n",
      "[004/00195] train_loss: 0.026467\n",
      "[004/00245] train_loss: 0.025575\n",
      "[004/00295] train_loss: 0.026362\n",
      "[004/00345] train_loss: 0.025969\n",
      "[004/00395] train_loss: 0.026063\n",
      "[004/00445] train_loss: 0.026173\n",
      "[004/00495] train_loss: 0.027671\n",
      "[004/00545] train_loss: 0.025801\n",
      "[004/00595] train_loss: 0.025695\n",
      "[004/00645] train_loss: 0.026241\n",
      "[004/00695] train_loss: 0.025078\n",
      "[004/00745] train_loss: 0.026398\n",
      "[004/00795] train_loss: 0.025703\n",
      "[004/00845] train_loss: 0.025392\n",
      "[004/00895] train_loss: 0.026433\n",
      "[004/00945] train_loss: 0.026117\n",
      "[004/00995] train_loss: 0.026070\n",
      "[004/01045] train_loss: 0.025561\n",
      "[004/01095] train_loss: 0.025938\n",
      "[004/01145] train_loss: 0.026115\n",
      "[004/01195] train_loss: 0.026197\n",
      "[005/00019] train_loss: 0.027099\n",
      "[005/00069] train_loss: 0.026332\n",
      "[005/00119] train_loss: 0.026700\n",
      "[005/00169] train_loss: 0.027106\n",
      "[005/00219] train_loss: 0.025807\n",
      "[005/00269] train_loss: 0.025124\n",
      "[005/00319] train_loss: 0.025891\n",
      "[005/00369] train_loss: 0.026129\n",
      "[005/00419] train_loss: 0.026045\n",
      "[005/00469] train_loss: 0.024381\n",
      "[005/00519] train_loss: 0.024917\n",
      "[005/00569] train_loss: 0.025113\n",
      "[005/00619] train_loss: 0.024247\n",
      "[005/00669] train_loss: 0.024715\n",
      "[005/00719] train_loss: 0.024480\n",
      "[005/00769] train_loss: 0.025648\n",
      "[005/00819] train_loss: 0.024741\n",
      "[005/00869] train_loss: 0.024184\n",
      "[005/00919] train_loss: 0.024759\n",
      "[005/00969] train_loss: 0.024059\n",
      "[005/01019] train_loss: 0.025555\n",
      "[005/01069] train_loss: 0.024693\n",
      "[005/01119] train_loss: 0.024524\n",
      "[005/01169] train_loss: 0.024164\n",
      "[005/01219] train_loss: 0.024949\n",
      "[006/00043] train_loss: 0.026749\n",
      "[006/00093] train_loss: 0.026405\n",
      "[006/00143] train_loss: 0.025239\n",
      "[006/00193] train_loss: 0.024370\n",
      "[006/00243] train_loss: 0.024253\n",
      "[006/00293] train_loss: 0.024302\n",
      "[006/00343] train_loss: 0.024448\n",
      "[006/00393] train_loss: 0.023888\n",
      "[006/00443] train_loss: 0.024510\n",
      "[006/00493] train_loss: 0.024186\n",
      "[006/00543] train_loss: 0.024713\n",
      "[006/00593] train_loss: 0.023661\n",
      "[006/00643] train_loss: 0.023933\n",
      "[006/00693] train_loss: 0.023427\n",
      "[006/00743] train_loss: 0.023648\n",
      "[006/00793] train_loss: 0.023513\n",
      "[006/00843] train_loss: 0.023763\n",
      "[006/00893] train_loss: 0.023068\n",
      "[006/00943] train_loss: 0.023429\n",
      "[006/00993] train_loss: 0.023559\n",
      "[006/01043] train_loss: 0.023445\n",
      "[006/01093] train_loss: 0.024153\n",
      "[006/01143] train_loss: 0.024254\n",
      "[006/01193] train_loss: 0.024485\n",
      "[007/00017] train_loss: 0.025451\n",
      "[007/00067] train_loss: 0.026099\n",
      "[007/00117] train_loss: 0.024673\n",
      "[007/00167] train_loss: 0.024475\n",
      "[007/00217] train_loss: 0.023919\n",
      "[007/00267] train_loss: 0.023651\n",
      "[007/00317] train_loss: 0.024067\n",
      "[007/00367] train_loss: 0.023943\n",
      "[007/00417] train_loss: 0.023309\n",
      "[007/00467] train_loss: 0.023484\n",
      "[007/00517] train_loss: 0.023433\n",
      "[007/00567] train_loss: 0.023514\n",
      "[007/00617] train_loss: 0.024008\n",
      "[007/00667] train_loss: 0.022453\n",
      "[007/00717] train_loss: 0.023439\n",
      "[007/00767] train_loss: 0.023229\n",
      "[007/00817] train_loss: 0.022980\n",
      "[007/00867] train_loss: 0.023701\n",
      "[007/00917] train_loss: 0.022535\n",
      "[007/00967] train_loss: 0.023977\n",
      "[007/01017] train_loss: 0.024141\n",
      "[007/01067] train_loss: 0.023223\n",
      "[007/01117] train_loss: 0.023054\n",
      "[007/01167] train_loss: 0.022697\n",
      "[007/01217] train_loss: 0.022324\n",
      "[008/00041] train_loss: 0.026336\n",
      "[008/00091] train_loss: 0.025215\n",
      "[008/00141] train_loss: 0.023503\n",
      "[008/00191] train_loss: 0.023249\n",
      "[008/00241] train_loss: 0.023124\n",
      "[008/00291] train_loss: 0.023529\n",
      "[008/00341] train_loss: 0.022931\n",
      "[008/00391] train_loss: 0.023250\n",
      "[008/00441] train_loss: 0.022595\n",
      "[008/00491] train_loss: 0.021940\n",
      "[008/00541] train_loss: 0.023351\n",
      "[008/00591] train_loss: 0.021732\n",
      "[008/00641] train_loss: 0.022993\n",
      "[008/00691] train_loss: 0.022714\n",
      "[008/00741] train_loss: 0.022130\n",
      "[008/00791] train_loss: 0.022124\n",
      "[008/00841] train_loss: 0.022594\n",
      "[008/00891] train_loss: 0.023740\n",
      "[008/00941] train_loss: 0.022236\n",
      "[008/00991] train_loss: 0.022537\n",
      "[008/01041] train_loss: 0.022599\n",
      "[008/01091] train_loss: 0.022616\n",
      "[008/01141] train_loss: 0.022302\n",
      "[008/01191] train_loss: 0.021638\n",
      "[009/00015] train_loss: 0.022583\n",
      "[009/00065] train_loss: 0.026057\n",
      "[009/00115] train_loss: 0.024286\n",
      "[009/00165] train_loss: 0.023604\n",
      "[009/00215] train_loss: 0.022773\n",
      "[009/00265] train_loss: 0.022066\n",
      "[009/00315] train_loss: 0.022974\n",
      "[009/00365] train_loss: 0.022331\n",
      "[009/00415] train_loss: 0.022169\n",
      "[009/00465] train_loss: 0.022639\n",
      "[009/00515] train_loss: 0.021999\n",
      "[009/00565] train_loss: 0.022172\n",
      "[009/00615] train_loss: 0.021643\n",
      "[009/00665] train_loss: 0.021834\n",
      "[009/00715] train_loss: 0.021926\n",
      "[009/00765] train_loss: 0.022087\n",
      "[009/00815] train_loss: 0.022657\n",
      "[009/00865] train_loss: 0.021318\n",
      "[009/00915] train_loss: 0.022518\n",
      "[009/00965] train_loss: 0.021615\n",
      "[009/01015] train_loss: 0.022556\n",
      "[009/01065] train_loss: 0.021530\n",
      "[009/01115] train_loss: 0.022558\n",
      "[009/01165] train_loss: 0.021659\n",
      "[009/01215] train_loss: 0.023460\n",
      "[010/00039] train_loss: 0.024577\n",
      "[010/00089] train_loss: 0.023211\n",
      "[010/00139] train_loss: 0.023939\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[010/00189] train_loss: 0.023408\n",
      "[010/00239] train_loss: 0.022441\n",
      "[010/00289] train_loss: 0.022266\n",
      "[010/00339] train_loss: 0.022392\n",
      "[010/00389] train_loss: 0.022129\n",
      "[010/00439] train_loss: 0.021723\n",
      "[010/00489] train_loss: 0.021392\n",
      "[010/00539] train_loss: 0.021125\n",
      "[010/00589] train_loss: 0.021602\n",
      "[010/00639] train_loss: 0.021868\n",
      "[010/00689] train_loss: 0.021186\n",
      "[010/00739] train_loss: 0.022027\n",
      "[010/00789] train_loss: 0.020874\n",
      "[010/00839] train_loss: 0.021381\n",
      "[010/00889] train_loss: 0.021422\n",
      "[010/00939] train_loss: 0.022314\n",
      "[010/00989] train_loss: 0.020986\n",
      "[010/01039] train_loss: 0.021163\n",
      "[010/01089] train_loss: 0.021263\n",
      "[010/01139] train_loss: 0.021447\n",
      "[010/01189] train_loss: 0.021337\n",
      "[011/00013] train_loss: 0.021945\n",
      "[011/00063] train_loss: 0.023853\n",
      "[011/00113] train_loss: 0.023660\n",
      "[011/00163] train_loss: 0.022409\n",
      "[011/00213] train_loss: 0.022493\n",
      "[011/00263] train_loss: 0.020799\n",
      "[011/00313] train_loss: 0.021785\n",
      "[011/00363] train_loss: 0.021331\n",
      "[011/00413] train_loss: 0.020588\n",
      "[011/00463] train_loss: 0.021372\n",
      "[011/00513] train_loss: 0.021394\n",
      "[011/00563] train_loss: 0.021865\n",
      "[011/00613] train_loss: 0.021706\n",
      "[011/00663] train_loss: 0.021523\n",
      "[011/00713] train_loss: 0.021666\n",
      "[011/00763] train_loss: 0.020990\n",
      "[011/00813] train_loss: 0.021068\n",
      "[011/00863] train_loss: 0.021579\n",
      "[011/00913] train_loss: 0.021195\n",
      "[011/00963] train_loss: 0.020855\n",
      "[011/01013] train_loss: 0.021543\n",
      "[011/01063] train_loss: 0.020436\n",
      "[011/01113] train_loss: 0.020937\n",
      "[011/01163] train_loss: 0.020589\n",
      "[011/01213] train_loss: 0.021660\n",
      "[012/00037] train_loss: 0.022947\n",
      "[012/00087] train_loss: 0.024545\n",
      "[012/00137] train_loss: 0.022029\n",
      "[012/00187] train_loss: 0.022235\n",
      "[012/00237] train_loss: 0.022122\n",
      "[012/00287] train_loss: 0.020889\n",
      "[012/00337] train_loss: 0.020688\n",
      "[012/00387] train_loss: 0.021064\n",
      "[012/00437] train_loss: 0.021239\n",
      "[012/00487] train_loss: 0.021816\n",
      "[012/00537] train_loss: 0.020771\n",
      "[012/00587] train_loss: 0.019902\n",
      "[012/00637] train_loss: 0.021190\n",
      "[012/00687] train_loss: 0.020936\n",
      "[012/00737] train_loss: 0.020275\n",
      "[012/00787] train_loss: 0.020165\n",
      "[012/00837] train_loss: 0.020761\n",
      "[012/00887] train_loss: 0.020488\n",
      "[012/00937] train_loss: 0.020485\n",
      "[012/00987] train_loss: 0.021064\n",
      "[012/01037] train_loss: 0.021059\n",
      "[012/01087] train_loss: 0.021290\n",
      "[012/01137] train_loss: 0.020216\n",
      "[012/01187] train_loss: 0.022028\n",
      "[013/00011] train_loss: 0.022265\n",
      "[013/00061] train_loss: 0.022987\n",
      "[013/00111] train_loss: 0.022919\n",
      "[013/00161] train_loss: 0.022245\n",
      "[013/00211] train_loss: 0.021685\n",
      "[013/00261] train_loss: 0.021364\n",
      "[013/00311] train_loss: 0.022580\n",
      "[013/00361] train_loss: 0.021281\n",
      "[013/00411] train_loss: 0.020670\n",
      "[013/00461] train_loss: 0.019575\n",
      "[013/00511] train_loss: 0.021193\n",
      "[013/00561] train_loss: 0.020687\n",
      "[013/00611] train_loss: 0.020722\n",
      "[013/00661] train_loss: 0.020013\n",
      "[013/00711] train_loss: 0.021173\n",
      "[013/00761] train_loss: 0.020198\n",
      "[013/00811] train_loss: 0.020918\n",
      "[013/00861] train_loss: 0.020337\n",
      "[013/00911] train_loss: 0.020308\n",
      "[013/00961] train_loss: 0.020477\n",
      "[013/01011] train_loss: 0.020865\n",
      "[013/01061] train_loss: 0.019883\n",
      "[013/01111] train_loss: 0.019944\n",
      "[013/01161] train_loss: 0.020208\n",
      "[013/01211] train_loss: 0.020297\n",
      "[014/00035] train_loss: 0.022649\n",
      "[014/00085] train_loss: 0.022620\n",
      "[014/00135] train_loss: 0.021700\n",
      "[014/00185] train_loss: 0.022653\n",
      "[014/00235] train_loss: 0.022117\n",
      "[014/00285] train_loss: 0.020619\n",
      "[014/00335] train_loss: 0.020407\n",
      "[014/00385] train_loss: 0.021057\n",
      "[014/00435] train_loss: 0.019769\n",
      "[014/00485] train_loss: 0.021101\n",
      "[014/00535] train_loss: 0.019770\n",
      "[014/00585] train_loss: 0.020159\n",
      "[014/00635] train_loss: 0.019289\n",
      "[014/00685] train_loss: 0.020306\n",
      "[014/00735] train_loss: 0.020083\n",
      "[014/00785] train_loss: 0.021027\n",
      "[014/00835] train_loss: 0.019940\n",
      "[014/00885] train_loss: 0.019832\n",
      "[014/00935] train_loss: 0.021121\n",
      "[014/00985] train_loss: 0.020600\n",
      "[014/01035] train_loss: 0.019891\n",
      "[014/01085] train_loss: 0.021123\n",
      "[014/01135] train_loss: 0.019609\n",
      "[014/01185] train_loss: 0.020154\n",
      "[015/00009] train_loss: 0.020679\n",
      "[015/00059] train_loss: 0.022210\n",
      "[015/00109] train_loss: 0.022211\n",
      "[015/00159] train_loss: 0.022080\n",
      "[015/00209] train_loss: 0.021484\n",
      "[015/00259] train_loss: 0.022305\n",
      "[015/00309] train_loss: 0.020954\n",
      "[015/00359] train_loss: 0.020333\n",
      "[015/00409] train_loss: 0.020925\n",
      "[015/00459] train_loss: 0.020144\n",
      "[015/00509] train_loss: 0.020456\n",
      "[015/00559] train_loss: 0.019443\n",
      "[015/00609] train_loss: 0.020666\n",
      "[015/00659] train_loss: 0.020928\n",
      "[015/00709] train_loss: 0.019853\n",
      "[015/00759] train_loss: 0.019353\n",
      "[015/00809] train_loss: 0.018991\n",
      "[015/00859] train_loss: 0.019594\n",
      "[015/00909] train_loss: 0.019769\n",
      "[015/00959] train_loss: 0.020085\n",
      "[015/01009] train_loss: 0.019048\n",
      "[015/01059] train_loss: 0.020164\n",
      "[015/01109] train_loss: 0.019967\n",
      "[015/01159] train_loss: 0.019593\n",
      "[015/01209] train_loss: 0.019797\n",
      "[016/00033] train_loss: 0.022776\n",
      "[016/00083] train_loss: 0.022450\n",
      "[016/00133] train_loss: 0.022659\n",
      "[016/00183] train_loss: 0.020822\n",
      "[016/00233] train_loss: 0.020594\n",
      "[016/00283] train_loss: 0.020912\n",
      "[016/00333] train_loss: 0.019645\n",
      "[016/00383] train_loss: 0.020319\n",
      "[016/00433] train_loss: 0.020603\n",
      "[016/00483] train_loss: 0.020064\n",
      "[016/00533] train_loss: 0.021078\n",
      "[016/00583] train_loss: 0.020655\n",
      "[016/00633] train_loss: 0.019718\n",
      "[016/00683] train_loss: 0.020257\n",
      "[016/00733] train_loss: 0.020087\n",
      "[016/00783] train_loss: 0.020291\n",
      "[016/00833] train_loss: 0.020256\n",
      "[016/00883] train_loss: 0.019488\n",
      "[016/00933] train_loss: 0.019514\n",
      "[016/00983] train_loss: 0.020026\n",
      "[016/01033] train_loss: 0.018885\n",
      "[016/01083] train_loss: 0.019992\n",
      "[016/01133] train_loss: 0.018735\n",
      "[016/01183] train_loss: 0.019508\n",
      "[017/00007] train_loss: 0.019592\n",
      "[017/00057] train_loss: 0.022485\n",
      "[017/00107] train_loss: 0.021789\n",
      "[017/00157] train_loss: 0.020857\n",
      "[017/00207] train_loss: 0.020532\n",
      "[017/00257] train_loss: 0.020340\n",
      "[017/00307] train_loss: 0.019792\n",
      "[017/00357] train_loss: 0.020529\n",
      "[017/00407] train_loss: 0.020809\n",
      "[017/00457] train_loss: 0.019439\n",
      "[017/00507] train_loss: 0.020495\n",
      "[017/00557] train_loss: 0.019917\n",
      "[017/00607] train_loss: 0.020248\n",
      "[017/00657] train_loss: 0.019809\n",
      "[017/00707] train_loss: 0.019744\n",
      "[017/00757] train_loss: 0.019636\n",
      "[017/00807] train_loss: 0.019133\n",
      "[017/00857] train_loss: 0.019300\n",
      "[017/00907] train_loss: 0.019877\n",
      "[017/00957] train_loss: 0.019832\n",
      "[017/01007] train_loss: 0.019267\n",
      "[017/01057] train_loss: 0.019462\n",
      "[017/01107] train_loss: 0.019447\n",
      "[017/01157] train_loss: 0.019503\n",
      "[017/01207] train_loss: 0.019321\n",
      "[018/00031] train_loss: 0.021886\n",
      "[018/00081] train_loss: 0.022253\n",
      "[018/00131] train_loss: 0.021438\n",
      "[018/00181] train_loss: 0.021182\n",
      "[018/00231] train_loss: 0.020442\n",
      "[018/00281] train_loss: 0.019328\n",
      "[018/00331] train_loss: 0.020257\n",
      "[018/00381] train_loss: 0.020455\n",
      "[018/00431] train_loss: 0.019579\n",
      "[018/00481] train_loss: 0.020356\n",
      "[018/00531] train_loss: 0.019206\n",
      "[018/00581] train_loss: 0.019510\n",
      "[018/00631] train_loss: 0.019769\n",
      "[018/00681] train_loss: 0.019868\n",
      "[018/00731] train_loss: 0.018840\n",
      "[018/00781] train_loss: 0.019618\n",
      "[018/00831] train_loss: 0.018903\n",
      "[018/00881] train_loss: 0.019952\n",
      "[018/00931] train_loss: 0.019063\n",
      "[018/00981] train_loss: 0.019476\n",
      "[018/01031] train_loss: 0.020013\n",
      "[018/01081] train_loss: 0.018583\n",
      "[018/01131] train_loss: 0.018475\n",
      "[018/01181] train_loss: 0.019673\n",
      "[019/00005] train_loss: 0.019572\n",
      "[019/00055] train_loss: 0.021801\n",
      "[019/00105] train_loss: 0.020907\n",
      "[019/00155] train_loss: 0.020802\n",
      "[019/00205] train_loss: 0.020366\n",
      "[019/00255] train_loss: 0.020318\n",
      "[019/00305] train_loss: 0.019299\n",
      "[019/00355] train_loss: 0.020040\n",
      "[019/00405] train_loss: 0.019631\n",
      "[019/00455] train_loss: 0.019334\n",
      "[019/00505] train_loss: 0.019112\n",
      "[019/00555] train_loss: 0.019992\n",
      "[019/00605] train_loss: 0.020725\n",
      "[019/00655] train_loss: 0.019108\n",
      "[019/00705] train_loss: 0.019132\n",
      "[019/00755] train_loss: 0.019282\n",
      "[019/00805] train_loss: 0.018816\n",
      "[019/00855] train_loss: 0.019165\n",
      "[019/00905] train_loss: 0.019137\n",
      "[019/00955] train_loss: 0.019477\n",
      "[019/01005] train_loss: 0.018903\n",
      "[019/01055] train_loss: 0.018688\n",
      "[019/01105] train_loss: 0.019579\n",
      "[019/01155] train_loss: 0.018384\n",
      "[019/01205] train_loss: 0.018004\n",
      "[020/00029] train_loss: 0.020619\n",
      "[020/00079] train_loss: 0.021543\n",
      "[020/00129] train_loss: 0.020188\n",
      "[020/00179] train_loss: 0.020263\n",
      "[020/00229] train_loss: 0.019901\n",
      "[020/00279] train_loss: 0.019135\n",
      "[020/00329] train_loss: 0.019047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[020/00379] train_loss: 0.019508\n",
      "[020/00429] train_loss: 0.019263\n",
      "[020/00479] train_loss: 0.019945\n",
      "[020/00529] train_loss: 0.019150\n",
      "[020/00579] train_loss: 0.019514\n",
      "[020/00629] train_loss: 0.018979\n",
      "[020/00679] train_loss: 0.019183\n",
      "[020/00729] train_loss: 0.019447\n",
      "[020/00779] train_loss: 0.018626\n",
      "[020/00829] train_loss: 0.020072\n",
      "[020/00879] train_loss: 0.019172\n",
      "[020/00929] train_loss: 0.019102\n",
      "[020/00979] train_loss: 0.019687\n",
      "[020/01029] train_loss: 0.018170\n",
      "[020/01079] train_loss: 0.018163\n",
      "[020/01129] train_loss: 0.019019\n",
      "[020/01179] train_loss: 0.019010\n",
      "[021/00003] train_loss: 0.019129\n",
      "[021/00053] train_loss: 0.022670\n",
      "[021/00103] train_loss: 0.022024\n",
      "[021/00153] train_loss: 0.019723\n",
      "[021/00203] train_loss: 0.020124\n",
      "[021/00253] train_loss: 0.019652\n",
      "[021/00303] train_loss: 0.020027\n",
      "[021/00353] train_loss: 0.019652\n",
      "[021/00403] train_loss: 0.019273\n",
      "[021/00453] train_loss: 0.020049\n",
      "[021/00503] train_loss: 0.018939\n",
      "[021/00553] train_loss: 0.019374\n",
      "[021/00603] train_loss: 0.019058\n",
      "[021/00653] train_loss: 0.019970\n",
      "[021/00703] train_loss: 0.019208\n",
      "[021/00753] train_loss: 0.019263\n",
      "[021/00803] train_loss: 0.018432\n",
      "[021/00853] train_loss: 0.018630\n",
      "[021/00903] train_loss: 0.019136\n",
      "[021/00953] train_loss: 0.019047\n",
      "[021/01003] train_loss: 0.018450\n",
      "[021/01053] train_loss: 0.019015\n",
      "[021/01103] train_loss: 0.018967\n",
      "[021/01153] train_loss: 0.018479\n",
      "[021/01203] train_loss: 0.018054\n",
      "[022/00027] train_loss: 0.021085\n",
      "[022/00077] train_loss: 0.021302\n",
      "[022/00127] train_loss: 0.021548\n",
      "[022/00177] train_loss: 0.019295\n",
      "[022/00227] train_loss: 0.019558\n",
      "[022/00277] train_loss: 0.019941\n",
      "[022/00327] train_loss: 0.019330\n",
      "[022/00377] train_loss: 0.019556\n",
      "[022/00427] train_loss: 0.019732\n",
      "[022/00477] train_loss: 0.018384\n",
      "[022/00527] train_loss: 0.018420\n",
      "[022/00577] train_loss: 0.018257\n",
      "[022/00627] train_loss: 0.017997\n",
      "[022/00677] train_loss: 0.019147\n",
      "[022/00727] train_loss: 0.018306\n",
      "[022/00777] train_loss: 0.019646\n",
      "[022/00827] train_loss: 0.019070\n",
      "[022/00877] train_loss: 0.018752\n",
      "[022/00927] train_loss: 0.018373\n",
      "[022/00977] train_loss: 0.017701\n",
      "[022/01027] train_loss: 0.018686\n",
      "[022/01077] train_loss: 0.019077\n",
      "[022/01127] train_loss: 0.018439\n",
      "[022/01177] train_loss: 0.018303\n",
      "[023/00001] train_loss: 0.018943\n",
      "[023/00051] train_loss: 0.021509\n",
      "[023/00101] train_loss: 0.020971\n",
      "[023/00151] train_loss: 0.021289\n",
      "[023/00201] train_loss: 0.019770\n",
      "[023/00251] train_loss: 0.019081\n",
      "[023/00301] train_loss: 0.018943\n",
      "[023/00351] train_loss: 0.018047\n",
      "[023/00401] train_loss: 0.019247\n",
      "[023/00451] train_loss: 0.018876\n",
      "[023/00501] train_loss: 0.018920\n",
      "[023/00551] train_loss: 0.018531\n",
      "[023/00601] train_loss: 0.018459\n",
      "[023/00651] train_loss: 0.018714\n",
      "[023/00701] train_loss: 0.019346\n",
      "[023/00751] train_loss: 0.018083\n",
      "[023/00801] train_loss: 0.019362\n",
      "[023/00851] train_loss: 0.018208\n",
      "[023/00901] train_loss: 0.018304\n",
      "[023/00951] train_loss: 0.017817\n",
      "[023/01001] train_loss: 0.017865\n",
      "[023/01051] train_loss: 0.018486\n",
      "[023/01101] train_loss: 0.018069\n",
      "[023/01151] train_loss: 0.017657\n",
      "[023/01201] train_loss: 0.018825\n",
      "[024/00025] train_loss: 0.019938\n",
      "[024/00075] train_loss: 0.020827\n",
      "[024/00125] train_loss: 0.020430\n",
      "[024/00175] train_loss: 0.020014\n",
      "[024/00225] train_loss: 0.020101\n",
      "[024/00275] train_loss: 0.019710\n",
      "[024/00325] train_loss: 0.018076\n",
      "[024/00375] train_loss: 0.019043\n",
      "[024/00425] train_loss: 0.018587\n",
      "[024/00475] train_loss: 0.019125\n",
      "[024/00525] train_loss: 0.018915\n",
      "[024/00575] train_loss: 0.019236\n",
      "[024/00625] train_loss: 0.018416\n",
      "[024/00675] train_loss: 0.018729\n",
      "[024/00725] train_loss: 0.017811\n",
      "[024/00775] train_loss: 0.018775\n",
      "[024/00825] train_loss: 0.018661\n",
      "[024/00875] train_loss: 0.018615\n",
      "[024/00925] train_loss: 0.018566\n",
      "[024/00975] train_loss: 0.018586\n",
      "[024/01025] train_loss: 0.018684\n",
      "[024/01075] train_loss: 0.019033\n",
      "[024/01125] train_loss: 0.018203\n",
      "[024/01175] train_loss: 0.017459\n",
      "[024/01225] train_loss: 0.018235\n",
      "[025/00049] train_loss: 0.020305\n",
      "[025/00099] train_loss: 0.020499\n",
      "[025/00149] train_loss: 0.019851\n",
      "[025/00199] train_loss: 0.018971\n",
      "[025/00249] train_loss: 0.019242\n",
      "[025/00299] train_loss: 0.019092\n",
      "[025/00349] train_loss: 0.018700\n",
      "[025/00399] train_loss: 0.019253\n",
      "[025/00449] train_loss: 0.019171\n",
      "[025/00499] train_loss: 0.018308\n",
      "[025/00549] train_loss: 0.019083\n",
      "[025/00599] train_loss: 0.017855\n",
      "[025/00649] train_loss: 0.018312\n",
      "[025/00699] train_loss: 0.017759\n",
      "[025/00749] train_loss: 0.018099\n",
      "[025/00799] train_loss: 0.017941\n",
      "[025/00849] train_loss: 0.018492\n",
      "[025/00899] train_loss: 0.017599\n",
      "[025/00949] train_loss: 0.018008\n",
      "[025/00999] train_loss: 0.017798\n",
      "[025/01049] train_loss: 0.018239\n",
      "[025/01099] train_loss: 0.018326\n",
      "[025/01149] train_loss: 0.017962\n",
      "[025/01199] train_loss: 0.018706\n",
      "[026/00023] train_loss: 0.019556\n",
      "[026/00073] train_loss: 0.021557\n",
      "[026/00123] train_loss: 0.019566\n",
      "[026/00173] train_loss: 0.019861\n",
      "[026/00223] train_loss: 0.018281\n",
      "[026/00273] train_loss: 0.019819\n",
      "[026/00323] train_loss: 0.018893\n",
      "[026/00373] train_loss: 0.018664\n",
      "[026/00423] train_loss: 0.018072\n",
      "[026/00473] train_loss: 0.018035\n",
      "[026/00523] train_loss: 0.017871\n",
      "[026/00573] train_loss: 0.017671\n",
      "[026/00623] train_loss: 0.018264\n",
      "[026/00673] train_loss: 0.018496\n",
      "[026/00723] train_loss: 0.019274\n",
      "[026/00773] train_loss: 0.018415\n",
      "[026/00823] train_loss: 0.018724\n",
      "[026/00873] train_loss: 0.018294\n",
      "[026/00923] train_loss: 0.019091\n",
      "[026/00973] train_loss: 0.018773\n",
      "[026/01023] train_loss: 0.018114\n",
      "[026/01073] train_loss: 0.017557\n",
      "[026/01123] train_loss: 0.018851\n",
      "[026/01173] train_loss: 0.017435\n",
      "[026/01223] train_loss: 0.017562\n",
      "[027/00047] train_loss: 0.020372\n",
      "[027/00097] train_loss: 0.021706\n",
      "[027/00147] train_loss: 0.019311\n",
      "[027/00197] train_loss: 0.020345\n",
      "[027/00247] train_loss: 0.018049\n",
      "[027/00297] train_loss: 0.018914\n",
      "[027/00347] train_loss: 0.018692\n",
      "[027/00397] train_loss: 0.019507\n",
      "[027/00447] train_loss: 0.017804\n",
      "[027/00497] train_loss: 0.017992\n",
      "[027/00547] train_loss: 0.019264\n",
      "[027/00597] train_loss: 0.018418\n",
      "[027/00647] train_loss: 0.017949\n",
      "[027/00697] train_loss: 0.017750\n",
      "[027/00747] train_loss: 0.018245\n",
      "[027/00797] train_loss: 0.018014\n",
      "[027/00847] train_loss: 0.017745\n",
      "[027/00897] train_loss: 0.018233\n",
      "[027/00947] train_loss: 0.018344\n",
      "[027/00997] train_loss: 0.017898\n",
      "[027/01047] train_loss: 0.017551\n",
      "[027/01097] train_loss: 0.018096\n",
      "[027/01147] train_loss: 0.018132\n",
      "[027/01197] train_loss: 0.017887\n",
      "[028/00021] train_loss: 0.019495\n",
      "[028/00071] train_loss: 0.019863\n",
      "[028/00121] train_loss: 0.020361\n",
      "[028/00171] train_loss: 0.018875\n",
      "[028/00221] train_loss: 0.018269\n",
      "[028/00271] train_loss: 0.019473\n",
      "[028/00321] train_loss: 0.018726\n",
      "[028/00371] train_loss: 0.018632\n",
      "[028/00421] train_loss: 0.018028\n",
      "[028/00471] train_loss: 0.018055\n",
      "[028/00521] train_loss: 0.019327\n",
      "[028/00571] train_loss: 0.017960\n",
      "[028/00621] train_loss: 0.018004\n",
      "[028/00671] train_loss: 0.017375\n",
      "[028/00721] train_loss: 0.017577\n",
      "[028/00771] train_loss: 0.017917\n",
      "[028/00821] train_loss: 0.017492\n",
      "[028/00871] train_loss: 0.017880\n",
      "[028/00921] train_loss: 0.017946\n",
      "[028/00971] train_loss: 0.017498\n",
      "[028/01021] train_loss: 0.018434\n",
      "[028/01071] train_loss: 0.018162\n",
      "[028/01121] train_loss: 0.017802\n",
      "[028/01171] train_loss: 0.017448\n",
      "[028/01221] train_loss: 0.017527\n",
      "[029/00045] train_loss: 0.020063\n",
      "[029/00095] train_loss: 0.020737\n",
      "[029/00145] train_loss: 0.020115\n",
      "[029/00195] train_loss: 0.019404\n",
      "[029/00245] train_loss: 0.019210\n",
      "[029/00295] train_loss: 0.019891\n",
      "[029/00345] train_loss: 0.018175\n",
      "[029/00395] train_loss: 0.018142\n",
      "[029/00445] train_loss: 0.018100\n",
      "[029/00495] train_loss: 0.017480\n",
      "[029/00545] train_loss: 0.019236\n",
      "[029/00595] train_loss: 0.017193\n",
      "[029/00645] train_loss: 0.017452\n",
      "[029/00695] train_loss: 0.018383\n",
      "[029/00745] train_loss: 0.017904\n",
      "[029/00795] train_loss: 0.017975\n",
      "[029/00845] train_loss: 0.017151\n",
      "[029/00895] train_loss: 0.017990\n",
      "[029/00945] train_loss: 0.017751\n",
      "[029/00995] train_loss: 0.017810\n",
      "[029/01045] train_loss: 0.016973\n",
      "[029/01095] train_loss: 0.017632\n",
      "[029/01145] train_loss: 0.018034\n",
      "[029/01195] train_loss: 0.017438\n",
      "[030/00019] train_loss: 0.019066\n",
      "[030/00069] train_loss: 0.020353\n",
      "[030/00119] train_loss: 0.019498\n",
      "[030/00169] train_loss: 0.019580\n",
      "[030/00219] train_loss: 0.018720\n",
      "[030/00269] train_loss: 0.019334\n",
      "[030/00319] train_loss: 0.018418\n",
      "[030/00369] train_loss: 0.018413\n",
      "[030/00419] train_loss: 0.018404\n",
      "[030/00469] train_loss: 0.018240\n",
      "[030/00519] train_loss: 0.019137\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[030/00569] train_loss: 0.017893\n",
      "[030/00619] train_loss: 0.018630\n",
      "[030/00669] train_loss: 0.017503\n",
      "[030/00719] train_loss: 0.017136\n",
      "[030/00769] train_loss: 0.017749\n",
      "[030/00819] train_loss: 0.017079\n",
      "[030/00869] train_loss: 0.016929\n",
      "[030/00919] train_loss: 0.018279\n",
      "[030/00969] train_loss: 0.017525\n",
      "[030/01019] train_loss: 0.017537\n",
      "[030/01069] train_loss: 0.017449\n",
      "[030/01119] train_loss: 0.017216\n",
      "[030/01169] train_loss: 0.017580\n",
      "[030/01219] train_loss: 0.018233\n",
      "[031/00043] train_loss: 0.020877\n",
      "[031/00093] train_loss: 0.019837\n",
      "[031/00143] train_loss: 0.019205\n",
      "[031/00193] train_loss: 0.018168\n",
      "[031/00243] train_loss: 0.018035\n",
      "[031/00293] train_loss: 0.017851\n",
      "[031/00343] train_loss: 0.018217\n",
      "[031/00393] train_loss: 0.018137\n",
      "[031/00443] train_loss: 0.017754\n",
      "[031/00493] train_loss: 0.017315\n",
      "[031/00543] train_loss: 0.017857\n",
      "[031/00593] train_loss: 0.018018\n",
      "[031/00643] train_loss: 0.017690\n",
      "[031/00693] train_loss: 0.017492\n",
      "[031/00743] train_loss: 0.018378\n",
      "[031/00793] train_loss: 0.017793\n",
      "[031/00843] train_loss: 0.017298\n",
      "[031/00893] train_loss: 0.017249\n",
      "[031/00943] train_loss: 0.018841\n",
      "[031/00993] train_loss: 0.017459\n",
      "[031/01043] train_loss: 0.016976\n",
      "[031/01093] train_loss: 0.017604\n",
      "[031/01143] train_loss: 0.016982\n",
      "[031/01193] train_loss: 0.017985\n",
      "[032/00017] train_loss: 0.018945\n",
      "[032/00067] train_loss: 0.020182\n",
      "[032/00117] train_loss: 0.020588\n",
      "[032/00167] train_loss: 0.019636\n",
      "[032/00217] train_loss: 0.017952\n",
      "[032/00267] train_loss: 0.018258\n",
      "[032/00317] train_loss: 0.018078\n",
      "[032/00367] train_loss: 0.017726\n",
      "[032/00417] train_loss: 0.017583\n",
      "[032/00467] train_loss: 0.018337\n",
      "[032/00517] train_loss: 0.017738\n",
      "[032/00567] train_loss: 0.017336\n",
      "[032/00617] train_loss: 0.017966\n",
      "[032/00667] train_loss: 0.017770\n",
      "[032/00717] train_loss: 0.017164\n",
      "[032/00767] train_loss: 0.016967\n",
      "[032/00817] train_loss: 0.017301\n",
      "[032/00867] train_loss: 0.016779\n",
      "[032/00917] train_loss: 0.017768\n",
      "[032/00967] train_loss: 0.017535\n",
      "[032/01017] train_loss: 0.017703\n",
      "[032/01067] train_loss: 0.017912\n",
      "[032/01117] train_loss: 0.017235\n",
      "[032/01167] train_loss: 0.017415\n",
      "[032/01217] train_loss: 0.017390\n",
      "[033/00041] train_loss: 0.021056\n",
      "[033/00091] train_loss: 0.020546\n",
      "[033/00141] train_loss: 0.019421\n",
      "[033/00191] train_loss: 0.018270\n",
      "[033/00241] train_loss: 0.018212\n",
      "[033/00291] train_loss: 0.018535\n",
      "[033/00341] train_loss: 0.018005\n",
      "[033/00391] train_loss: 0.018581\n",
      "[033/00441] train_loss: 0.016531\n",
      "[033/00491] train_loss: 0.017481\n",
      "[033/00541] train_loss: 0.018104\n",
      "[033/00591] train_loss: 0.018054\n",
      "[033/00641] train_loss: 0.017638\n",
      "[033/00691] train_loss: 0.017856\n",
      "[033/00741] train_loss: 0.017087\n",
      "[033/00791] train_loss: 0.017667\n",
      "[033/00841] train_loss: 0.017070\n",
      "[033/00891] train_loss: 0.017627\n",
      "[033/00941] train_loss: 0.017026\n",
      "[033/00991] train_loss: 0.016752\n",
      "[033/01041] train_loss: 0.017681\n",
      "[033/01091] train_loss: 0.016810\n",
      "[033/01141] train_loss: 0.017006\n",
      "[033/01191] train_loss: 0.016721\n",
      "[034/00015] train_loss: 0.018564\n",
      "[034/00065] train_loss: 0.020333\n",
      "[034/00115] train_loss: 0.019240\n",
      "[034/00165] train_loss: 0.019259\n",
      "[034/00215] train_loss: 0.018563\n",
      "[034/00265] train_loss: 0.018688\n",
      "[034/00315] train_loss: 0.018799\n",
      "[034/00365] train_loss: 0.018203\n",
      "[034/00415] train_loss: 0.018083\n",
      "[034/00465] train_loss: 0.018043\n",
      "[034/00515] train_loss: 0.018107\n",
      "[034/00565] train_loss: 0.017100\n",
      "[034/00615] train_loss: 0.017263\n",
      "[034/00665] train_loss: 0.018050\n",
      "[034/00715] train_loss: 0.016962\n",
      "[034/00765] train_loss: 0.017122\n",
      "[034/00815] train_loss: 0.017353\n",
      "[034/00865] train_loss: 0.016676\n",
      "[034/00915] train_loss: 0.016732\n",
      "[034/00965] train_loss: 0.017093\n",
      "[034/01015] train_loss: 0.017493\n",
      "[034/01065] train_loss: 0.018048\n",
      "[034/01115] train_loss: 0.017507\n",
      "[034/01165] train_loss: 0.016845\n",
      "[034/01215] train_loss: 0.017235\n",
      "[035/00039] train_loss: 0.019535\n",
      "[035/00089] train_loss: 0.019755\n",
      "[035/00139] train_loss: 0.018829\n",
      "[035/00189] train_loss: 0.018137\n",
      "[035/00239] train_loss: 0.018212\n",
      "[035/00289] train_loss: 0.017924\n",
      "[035/00339] train_loss: 0.018331\n",
      "[035/00389] train_loss: 0.017752\n",
      "[035/00439] train_loss: 0.017069\n",
      "[035/00489] train_loss: 0.017345\n",
      "[035/00539] train_loss: 0.018040\n",
      "[035/00589] train_loss: 0.017133\n",
      "[035/00639] train_loss: 0.017224\n",
      "[035/00689] train_loss: 0.017120\n",
      "[035/00739] train_loss: 0.017390\n",
      "[035/00789] train_loss: 0.016721\n",
      "[035/00839] train_loss: 0.017701\n",
      "[035/00889] train_loss: 0.017391\n",
      "[035/00939] train_loss: 0.017053\n",
      "[035/00989] train_loss: 0.016963\n",
      "[035/01039] train_loss: 0.017044\n",
      "[035/01089] train_loss: 0.018373\n",
      "[035/01139] train_loss: 0.017129\n",
      "[035/01189] train_loss: 0.017112\n",
      "[036/00013] train_loss: 0.017178\n",
      "[036/00063] train_loss: 0.020077\n",
      "[036/00113] train_loss: 0.018432\n",
      "[036/00163] train_loss: 0.018841\n",
      "[036/00213] train_loss: 0.018185\n",
      "[036/00263] train_loss: 0.018798\n",
      "[036/00313] train_loss: 0.018244\n",
      "[036/00363] train_loss: 0.017327\n",
      "[036/00413] train_loss: 0.017355\n",
      "[036/00463] train_loss: 0.017785\n",
      "[036/00513] train_loss: 0.018423\n",
      "[036/00563] train_loss: 0.016753\n",
      "[036/00613] train_loss: 0.017052\n",
      "[036/00663] train_loss: 0.017165\n",
      "[036/00713] train_loss: 0.017954\n",
      "[036/00763] train_loss: 0.016549\n",
      "[036/00813] train_loss: 0.017053\n",
      "[036/00863] train_loss: 0.016761\n",
      "[036/00913] train_loss: 0.017328\n",
      "[036/00963] train_loss: 0.017654\n",
      "[036/01013] train_loss: 0.017313\n",
      "[036/01063] train_loss: 0.016180\n",
      "[036/01113] train_loss: 0.017068\n",
      "[036/01163] train_loss: 0.017340\n",
      "[036/01213] train_loss: 0.018004\n",
      "[037/00037] train_loss: 0.019610\n",
      "[037/00087] train_loss: 0.018556\n",
      "[037/00137] train_loss: 0.019318\n",
      "[037/00187] train_loss: 0.018277\n",
      "[037/00237] train_loss: 0.018071\n",
      "[037/00287] train_loss: 0.018676\n",
      "[037/00337] train_loss: 0.017844\n",
      "[037/00387] train_loss: 0.017702\n",
      "[037/00437] train_loss: 0.017906\n",
      "[037/00487] train_loss: 0.017823\n",
      "[037/00537] train_loss: 0.016852\n",
      "[037/00587] train_loss: 0.017129\n",
      "[037/00637] train_loss: 0.018038\n",
      "[037/00687] train_loss: 0.017245\n",
      "[037/00737] train_loss: 0.017096\n",
      "[037/00787] train_loss: 0.017785\n",
      "[037/00837] train_loss: 0.016090\n",
      "[037/00887] train_loss: 0.016680\n",
      "[037/00937] train_loss: 0.017776\n",
      "[037/00987] train_loss: 0.017338\n",
      "[037/01037] train_loss: 0.016924\n",
      "[037/01087] train_loss: 0.016236\n",
      "[037/01137] train_loss: 0.016453\n",
      "[037/01187] train_loss: 0.017057\n",
      "[038/00011] train_loss: 0.018212\n",
      "[038/00061] train_loss: 0.019830\n",
      "[038/00111] train_loss: 0.018939\n",
      "[038/00161] train_loss: 0.018949\n",
      "[038/00211] train_loss: 0.018674\n",
      "[038/00261] train_loss: 0.016905\n",
      "[038/00311] train_loss: 0.017676\n",
      "[038/00361] train_loss: 0.017624\n",
      "[038/00411] train_loss: 0.017373\n",
      "[038/00461] train_loss: 0.018194\n",
      "[038/00511] train_loss: 0.017360\n",
      "[038/00561] train_loss: 0.017145\n",
      "[038/00611] train_loss: 0.017357\n",
      "[038/00661] train_loss: 0.017737\n",
      "[038/00711] train_loss: 0.016219\n",
      "[038/00761] train_loss: 0.017000\n",
      "[038/00811] train_loss: 0.016942\n",
      "[038/00861] train_loss: 0.017501\n",
      "[038/00911] train_loss: 0.016251\n",
      "[038/00961] train_loss: 0.016511\n",
      "[038/01011] train_loss: 0.016869\n",
      "[038/01061] train_loss: 0.016777\n",
      "[038/01111] train_loss: 0.016968\n",
      "[038/01161] train_loss: 0.017320\n",
      "[038/01211] train_loss: 0.017402\n",
      "[039/00035] train_loss: 0.018738\n",
      "[039/00085] train_loss: 0.018845\n",
      "[039/00135] train_loss: 0.018049\n",
      "[039/00185] train_loss: 0.018895\n",
      "[039/00235] train_loss: 0.017046\n",
      "[039/00285] train_loss: 0.017281\n",
      "[039/00335] train_loss: 0.016757\n",
      "[039/00385] train_loss: 0.017869\n",
      "[039/00435] train_loss: 0.017796\n",
      "[039/00485] train_loss: 0.017355\n",
      "[039/00535] train_loss: 0.017083\n",
      "[039/00585] train_loss: 0.018115\n",
      "[039/00635] train_loss: 0.017867\n",
      "[039/00685] train_loss: 0.016819\n",
      "[039/00735] train_loss: 0.017000\n",
      "[039/00785] train_loss: 0.017154\n",
      "[039/00835] train_loss: 0.016512\n",
      "[039/00885] train_loss: 0.016722\n",
      "[039/00935] train_loss: 0.017215\n",
      "[039/00985] train_loss: 0.017452\n",
      "[039/01035] train_loss: 0.016604\n",
      "[039/01085] train_loss: 0.016240\n",
      "[039/01135] train_loss: 0.016249\n",
      "[039/01185] train_loss: 0.017857\n",
      "[040/00009] train_loss: 0.017646\n",
      "[040/00059] train_loss: 0.019088\n",
      "[040/00109] train_loss: 0.019290\n",
      "[040/00159] train_loss: 0.018670\n",
      "[040/00209] train_loss: 0.018770\n",
      "[040/00259] train_loss: 0.017680\n",
      "[040/00309] train_loss: 0.017481\n",
      "[040/00359] train_loss: 0.016850\n",
      "[040/00409] train_loss: 0.017677\n",
      "[040/00459] train_loss: 0.017276\n",
      "[040/00509] train_loss: 0.016633\n",
      "[040/00559] train_loss: 0.016133\n",
      "[040/00609] train_loss: 0.016205\n",
      "[040/00659] train_loss: 0.017650\n",
      "[040/00709] train_loss: 0.016955\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[040/00759] train_loss: 0.016628\n",
      "[040/00809] train_loss: 0.016853\n",
      "[040/00859] train_loss: 0.018051\n",
      "[040/00909] train_loss: 0.017245\n",
      "[040/00959] train_loss: 0.016799\n",
      "[040/01009] train_loss: 0.016650\n",
      "[040/01059] train_loss: 0.017068\n",
      "[040/01109] train_loss: 0.017819\n",
      "[040/01159] train_loss: 0.017021\n",
      "[040/01209] train_loss: 0.017265\n",
      "[041/00033] train_loss: 0.018832\n",
      "[041/00083] train_loss: 0.019232\n",
      "[041/00133] train_loss: 0.018337\n",
      "[041/00183] train_loss: 0.017370\n",
      "[041/00233] train_loss: 0.017461\n",
      "[041/00283] train_loss: 0.016829\n",
      "[041/00333] train_loss: 0.016659\n",
      "[041/00383] train_loss: 0.017297\n",
      "[041/00433] train_loss: 0.017042\n",
      "[041/00483] train_loss: 0.016251\n",
      "[041/00533] train_loss: 0.016541\n",
      "[041/00583] train_loss: 0.017254\n",
      "[041/00633] train_loss: 0.017636\n",
      "[041/00683] train_loss: 0.017025\n",
      "[041/00733] train_loss: 0.016611\n",
      "[041/00783] train_loss: 0.016526\n",
      "[041/00833] train_loss: 0.016563\n",
      "[041/00883] train_loss: 0.016698\n",
      "[041/00933] train_loss: 0.017461\n",
      "[041/00983] train_loss: 0.016618\n",
      "[041/01033] train_loss: 0.017178\n",
      "[041/01083] train_loss: 0.016936\n",
      "[041/01133] train_loss: 0.016543\n",
      "[041/01183] train_loss: 0.016719\n",
      "[042/00007] train_loss: 0.018399\n",
      "[042/00057] train_loss: 0.018609\n",
      "[042/00107] train_loss: 0.019631\n",
      "[042/00157] train_loss: 0.017637\n",
      "[042/00207] train_loss: 0.017673\n",
      "[042/00257] train_loss: 0.018171\n",
      "[042/00307] train_loss: 0.017283\n",
      "[042/00357] train_loss: 0.016392\n",
      "[042/00407] train_loss: 0.016952\n",
      "[042/00457] train_loss: 0.017106\n",
      "[042/00507] train_loss: 0.016586\n",
      "[042/00557] train_loss: 0.017618\n",
      "[042/00607] train_loss: 0.017092\n",
      "[042/00657] train_loss: 0.017231\n",
      "[042/00707] train_loss: 0.016601\n",
      "[042/00757] train_loss: 0.016305\n",
      "[042/00807] train_loss: 0.016211\n",
      "[042/00857] train_loss: 0.016804\n",
      "[042/00907] train_loss: 0.016387\n",
      "[042/00957] train_loss: 0.015587\n",
      "[042/01007] train_loss: 0.017241\n",
      "[042/01057] train_loss: 0.016300\n",
      "[042/01107] train_loss: 0.016499\n",
      "[042/01157] train_loss: 0.017107\n",
      "[042/01207] train_loss: 0.015674\n",
      "[043/00031] train_loss: 0.018635\n",
      "[043/00081] train_loss: 0.018304\n",
      "[043/00131] train_loss: 0.017529\n",
      "[043/00181] train_loss: 0.017620\n",
      "[043/00231] train_loss: 0.017244\n",
      "[043/00281] train_loss: 0.017686\n",
      "[043/00331] train_loss: 0.017897\n",
      "[043/00381] train_loss: 0.018071\n",
      "[043/00431] train_loss: 0.017112\n",
      "[043/00481] train_loss: 0.017023\n",
      "[043/00531] train_loss: 0.016598\n",
      "[043/00581] train_loss: 0.016102\n",
      "[043/00631] train_loss: 0.016900\n",
      "[043/00681] train_loss: 0.016801\n",
      "[043/00731] train_loss: 0.017469\n",
      "[043/00781] train_loss: 0.016894\n",
      "[043/00831] train_loss: 0.016309\n",
      "[043/00881] train_loss: 0.017291\n",
      "[043/00931] train_loss: 0.016078\n",
      "[043/00981] train_loss: 0.016943\n",
      "[043/01031] train_loss: 0.016916\n",
      "[043/01081] train_loss: 0.016369\n",
      "[043/01131] train_loss: 0.016222\n",
      "[043/01181] train_loss: 0.016751\n",
      "[044/00005] train_loss: 0.016809\n",
      "[044/00055] train_loss: 0.020065\n",
      "[044/00105] train_loss: 0.017954\n",
      "[044/00155] train_loss: 0.018559\n",
      "[044/00205] train_loss: 0.017313\n",
      "[044/00255] train_loss: 0.016760\n",
      "[044/00305] train_loss: 0.016606\n",
      "[044/00355] train_loss: 0.016872\n",
      "[044/00405] train_loss: 0.017064\n",
      "[044/00455] train_loss: 0.017138\n",
      "[044/00505] train_loss: 0.016077\n",
      "[044/00555] train_loss: 0.017900\n",
      "[044/00605] train_loss: 0.017109\n",
      "[044/00655] train_loss: 0.016101\n",
      "[044/00705] train_loss: 0.017679\n",
      "[044/00755] train_loss: 0.016839\n",
      "[044/00805] train_loss: 0.016645\n",
      "[044/00855] train_loss: 0.017137\n",
      "[044/00905] train_loss: 0.016271\n",
      "[044/00955] train_loss: 0.016621\n",
      "[044/01005] train_loss: 0.017321\n",
      "[044/01055] train_loss: 0.017032\n",
      "[044/01105] train_loss: 0.016753\n",
      "[044/01155] train_loss: 0.016609\n",
      "[044/01205] train_loss: 0.016045\n",
      "[045/00029] train_loss: 0.018372\n",
      "[045/00079] train_loss: 0.018579\n",
      "[045/00129] train_loss: 0.018096\n",
      "[045/00179] train_loss: 0.019195\n",
      "[045/00229] train_loss: 0.017399\n",
      "[045/00279] train_loss: 0.016260\n",
      "[045/00329] train_loss: 0.016800\n",
      "[045/00379] train_loss: 0.016497\n",
      "[045/00429] train_loss: 0.017941\n",
      "[045/00479] train_loss: 0.017085\n",
      "[045/00529] train_loss: 0.017196\n",
      "[045/00579] train_loss: 0.015960\n",
      "[045/00629] train_loss: 0.016084\n",
      "[045/00679] train_loss: 0.015855\n",
      "[045/00729] train_loss: 0.016112\n",
      "[045/00779] train_loss: 0.016986\n",
      "[045/00829] train_loss: 0.016435\n",
      "[045/00879] train_loss: 0.016069\n",
      "[045/00929] train_loss: 0.015935\n",
      "[045/00979] train_loss: 0.015866\n",
      "[045/01029] train_loss: 0.016609\n",
      "[045/01079] train_loss: 0.016539\n",
      "[045/01129] train_loss: 0.017004\n",
      "[045/01179] train_loss: 0.016328\n",
      "[046/00003] train_loss: 0.016556\n",
      "[046/00053] train_loss: 0.019999\n",
      "[046/00103] train_loss: 0.019116\n",
      "[046/00153] train_loss: 0.018217\n",
      "[046/00203] train_loss: 0.016869\n",
      "[046/00253] train_loss: 0.016837\n",
      "[046/00303] train_loss: 0.017232\n",
      "[046/00353] train_loss: 0.016412\n",
      "[046/00403] train_loss: 0.017555\n",
      "[046/00453] train_loss: 0.016334\n",
      "[046/00503] train_loss: 0.017118\n",
      "[046/00553] train_loss: 0.016412\n",
      "[046/00603] train_loss: 0.016390\n",
      "[046/00653] train_loss: 0.016787\n",
      "[046/00703] train_loss: 0.016761\n",
      "[046/00753] train_loss: 0.017302\n",
      "[046/00803] train_loss: 0.016092\n",
      "[046/00853] train_loss: 0.016186\n",
      "[046/00903] train_loss: 0.016819\n",
      "[046/00953] train_loss: 0.016919\n",
      "[046/01003] train_loss: 0.015799\n",
      "[046/01053] train_loss: 0.017166\n",
      "[046/01103] train_loss: 0.016462\n",
      "[046/01153] train_loss: 0.016627\n",
      "[046/01203] train_loss: 0.015860\n",
      "[047/00027] train_loss: 0.018499\n",
      "[047/00077] train_loss: 0.018255\n",
      "[047/00127] train_loss: 0.019108\n",
      "[047/00177] train_loss: 0.017746\n",
      "[047/00227] train_loss: 0.017566\n",
      "[047/00277] train_loss: 0.016762\n",
      "[047/00327] train_loss: 0.017054\n",
      "[047/00377] train_loss: 0.017747\n",
      "[047/00427] train_loss: 0.017386\n",
      "[047/00477] train_loss: 0.017118\n",
      "[047/00527] train_loss: 0.016767\n",
      "[047/00577] train_loss: 0.016431\n",
      "[047/00627] train_loss: 0.015949\n",
      "[047/00677] train_loss: 0.015124\n",
      "[047/00727] train_loss: 0.015912\n",
      "[047/00777] train_loss: 0.015923\n",
      "[047/00827] train_loss: 0.015902\n",
      "[047/00877] train_loss: 0.016349\n",
      "[047/00927] train_loss: 0.016401\n",
      "[047/00977] train_loss: 0.016608\n",
      "[047/01027] train_loss: 0.016654\n",
      "[047/01077] train_loss: 0.015963\n",
      "[047/01127] train_loss: 0.015745\n",
      "[047/01177] train_loss: 0.016211\n",
      "[048/00001] train_loss: 0.017531\n",
      "[048/00051] train_loss: 0.018249\n",
      "[048/00101] train_loss: 0.018035\n",
      "[048/00151] train_loss: 0.018144\n",
      "[048/00201] train_loss: 0.017041\n",
      "[048/00251] train_loss: 0.016704\n",
      "[048/00301] train_loss: 0.017376\n",
      "[048/00351] train_loss: 0.017638\n",
      "[048/00401] train_loss: 0.016594\n",
      "[048/00451] train_loss: 0.016973\n",
      "[048/00501] train_loss: 0.015723\n",
      "[048/00551] train_loss: 0.016934\n",
      "[048/00601] train_loss: 0.015830\n",
      "[048/00651] train_loss: 0.015818\n",
      "[048/00701] train_loss: 0.016613\n",
      "[048/00751] train_loss: 0.016265\n",
      "[048/00801] train_loss: 0.016915\n",
      "[048/00851] train_loss: 0.015832\n",
      "[048/00901] train_loss: 0.017187\n",
      "[048/00951] train_loss: 0.016164\n",
      "[048/01001] train_loss: 0.016018\n",
      "[048/01051] train_loss: 0.016497\n",
      "[048/01101] train_loss: 0.015955\n",
      "[048/01151] train_loss: 0.016144\n",
      "[048/01201] train_loss: 0.015657\n",
      "[049/00025] train_loss: 0.017061\n",
      "[049/00075] train_loss: 0.018957\n",
      "[049/00125] train_loss: 0.018233\n",
      "[049/00175] train_loss: 0.018599\n",
      "[049/00225] train_loss: 0.017868\n",
      "[049/00275] train_loss: 0.017728\n",
      "[049/00325] train_loss: 0.017238\n",
      "[049/00375] train_loss: 0.016386\n",
      "[049/00425] train_loss: 0.016751\n",
      "[049/00475] train_loss: 0.015660\n",
      "[049/00525] train_loss: 0.016577\n",
      "[049/00575] train_loss: 0.016299\n",
      "[049/00625] train_loss: 0.016298\n",
      "[049/00675] train_loss: 0.015693\n",
      "[049/00725] train_loss: 0.016540\n",
      "[049/00775] train_loss: 0.015864\n",
      "[049/00825] train_loss: 0.016411\n",
      "[049/00875] train_loss: 0.016444\n",
      "[049/00925] train_loss: 0.016582\n",
      "[049/00975] train_loss: 0.016929\n",
      "[049/01025] train_loss: 0.015932\n",
      "[049/01075] train_loss: 0.016213\n",
      "[049/01125] train_loss: 0.016225\n",
      "[049/01175] train_loss: 0.015984\n",
      "[049/01225] train_loss: 0.016051\n",
      "[050/00049] train_loss: 0.020818\n",
      "[050/00099] train_loss: 0.018351\n",
      "[050/00149] train_loss: 0.017370\n",
      "[050/00199] train_loss: 0.016465\n",
      "[050/00249] train_loss: 0.016504\n",
      "[050/00299] train_loss: 0.016058\n",
      "[050/00349] train_loss: 0.016709\n",
      "[050/00399] train_loss: 0.016659\n",
      "[050/00449] train_loss: 0.017061\n",
      "[050/00499] train_loss: 0.016369\n",
      "[050/00549] train_loss: 0.015701\n",
      "[050/00599] train_loss: 0.017341\n",
      "[050/00649] train_loss: 0.017518\n",
      "[050/00699] train_loss: 0.015252\n",
      "[050/00749] train_loss: 0.016813\n",
      "[050/00799] train_loss: 0.017281\n",
      "[050/00849] train_loss: 0.016805\n",
      "[050/00899] train_loss: 0.016393\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[050/00949] train_loss: 0.015100\n",
      "[050/00999] train_loss: 0.016149\n",
      "[050/01049] train_loss: 0.015254\n",
      "[050/01099] train_loss: 0.015815\n",
      "[050/01149] train_loss: 0.016679\n",
      "[050/01199] train_loss: 0.015972\n",
      "[051/00023] train_loss: 0.018114\n",
      "[051/00073] train_loss: 0.017840\n",
      "[051/00123] train_loss: 0.017851\n",
      "[051/00173] train_loss: 0.018358\n",
      "[051/00223] train_loss: 0.016292\n",
      "[051/00273] train_loss: 0.016918\n",
      "[051/00323] train_loss: 0.015937\n",
      "[051/00373] train_loss: 0.016632\n",
      "[051/00423] train_loss: 0.015668\n",
      "[051/00473] train_loss: 0.015985\n",
      "[051/00523] train_loss: 0.016617\n",
      "[051/00573] train_loss: 0.016472\n",
      "[051/00623] train_loss: 0.017008\n",
      "[051/00673] train_loss: 0.016145\n",
      "[051/00723] train_loss: 0.015420\n",
      "[051/00773] train_loss: 0.016512\n",
      "[051/00823] train_loss: 0.015528\n",
      "[051/00873] train_loss: 0.016181\n",
      "[051/00923] train_loss: 0.016107\n",
      "[051/00973] train_loss: 0.016821\n",
      "[051/01023] train_loss: 0.015783\n",
      "[051/01073] train_loss: 0.016864\n",
      "[051/01123] train_loss: 0.016538\n",
      "[051/01173] train_loss: 0.016569\n",
      "[051/01223] train_loss: 0.016331\n",
      "[052/00047] train_loss: 0.018826\n",
      "[052/00097] train_loss: 0.018279\n",
      "[052/00147] train_loss: 0.017611\n",
      "[052/00197] train_loss: 0.018015\n",
      "[052/00247] train_loss: 0.016781\n",
      "[052/00297] train_loss: 0.016350\n",
      "[052/00347] train_loss: 0.016870\n",
      "[052/00397] train_loss: 0.016089\n",
      "[052/00447] train_loss: 0.016563\n",
      "[052/00497] train_loss: 0.017493\n",
      "[052/00547] train_loss: 0.016697\n",
      "[052/00597] train_loss: 0.016738\n",
      "[052/00647] train_loss: 0.015381\n",
      "[052/00697] train_loss: 0.016495\n",
      "[052/00747] train_loss: 0.015926\n",
      "[052/00797] train_loss: 0.015811\n",
      "[052/00847] train_loss: 0.016928\n",
      "[052/00897] train_loss: 0.017132\n",
      "[052/00947] train_loss: 0.015815\n",
      "[052/00997] train_loss: 0.015867\n",
      "[052/01047] train_loss: 0.015978\n",
      "[052/01097] train_loss: 0.015828\n",
      "[052/01147] train_loss: 0.016044\n",
      "[052/01197] train_loss: 0.016331\n",
      "[053/00021] train_loss: 0.016379\n",
      "[053/00071] train_loss: 0.018125\n",
      "[053/00121] train_loss: 0.018862\n",
      "[053/00171] train_loss: 0.017130\n",
      "[053/00221] train_loss: 0.018290\n",
      "[053/00271] train_loss: 0.017275\n",
      "[053/00321] train_loss: 0.016658\n",
      "[053/00371] train_loss: 0.016213\n",
      "[053/00421] train_loss: 0.016123\n",
      "[053/00471] train_loss: 0.016379\n",
      "[053/00521] train_loss: 0.016885\n",
      "[053/00571] train_loss: 0.016015\n",
      "[053/00621] train_loss: 0.017026\n",
      "[053/00671] train_loss: 0.017025\n",
      "[053/00721] train_loss: 0.015980\n",
      "[053/00771] train_loss: 0.015853\n",
      "[053/00821] train_loss: 0.015965\n",
      "[053/00871] train_loss: 0.015269\n",
      "[053/00921] train_loss: 0.016598\n",
      "[053/00971] train_loss: 0.016214\n",
      "[053/01021] train_loss: 0.015760\n",
      "[053/01071] train_loss: 0.015488\n",
      "[053/01121] train_loss: 0.015059\n",
      "[053/01171] train_loss: 0.015440\n",
      "[053/01221] train_loss: 0.015717\n",
      "[054/00045] train_loss: 0.018694\n",
      "[054/00095] train_loss: 0.017435\n",
      "[054/00145] train_loss: 0.017160\n",
      "[054/00195] train_loss: 0.016742\n",
      "[054/00245] train_loss: 0.017190\n",
      "[054/00295] train_loss: 0.015963\n",
      "[054/00345] train_loss: 0.016448\n",
      "[054/00395] train_loss: 0.015758\n",
      "[054/00445] train_loss: 0.016623\n",
      "[054/00495] train_loss: 0.015450\n",
      "[054/00545] train_loss: 0.016598\n",
      "[054/00595] train_loss: 0.015534\n",
      "[054/00645] train_loss: 0.015889\n",
      "[054/00695] train_loss: 0.015332\n",
      "[054/00745] train_loss: 0.016026\n",
      "[054/00795] train_loss: 0.015593\n",
      "[054/00845] train_loss: 0.015907\n",
      "[054/00895] train_loss: 0.015996\n",
      "[054/00945] train_loss: 0.016481\n",
      "[054/00995] train_loss: 0.016386\n",
      "[054/01045] train_loss: 0.016470\n",
      "[054/01095] train_loss: 0.016468\n",
      "[054/01145] train_loss: 0.016631\n",
      "[054/01195] train_loss: 0.016447\n",
      "[055/00019] train_loss: 0.016931\n",
      "[055/00069] train_loss: 0.018977\n",
      "[055/00119] train_loss: 0.018109\n",
      "[055/00169] train_loss: 0.017665\n",
      "[055/00219] train_loss: 0.016338\n",
      "[055/00269] train_loss: 0.016738\n",
      "[055/00319] train_loss: 0.016407\n",
      "[055/00369] train_loss: 0.015511\n",
      "[055/00419] train_loss: 0.016792\n",
      "[055/00469] train_loss: 0.016726\n",
      "[055/00519] train_loss: 0.016482\n",
      "[055/00569] train_loss: 0.015858\n",
      "[055/00619] train_loss: 0.016973\n",
      "[055/00669] train_loss: 0.016467\n",
      "[055/00719] train_loss: 0.016875\n",
      "[055/00769] train_loss: 0.016002\n",
      "[055/00819] train_loss: 0.016299\n",
      "[055/00869] train_loss: 0.014930\n",
      "[055/00919] train_loss: 0.015387\n",
      "[055/00969] train_loss: 0.015139\n",
      "[055/01019] train_loss: 0.016196\n",
      "[055/01069] train_loss: 0.015475\n",
      "[055/01119] train_loss: 0.016108\n",
      "[055/01169] train_loss: 0.016059\n",
      "[055/01219] train_loss: 0.015586\n",
      "[056/00043] train_loss: 0.017558\n",
      "[056/00093] train_loss: 0.017478\n",
      "[056/00143] train_loss: 0.016715\n",
      "[056/00193] train_loss: 0.017037\n",
      "[056/00243] train_loss: 0.017398\n",
      "[056/00293] train_loss: 0.016793\n",
      "[056/00343] train_loss: 0.015733\n",
      "[056/00393] train_loss: 0.016541\n",
      "[056/00443] train_loss: 0.016124\n",
      "[056/00493] train_loss: 0.016939\n",
      "[056/00543] train_loss: 0.016251\n",
      "[056/00593] train_loss: 0.015920\n",
      "[056/00643] train_loss: 0.015727\n",
      "[056/00693] train_loss: 0.016199\n",
      "[056/00743] train_loss: 0.015915\n",
      "[056/00793] train_loss: 0.016181\n",
      "[056/00843] train_loss: 0.015466\n",
      "[056/00893] train_loss: 0.016836\n",
      "[056/00943] train_loss: 0.015374\n",
      "[056/00993] train_loss: 0.015156\n",
      "[056/01043] train_loss: 0.015278\n",
      "[056/01093] train_loss: 0.016708\n",
      "[056/01143] train_loss: 0.015682\n",
      "[056/01193] train_loss: 0.015222\n",
      "[057/00017] train_loss: 0.016936\n",
      "[057/00067] train_loss: 0.018070\n",
      "[057/00117] train_loss: 0.017916\n",
      "[057/00167] train_loss: 0.016872\n",
      "[057/00217] train_loss: 0.016538\n",
      "[057/00267] train_loss: 0.016891\n",
      "[057/00317] train_loss: 0.016190\n",
      "[057/00367] train_loss: 0.015818\n",
      "[057/00417] train_loss: 0.016340\n",
      "[057/00467] train_loss: 0.016264\n",
      "[057/00517] train_loss: 0.016790\n",
      "[057/00567] train_loss: 0.016383\n",
      "[057/00617] train_loss: 0.015379\n",
      "[057/00667] train_loss: 0.016390\n",
      "[057/00717] train_loss: 0.016351\n",
      "[057/00767] train_loss: 0.015702\n",
      "[057/00817] train_loss: 0.015467\n",
      "[057/00867] train_loss: 0.016714\n",
      "[057/00917] train_loss: 0.016280\n",
      "[057/00967] train_loss: 0.015386\n",
      "[057/01017] train_loss: 0.015964\n",
      "[057/01067] train_loss: 0.015340\n",
      "[057/01117] train_loss: 0.014904\n",
      "[057/01167] train_loss: 0.015922\n",
      "[057/01217] train_loss: 0.016712\n",
      "[058/00041] train_loss: 0.017934\n",
      "[058/00091] train_loss: 0.018045\n",
      "[058/00141] train_loss: 0.016759\n",
      "[058/00191] train_loss: 0.016109\n",
      "[058/00241] train_loss: 0.016726\n",
      "[058/00291] train_loss: 0.017242\n",
      "[058/00341] train_loss: 0.016255\n",
      "[058/00391] train_loss: 0.015777\n",
      "[058/00441] train_loss: 0.015954\n",
      "[058/00491] train_loss: 0.015647\n",
      "[058/00541] train_loss: 0.016274\n",
      "[058/00591] train_loss: 0.015600\n",
      "[058/00641] train_loss: 0.014563\n",
      "[058/00691] train_loss: 0.015808\n",
      "[058/00741] train_loss: 0.017105\n",
      "[058/00791] train_loss: 0.016146\n",
      "[058/00841] train_loss: 0.016085\n",
      "[058/00891] train_loss: 0.015883\n",
      "[058/00941] train_loss: 0.016289\n",
      "[058/00991] train_loss: 0.015750\n",
      "[058/01041] train_loss: 0.015915\n",
      "[058/01091] train_loss: 0.015995\n",
      "[058/01141] train_loss: 0.016108\n",
      "[058/01191] train_loss: 0.015399\n",
      "[059/00015] train_loss: 0.017046\n",
      "[059/00065] train_loss: 0.018348\n",
      "[059/00115] train_loss: 0.017366\n",
      "[059/00165] train_loss: 0.017099\n",
      "[059/00215] train_loss: 0.017097\n",
      "[059/00265] train_loss: 0.016464\n",
      "[059/00315] train_loss: 0.016534\n",
      "[059/00365] train_loss: 0.015617\n",
      "[059/00415] train_loss: 0.016604\n",
      "[059/00465] train_loss: 0.016172\n",
      "[059/00515] train_loss: 0.015758\n",
      "[059/00565] train_loss: 0.015540\n",
      "[059/00615] train_loss: 0.015698\n",
      "[059/00665] train_loss: 0.015364\n",
      "[059/00715] train_loss: 0.015724\n",
      "[059/00765] train_loss: 0.015628\n",
      "[059/00815] train_loss: 0.016063\n",
      "[059/00865] train_loss: 0.015849\n",
      "[059/00915] train_loss: 0.015574\n",
      "[059/00965] train_loss: 0.016433\n",
      "[059/01015] train_loss: 0.014834\n",
      "[059/01065] train_loss: 0.016514\n",
      "[059/01115] train_loss: 0.016645\n",
      "[059/01165] train_loss: 0.015591\n",
      "[059/01215] train_loss: 0.016126\n",
      "[060/00039] train_loss: 0.018729\n",
      "[060/00089] train_loss: 0.018220\n",
      "[060/00139] train_loss: 0.016333\n",
      "[060/00189] train_loss: 0.017210\n",
      "[060/00239] train_loss: 0.016617\n",
      "[060/00289] train_loss: 0.016775\n",
      "[060/00339] train_loss: 0.015392\n",
      "[060/00389] train_loss: 0.015687\n",
      "[060/00439] train_loss: 0.015617\n",
      "[060/00489] train_loss: 0.015771\n",
      "[060/00539] train_loss: 0.015710\n",
      "[060/00589] train_loss: 0.015413\n",
      "[060/00639] train_loss: 0.016086\n",
      "[060/00689] train_loss: 0.015014\n",
      "[060/00739] train_loss: 0.015118\n",
      "[060/00789] train_loss: 0.016428\n",
      "[060/00839] train_loss: 0.016377\n",
      "[060/00889] train_loss: 0.014621\n",
      "[060/00939] train_loss: 0.016490\n",
      "[060/00989] train_loss: 0.015827\n",
      "[060/01039] train_loss: 0.015692\n",
      "[060/01089] train_loss: 0.016055\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[060/01139] train_loss: 0.015853\n",
      "[060/01189] train_loss: 0.016023\n",
      "[061/00013] train_loss: 0.016305\n",
      "[061/00063] train_loss: 0.018121\n",
      "[061/00113] train_loss: 0.017097\n",
      "[061/00163] train_loss: 0.016600\n",
      "[061/00213] train_loss: 0.016876\n",
      "[061/00263] train_loss: 0.017033\n",
      "[061/00313] train_loss: 0.016472\n",
      "[061/00363] train_loss: 0.015692\n",
      "[061/00413] train_loss: 0.016188\n",
      "[061/00463] train_loss: 0.015494\n",
      "[061/00513] train_loss: 0.016111\n",
      "[061/00563] train_loss: 0.015414\n",
      "[061/00613] train_loss: 0.015501\n",
      "[061/00663] train_loss: 0.015380\n",
      "[061/00713] train_loss: 0.015581\n",
      "[061/00763] train_loss: 0.015576\n",
      "[061/00813] train_loss: 0.015878\n",
      "[061/00863] train_loss: 0.014899\n",
      "[061/00913] train_loss: 0.016220\n",
      "[061/00963] train_loss: 0.015771\n",
      "[061/01013] train_loss: 0.014545\n",
      "[061/01063] train_loss: 0.015034\n",
      "[061/01113] train_loss: 0.015091\n",
      "[061/01163] train_loss: 0.015963\n",
      "[061/01213] train_loss: 0.015562\n",
      "[062/00037] train_loss: 0.017200\n",
      "[062/00087] train_loss: 0.017531\n",
      "[062/00137] train_loss: 0.017012\n",
      "[062/00187] train_loss: 0.015852\n",
      "[062/00237] train_loss: 0.016631\n",
      "[062/00287] train_loss: 0.016064\n",
      "[062/00337] train_loss: 0.015692\n",
      "[062/00387] train_loss: 0.015613\n",
      "[062/00437] train_loss: 0.016696\n",
      "[062/00487] train_loss: 0.016388\n",
      "[062/00537] train_loss: 0.016220\n",
      "[062/00587] train_loss: 0.015767\n",
      "[062/00637] train_loss: 0.016067\n",
      "[062/00687] train_loss: 0.015568\n",
      "[062/00737] train_loss: 0.016362\n",
      "[062/00787] train_loss: 0.015388\n",
      "[062/00837] train_loss: 0.015973\n",
      "[062/00887] train_loss: 0.015961\n",
      "[062/00937] train_loss: 0.015506\n",
      "[062/00987] train_loss: 0.016205\n",
      "[062/01037] train_loss: 0.015383\n",
      "[062/01087] train_loss: 0.015506\n",
      "[062/01137] train_loss: 0.015444\n",
      "[062/01187] train_loss: 0.015827\n",
      "[063/00011] train_loss: 0.016542\n",
      "[063/00061] train_loss: 0.017897\n",
      "[063/00111] train_loss: 0.016865\n",
      "[063/00161] train_loss: 0.016885\n",
      "[063/00211] train_loss: 0.016725\n",
      "[063/00261] train_loss: 0.016028\n",
      "[063/00311] train_loss: 0.016299\n",
      "[063/00361] train_loss: 0.017094\n",
      "[063/00411] train_loss: 0.015000\n",
      "[063/00461] train_loss: 0.016254\n",
      "[063/00511] train_loss: 0.015287\n",
      "[063/00561] train_loss: 0.016377\n",
      "[063/00611] train_loss: 0.016129\n",
      "[063/00661] train_loss: 0.015836\n",
      "[063/00711] train_loss: 0.015656\n",
      "[063/00761] train_loss: 0.015880\n",
      "[063/00811] train_loss: 0.015363\n",
      "[063/00861] train_loss: 0.015166\n",
      "[063/00911] train_loss: 0.014862\n",
      "[063/00961] train_loss: 0.015448\n",
      "[063/01011] train_loss: 0.015267\n",
      "[063/01061] train_loss: 0.015045\n",
      "[063/01111] train_loss: 0.015632\n",
      "[063/01161] train_loss: 0.016559\n",
      "[063/01211] train_loss: 0.014946\n",
      "[064/00035] train_loss: 0.017535\n",
      "[064/00085] train_loss: 0.016133\n",
      "[064/00135] train_loss: 0.017060\n",
      "[064/00185] train_loss: 0.016431\n",
      "[064/00235] train_loss: 0.016310\n",
      "[064/00285] train_loss: 0.016566\n",
      "[064/00335] train_loss: 0.015272\n",
      "[064/00385] train_loss: 0.016025\n",
      "[064/00435] train_loss: 0.016395\n",
      "[064/00485] train_loss: 0.016257\n",
      "[064/00535] train_loss: 0.015425\n",
      "[064/00585] train_loss: 0.015113\n",
      "[064/00635] train_loss: 0.015482\n",
      "[064/00685] train_loss: 0.015018\n",
      "[064/00735] train_loss: 0.015609\n",
      "[064/00785] train_loss: 0.014737\n",
      "[064/00835] train_loss: 0.015971\n",
      "[064/00885] train_loss: 0.015197\n",
      "[064/00935] train_loss: 0.015807\n",
      "[064/00985] train_loss: 0.015627\n",
      "[064/01035] train_loss: 0.014763\n",
      "[064/01085] train_loss: 0.015564\n",
      "[064/01135] train_loss: 0.015762\n",
      "[064/01185] train_loss: 0.015808\n",
      "[065/00009] train_loss: 0.016911\n",
      "[065/00059] train_loss: 0.017944\n",
      "[065/00109] train_loss: 0.017546\n",
      "[065/00159] train_loss: 0.016471\n",
      "[065/00209] train_loss: 0.015854\n",
      "[065/00259] train_loss: 0.015811\n",
      "[065/00309] train_loss: 0.016028\n",
      "[065/00359] train_loss: 0.016059\n",
      "[065/00409] train_loss: 0.014948\n",
      "[065/00459] train_loss: 0.015728\n",
      "[065/00509] train_loss: 0.015823\n",
      "[065/00559] train_loss: 0.015231\n",
      "[065/00609] train_loss: 0.016233\n",
      "[065/00659] train_loss: 0.015544\n",
      "[065/00709] train_loss: 0.015240\n",
      "[065/00759] train_loss: 0.016389\n",
      "[065/00809] train_loss: 0.014957\n",
      "[065/00859] train_loss: 0.015456\n",
      "[065/00909] train_loss: 0.015957\n",
      "[065/00959] train_loss: 0.014940\n",
      "[065/01009] train_loss: 0.015913\n",
      "[065/01059] train_loss: 0.015086\n",
      "[065/01109] train_loss: 0.015924\n",
      "[065/01159] train_loss: 0.015787\n",
      "[065/01209] train_loss: 0.015166\n",
      "[066/00033] train_loss: 0.017491\n",
      "[066/00083] train_loss: 0.017223\n",
      "[066/00133] train_loss: 0.017492\n",
      "[066/00183] train_loss: 0.015813\n",
      "[066/00233] train_loss: 0.016255\n",
      "[066/00283] train_loss: 0.016058\n",
      "[066/00333] train_loss: 0.015964\n",
      "[066/00383] train_loss: 0.016969\n",
      "[066/00433] train_loss: 0.015904\n",
      "[066/00483] train_loss: 0.015762\n",
      "[066/00533] train_loss: 0.015555\n",
      "[066/00583] train_loss: 0.015499\n",
      "[066/00633] train_loss: 0.015701\n",
      "[066/00683] train_loss: 0.015754\n",
      "[066/00733] train_loss: 0.015598\n",
      "[066/00783] train_loss: 0.015303\n",
      "[066/00833] train_loss: 0.014991\n",
      "[066/00883] train_loss: 0.015120\n",
      "[066/00933] train_loss: 0.015265\n",
      "[066/00983] train_loss: 0.015032\n",
      "[066/01033] train_loss: 0.015488\n",
      "[066/01083] train_loss: 0.015167\n",
      "[066/01133] train_loss: 0.015051\n",
      "[066/01183] train_loss: 0.015214\n",
      "[067/00007] train_loss: 0.015749\n",
      "[067/00057] train_loss: 0.017312\n",
      "[067/00107] train_loss: 0.017584\n",
      "[067/00157] train_loss: 0.016605\n",
      "[067/00207] train_loss: 0.015949\n",
      "[067/00257] train_loss: 0.015758\n",
      "[067/00307] train_loss: 0.017319\n",
      "[067/00357] train_loss: 0.016103\n",
      "[067/00407] train_loss: 0.015422\n",
      "[067/00457] train_loss: 0.016152\n",
      "[067/00507] train_loss: 0.016695\n",
      "[067/00557] train_loss: 0.015019\n",
      "[067/00607] train_loss: 0.013772\n",
      "[067/00657] train_loss: 0.016243\n",
      "[067/00707] train_loss: 0.015056\n",
      "[067/00757] train_loss: 0.015069\n",
      "[067/00807] train_loss: 0.015171\n",
      "[067/00857] train_loss: 0.015441\n",
      "[067/00907] train_loss: 0.015716\n",
      "[067/00957] train_loss: 0.015505\n",
      "[067/01007] train_loss: 0.015533\n",
      "[067/01057] train_loss: 0.015153\n",
      "[067/01107] train_loss: 0.015050\n",
      "[067/01157] train_loss: 0.015792\n",
      "[067/01207] train_loss: 0.016068\n",
      "[068/00031] train_loss: 0.015962\n",
      "[068/00081] train_loss: 0.018093\n",
      "[068/00131] train_loss: 0.017593\n",
      "[068/00181] train_loss: 0.016593\n",
      "[068/00231] train_loss: 0.016187\n",
      "[068/00281] train_loss: 0.016127\n",
      "[068/00331] train_loss: 0.015736\n",
      "[068/00381] train_loss: 0.014994\n",
      "[068/00431] train_loss: 0.015830\n",
      "[068/00481] train_loss: 0.015520\n",
      "[068/00531] train_loss: 0.014949\n",
      "[068/00581] train_loss: 0.015505\n",
      "[068/00631] train_loss: 0.016346\n",
      "[068/00681] train_loss: 0.014811\n",
      "[068/00731] train_loss: 0.015196\n",
      "[068/00781] train_loss: 0.015109\n",
      "[068/00831] train_loss: 0.015154\n",
      "[068/00881] train_loss: 0.015256\n",
      "[068/00931] train_loss: 0.015452\n",
      "[068/00981] train_loss: 0.013876\n",
      "[068/01031] train_loss: 0.015821\n",
      "[068/01081] train_loss: 0.015867\n",
      "[068/01131] train_loss: 0.014390\n",
      "[068/01181] train_loss: 0.014711\n",
      "[069/00005] train_loss: 0.015948\n",
      "[069/00055] train_loss: 0.018192\n",
      "[069/00105] train_loss: 0.017542\n",
      "[069/00155] train_loss: 0.016835\n",
      "[069/00205] train_loss: 0.016312\n",
      "[069/00255] train_loss: 0.015208\n",
      "[069/00305] train_loss: 0.015188\n",
      "[069/00355] train_loss: 0.016610\n",
      "[069/00405] train_loss: 0.015623\n",
      "[069/00455] train_loss: 0.015486\n",
      "[069/00505] train_loss: 0.015896\n",
      "[069/00555] train_loss: 0.016281\n",
      "[069/00605] train_loss: 0.015977\n",
      "[069/00655] train_loss: 0.015407\n",
      "[069/00705] train_loss: 0.014816\n",
      "[069/00755] train_loss: 0.015564\n",
      "[069/00805] train_loss: 0.015017\n",
      "[069/00855] train_loss: 0.014956\n",
      "[069/00905] train_loss: 0.014797\n",
      "[069/00955] train_loss: 0.015707\n",
      "[069/01005] train_loss: 0.015984\n",
      "[069/01055] train_loss: 0.015324\n",
      "[069/01105] train_loss: 0.015184\n",
      "[069/01155] train_loss: 0.015180\n",
      "[069/01205] train_loss: 0.015352\n",
      "[070/00029] train_loss: 0.017801\n",
      "[070/00079] train_loss: 0.016633\n",
      "[070/00129] train_loss: 0.017193\n",
      "[070/00179] train_loss: 0.015457\n",
      "[070/00229] train_loss: 0.015997\n",
      "[070/00279] train_loss: 0.015863\n",
      "[070/00329] train_loss: 0.015002\n",
      "[070/00379] train_loss: 0.016203\n",
      "[070/00429] train_loss: 0.016019\n",
      "[070/00479] train_loss: 0.015643\n",
      "[070/00529] train_loss: 0.015573\n",
      "[070/00579] train_loss: 0.014978\n",
      "[070/00629] train_loss: 0.015327\n",
      "[070/00679] train_loss: 0.016102\n",
      "[070/00729] train_loss: 0.015092\n",
      "[070/00779] train_loss: 0.014517\n",
      "[070/00829] train_loss: 0.015082\n",
      "[070/00879] train_loss: 0.015417\n",
      "[070/00929] train_loss: 0.015503\n",
      "[070/00979] train_loss: 0.014947\n",
      "[070/01029] train_loss: 0.015309\n",
      "[070/01079] train_loss: 0.014586\n",
      "[070/01129] train_loss: 0.014538\n",
      "[070/01179] train_loss: 0.014837\n",
      "[071/00003] train_loss: 0.015215\n",
      "[071/00053] train_loss: 0.017691\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[071/00103] train_loss: 0.017540\n",
      "[071/00153] train_loss: 0.016523\n",
      "[071/00203] train_loss: 0.016149\n",
      "[071/00253] train_loss: 0.016573\n",
      "[071/00303] train_loss: 0.015376\n",
      "[071/00353] train_loss: 0.015529\n",
      "[071/00403] train_loss: 0.015297\n",
      "[071/00453] train_loss: 0.014747\n",
      "[071/00503] train_loss: 0.014817\n",
      "[071/00553] train_loss: 0.015800\n",
      "[071/00603] train_loss: 0.016313\n",
      "[071/00653] train_loss: 0.015626\n",
      "[071/00703] train_loss: 0.015784\n",
      "[071/00753] train_loss: 0.015365\n",
      "[071/00803] train_loss: 0.014969\n",
      "[071/00853] train_loss: 0.016174\n",
      "[071/00903] train_loss: 0.014382\n",
      "[071/00953] train_loss: 0.015584\n",
      "[071/01003] train_loss: 0.015474\n",
      "[071/01053] train_loss: 0.015237\n",
      "[071/01103] train_loss: 0.015576\n",
      "[071/01153] train_loss: 0.015269\n",
      "[071/01203] train_loss: 0.015198\n",
      "[072/00027] train_loss: 0.016030\n",
      "[072/00077] train_loss: 0.016473\n",
      "[072/00127] train_loss: 0.016948\n",
      "[072/00177] train_loss: 0.016761\n",
      "[072/00227] train_loss: 0.016079\n",
      "[072/00277] train_loss: 0.015127\n",
      "[072/00327] train_loss: 0.015069\n",
      "[072/00377] train_loss: 0.015413\n",
      "[072/00427] train_loss: 0.015013\n",
      "[072/00477] train_loss: 0.014862\n",
      "[072/00527] train_loss: 0.016078\n",
      "[072/00577] train_loss: 0.015012\n",
      "[072/00627] train_loss: 0.016375\n",
      "[072/00677] train_loss: 0.015300\n",
      "[072/00727] train_loss: 0.015370\n",
      "[072/00777] train_loss: 0.015496\n",
      "[072/00827] train_loss: 0.015449\n",
      "[072/00877] train_loss: 0.015702\n",
      "[072/00927] train_loss: 0.015390\n",
      "[072/00977] train_loss: 0.015183\n",
      "[072/01027] train_loss: 0.015280\n",
      "[072/01077] train_loss: 0.015439\n",
      "[072/01127] train_loss: 0.014886\n",
      "[072/01177] train_loss: 0.014579\n",
      "[073/00001] train_loss: 0.015499\n",
      "[073/00051] train_loss: 0.017037\n",
      "[073/00101] train_loss: 0.016299\n",
      "[073/00151] train_loss: 0.016236\n",
      "[073/00201] train_loss: 0.015946\n",
      "[073/00251] train_loss: 0.015785\n",
      "[073/00301] train_loss: 0.015428\n",
      "[073/00351] train_loss: 0.015746\n",
      "[073/00401] train_loss: 0.014929\n",
      "[073/00451] train_loss: 0.016374\n",
      "[073/00501] train_loss: 0.015601\n",
      "[073/00551] train_loss: 0.016003\n",
      "[073/00601] train_loss: 0.015609\n",
      "[073/00651] train_loss: 0.015604\n",
      "[073/00701] train_loss: 0.015836\n",
      "[073/00751] train_loss: 0.014573\n",
      "[073/00801] train_loss: 0.015453\n",
      "[073/00851] train_loss: 0.015692\n",
      "[073/00901] train_loss: 0.016204\n",
      "[073/00951] train_loss: 0.015322\n",
      "[073/01001] train_loss: 0.015093\n",
      "[073/01051] train_loss: 0.015400\n",
      "[073/01101] train_loss: 0.015497\n",
      "[073/01151] train_loss: 0.015062\n",
      "[073/01201] train_loss: 0.015169\n",
      "[074/00025] train_loss: 0.016206\n",
      "[074/00075] train_loss: 0.017127\n",
      "[074/00125] train_loss: 0.016381\n",
      "[074/00175] train_loss: 0.017064\n",
      "[074/00225] train_loss: 0.016348\n",
      "[074/00275] train_loss: 0.015773\n",
      "[074/00325] train_loss: 0.015762\n",
      "[074/00375] train_loss: 0.015451\n",
      "[074/00425] train_loss: 0.015323\n",
      "[074/00475] train_loss: 0.015082\n",
      "[074/00525] train_loss: 0.014262\n",
      "[074/00575] train_loss: 0.014994\n",
      "[074/00625] train_loss: 0.015569\n",
      "[074/00675] train_loss: 0.015111\n",
      "[074/00725] train_loss: 0.014796\n",
      "[074/00775] train_loss: 0.015427\n",
      "[074/00825] train_loss: 0.015089\n",
      "[074/00875] train_loss: 0.014985\n",
      "[074/00925] train_loss: 0.014428\n",
      "[074/00975] train_loss: 0.015068\n",
      "[074/01025] train_loss: 0.015382\n",
      "[074/01075] train_loss: 0.015732\n",
      "[074/01125] train_loss: 0.015322\n",
      "[074/01175] train_loss: 0.015991\n",
      "[074/01225] train_loss: 0.015157\n",
      "[075/00049] train_loss: 0.017557\n",
      "[075/00099] train_loss: 0.016515\n",
      "[075/00149] train_loss: 0.016645\n",
      "[075/00199] train_loss: 0.016145\n",
      "[075/00249] train_loss: 0.015569\n",
      "[075/00299] train_loss: 0.014242\n",
      "[075/00349] train_loss: 0.016234\n",
      "[075/00399] train_loss: 0.016988\n",
      "[075/00449] train_loss: 0.015513\n",
      "[075/00499] train_loss: 0.015077\n",
      "[075/00549] train_loss: 0.015637\n",
      "[075/00599] train_loss: 0.015882\n",
      "[075/00649] train_loss: 0.014620\n",
      "[075/00699] train_loss: 0.015383\n",
      "[075/00749] train_loss: 0.014057\n",
      "[075/00799] train_loss: 0.015194\n",
      "[075/00849] train_loss: 0.014512\n",
      "[075/00899] train_loss: 0.014769\n",
      "[075/00949] train_loss: 0.015174\n",
      "[075/00999] train_loss: 0.014906\n",
      "[075/01049] train_loss: 0.015736\n",
      "[075/01099] train_loss: 0.015712\n",
      "[075/01149] train_loss: 0.014745\n",
      "[075/01199] train_loss: 0.014225\n",
      "[076/00023] train_loss: 0.016253\n",
      "[076/00073] train_loss: 0.017219\n",
      "[076/00123] train_loss: 0.015963\n",
      "[076/00173] train_loss: 0.015753\n",
      "[076/00223] train_loss: 0.016535\n",
      "[076/00273] train_loss: 0.015935\n",
      "[076/00323] train_loss: 0.015320\n",
      "[076/00373] train_loss: 0.015016\n",
      "[076/00423] train_loss: 0.016201\n",
      "[076/00473] train_loss: 0.015075\n",
      "[076/00523] train_loss: 0.014813\n",
      "[076/00573] train_loss: 0.015608\n",
      "[076/00623] train_loss: 0.014980\n",
      "[076/00673] train_loss: 0.015751\n",
      "[076/00723] train_loss: 0.014671\n",
      "[076/00773] train_loss: 0.015493\n",
      "[076/00823] train_loss: 0.015948\n",
      "[076/00873] train_loss: 0.015419\n",
      "[076/00923] train_loss: 0.015182\n",
      "[076/00973] train_loss: 0.014264\n",
      "[076/01023] train_loss: 0.014949\n",
      "[076/01073] train_loss: 0.014823\n",
      "[076/01123] train_loss: 0.014442\n",
      "[076/01173] train_loss: 0.015041\n",
      "[076/01223] train_loss: 0.015529\n",
      "[077/00047] train_loss: 0.017182\n",
      "[077/00097] train_loss: 0.016455\n",
      "[077/00147] train_loss: 0.015867\n",
      "[077/00197] train_loss: 0.015212\n",
      "[077/00247] train_loss: 0.015883\n",
      "[077/00297] train_loss: 0.016465\n",
      "[077/00347] train_loss: 0.015670\n",
      "[077/00397] train_loss: 0.014762\n",
      "[077/00447] train_loss: 0.015353\n",
      "[077/00497] train_loss: 0.015474\n",
      "[077/00547] train_loss: 0.015528\n",
      "[077/00597] train_loss: 0.015369\n",
      "[077/00647] train_loss: 0.015710\n",
      "[077/00697] train_loss: 0.015443\n",
      "[077/00747] train_loss: 0.015518\n",
      "[077/00797] train_loss: 0.014874\n",
      "[077/00847] train_loss: 0.015985\n",
      "[077/00897] train_loss: 0.014929\n",
      "[077/00947] train_loss: 0.014789\n",
      "[077/00997] train_loss: 0.015234\n",
      "[077/01047] train_loss: 0.014904\n",
      "[077/01097] train_loss: 0.014998\n",
      "[077/01147] train_loss: 0.014726\n",
      "[077/01197] train_loss: 0.015110\n",
      "[078/00021] train_loss: 0.016284\n",
      "[078/00071] train_loss: 0.017355\n",
      "[078/00121] train_loss: 0.015404\n",
      "[078/00171] train_loss: 0.015680\n",
      "[078/00221] train_loss: 0.016285\n",
      "[078/00271] train_loss: 0.015659\n",
      "[078/00321] train_loss: 0.015854\n",
      "[078/00371] train_loss: 0.014912\n",
      "[078/00421] train_loss: 0.014889\n",
      "[078/00471] train_loss: 0.015300\n",
      "[078/00521] train_loss: 0.015331\n",
      "[078/00571] train_loss: 0.014573\n",
      "[078/00621] train_loss: 0.014798\n",
      "[078/00671] train_loss: 0.014597\n",
      "[078/00721] train_loss: 0.015155\n",
      "[078/00771] train_loss: 0.015319\n",
      "[078/00821] train_loss: 0.015011\n",
      "[078/00871] train_loss: 0.014876\n",
      "[078/00921] train_loss: 0.015276\n",
      "[078/00971] train_loss: 0.015278\n",
      "[078/01021] train_loss: 0.014735\n",
      "[078/01071] train_loss: 0.015142\n",
      "[078/01121] train_loss: 0.014536\n",
      "[078/01171] train_loss: 0.014894\n",
      "[078/01221] train_loss: 0.014785\n",
      "[079/00045] train_loss: 0.016771\n",
      "[079/00095] train_loss: 0.017333\n",
      "[079/00145] train_loss: 0.015938\n",
      "[079/00195] train_loss: 0.016482\n",
      "[079/00245] train_loss: 0.015938\n",
      "[079/00295] train_loss: 0.015441\n",
      "[079/00345] train_loss: 0.015231\n",
      "[079/00395] train_loss: 0.015562\n",
      "[079/00445] train_loss: 0.015451\n",
      "[079/00495] train_loss: 0.015426\n",
      "[079/00545] train_loss: 0.015310\n",
      "[079/00595] train_loss: 0.014742\n",
      "[079/00645] train_loss: 0.015367\n",
      "[079/00695] train_loss: 0.014288\n",
      "[079/00745] train_loss: 0.015328\n",
      "[079/00795] train_loss: 0.015071\n",
      "[079/00845] train_loss: 0.015451\n",
      "[079/00895] train_loss: 0.014876\n",
      "[079/00945] train_loss: 0.015484\n",
      "[079/00995] train_loss: 0.013763\n",
      "[079/01045] train_loss: 0.015342\n",
      "[079/01095] train_loss: 0.015108\n",
      "[079/01145] train_loss: 0.014676\n",
      "[079/01195] train_loss: 0.014433\n",
      "[080/00019] train_loss: 0.015635\n",
      "[080/00069] train_loss: 0.016456\n",
      "[080/00119] train_loss: 0.016628\n",
      "[080/00169] train_loss: 0.016797\n",
      "[080/00219] train_loss: 0.016660\n",
      "[080/00269] train_loss: 0.015399\n",
      "[080/00319] train_loss: 0.016258\n",
      "[080/00369] train_loss: 0.015929\n",
      "[080/00419] train_loss: 0.015071\n",
      "[080/00469] train_loss: 0.016532\n",
      "[080/00519] train_loss: 0.014320\n",
      "[080/00569] train_loss: 0.014806\n",
      "[080/00619] train_loss: 0.015504\n",
      "[080/00669] train_loss: 0.014433\n",
      "[080/00719] train_loss: 0.015014\n",
      "[080/00769] train_loss: 0.015130\n",
      "[080/00819] train_loss: 0.015830\n",
      "[080/00869] train_loss: 0.015372\n",
      "[080/00919] train_loss: 0.014126\n",
      "[080/00969] train_loss: 0.015198\n",
      "[080/01019] train_loss: 0.014364\n",
      "[080/01069] train_loss: 0.015307\n",
      "[080/01119] train_loss: 0.015209\n",
      "[080/01169] train_loss: 0.014971\n",
      "[080/01219] train_loss: 0.014299\n",
      "[081/00043] train_loss: 0.016353\n",
      "[081/00093] train_loss: 0.015931\n",
      "[081/00143] train_loss: 0.015786\n",
      "[081/00193] train_loss: 0.015628\n",
      "[081/00243] train_loss: 0.015470\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[081/00293] train_loss: 0.015180\n",
      "[081/00343] train_loss: 0.016608\n",
      "[081/00393] train_loss: 0.014519\n",
      "[081/00443] train_loss: 0.015751\n",
      "[081/00493] train_loss: 0.015255\n",
      "[081/00543] train_loss: 0.014491\n",
      "[081/00593] train_loss: 0.014823\n",
      "[081/00643] train_loss: 0.014720\n",
      "[081/00693] train_loss: 0.015258\n",
      "[081/00743] train_loss: 0.015027\n",
      "[081/00793] train_loss: 0.015120\n",
      "[081/00843] train_loss: 0.015176\n",
      "[081/00893] train_loss: 0.015260\n",
      "[081/00943] train_loss: 0.014809\n",
      "[081/00993] train_loss: 0.014828\n",
      "[081/01043] train_loss: 0.014389\n",
      "[081/01093] train_loss: 0.015315\n",
      "[081/01143] train_loss: 0.014772\n",
      "[081/01193] train_loss: 0.016154\n",
      "[082/00017] train_loss: 0.015596\n",
      "[082/00067] train_loss: 0.017043\n",
      "[082/00117] train_loss: 0.015223\n",
      "[082/00167] train_loss: 0.015827\n",
      "[082/00217] train_loss: 0.016317\n",
      "[082/00267] train_loss: 0.014951\n",
      "[082/00317] train_loss: 0.015855\n",
      "[082/00367] train_loss: 0.015523\n",
      "[082/00417] train_loss: 0.015170\n",
      "[082/00467] train_loss: 0.015391\n",
      "[082/00517] train_loss: 0.015690\n",
      "[082/00567] train_loss: 0.014951\n",
      "[082/00617] train_loss: 0.014809\n",
      "[082/00667] train_loss: 0.015809\n",
      "[082/00717] train_loss: 0.014972\n",
      "[082/00767] train_loss: 0.015125\n",
      "[082/00817] train_loss: 0.015160\n",
      "[082/00867] train_loss: 0.014627\n",
      "[082/00917] train_loss: 0.014908\n",
      "[082/00967] train_loss: 0.015356\n",
      "[082/01017] train_loss: 0.015038\n",
      "[082/01067] train_loss: 0.014323\n",
      "[082/01117] train_loss: 0.014524\n",
      "[082/01167] train_loss: 0.014561\n",
      "[082/01217] train_loss: 0.014927\n",
      "[083/00041] train_loss: 0.016993\n",
      "[083/00091] train_loss: 0.015851\n",
      "[083/00141] train_loss: 0.015886\n",
      "[083/00191] train_loss: 0.016496\n",
      "[083/00241] train_loss: 0.015466\n",
      "[083/00291] train_loss: 0.015103\n",
      "[083/00341] train_loss: 0.015627\n",
      "[083/00391] train_loss: 0.015814\n",
      "[083/00441] train_loss: 0.014848\n",
      "[083/00491] train_loss: 0.015814\n",
      "[083/00541] train_loss: 0.015022\n",
      "[083/00591] train_loss: 0.014313\n",
      "[083/00641] train_loss: 0.015280\n",
      "[083/00691] train_loss: 0.015848\n",
      "[083/00741] train_loss: 0.015171\n",
      "[083/00791] train_loss: 0.015059\n",
      "[083/00841] train_loss: 0.014438\n",
      "[083/00891] train_loss: 0.014313\n",
      "[083/00941] train_loss: 0.014864\n",
      "[083/00991] train_loss: 0.014933\n",
      "[083/01041] train_loss: 0.014266\n",
      "[083/01091] train_loss: 0.014371\n",
      "[083/01141] train_loss: 0.013889\n",
      "[083/01191] train_loss: 0.014086\n",
      "[084/00015] train_loss: 0.015433\n",
      "[084/00065] train_loss: 0.016515\n",
      "[084/00115] train_loss: 0.015651\n",
      "[084/00165] train_loss: 0.015805\n",
      "[084/00215] train_loss: 0.015712\n",
      "[084/00265] train_loss: 0.015466\n",
      "[084/00315] train_loss: 0.014877\n",
      "[084/00365] train_loss: 0.015251\n",
      "[084/00415] train_loss: 0.016435\n",
      "[084/00465] train_loss: 0.015286\n",
      "[084/00515] train_loss: 0.015400\n",
      "[084/00565] train_loss: 0.015673\n",
      "[084/00615] train_loss: 0.015611\n",
      "[084/00665] train_loss: 0.014632\n",
      "[084/00715] train_loss: 0.013916\n",
      "[084/00765] train_loss: 0.014912\n",
      "[084/00815] train_loss: 0.015292\n",
      "[084/00865] train_loss: 0.014450\n",
      "[084/00915] train_loss: 0.014801\n",
      "[084/00965] train_loss: 0.014877\n",
      "[084/01015] train_loss: 0.015384\n",
      "[084/01065] train_loss: 0.014482\n",
      "[084/01115] train_loss: 0.014102\n",
      "[084/01165] train_loss: 0.014115\n",
      "[084/01215] train_loss: 0.014811\n",
      "[085/00039] train_loss: 0.016727\n",
      "[085/00089] train_loss: 0.016528\n",
      "[085/00139] train_loss: 0.015891\n",
      "[085/00189] train_loss: 0.015259\n",
      "[085/00239] train_loss: 0.015413\n",
      "[085/00289] train_loss: 0.015187\n",
      "[085/00339] train_loss: 0.014678\n",
      "[085/00389] train_loss: 0.015919\n",
      "[085/00439] train_loss: 0.014393\n",
      "[085/00489] train_loss: 0.014629\n",
      "[085/00539] train_loss: 0.015479\n",
      "[085/00589] train_loss: 0.015377\n",
      "[085/00639] train_loss: 0.014544\n",
      "[085/00689] train_loss: 0.014609\n",
      "[085/00739] train_loss: 0.014317\n",
      "[085/00789] train_loss: 0.014469\n",
      "[085/00839] train_loss: 0.014171\n",
      "[085/00889] train_loss: 0.015358\n",
      "[085/00939] train_loss: 0.014526\n",
      "[085/00989] train_loss: 0.015156\n",
      "[085/01039] train_loss: 0.014621\n",
      "[085/01089] train_loss: 0.015235\n",
      "[085/01139] train_loss: 0.013958\n",
      "[085/01189] train_loss: 0.015266\n",
      "[086/00013] train_loss: 0.015942\n",
      "[086/00063] train_loss: 0.016099\n",
      "[086/00113] train_loss: 0.016485\n",
      "[086/00163] train_loss: 0.016298\n",
      "[086/00213] train_loss: 0.015278\n",
      "[086/00263] train_loss: 0.014859\n",
      "[086/00313] train_loss: 0.014585\n",
      "[086/00363] train_loss: 0.014628\n",
      "[086/00413] train_loss: 0.014974\n",
      "[086/00463] train_loss: 0.015115\n",
      "[086/00513] train_loss: 0.014691\n",
      "[086/00563] train_loss: 0.015516\n",
      "[086/00613] train_loss: 0.015252\n",
      "[086/00663] train_loss: 0.015011\n",
      "[086/00713] train_loss: 0.014990\n",
      "[086/00763] train_loss: 0.014654\n",
      "[086/00813] train_loss: 0.014701\n",
      "[086/00863] train_loss: 0.014561\n",
      "[086/00913] train_loss: 0.015052\n",
      "[086/00963] train_loss: 0.015275\n",
      "[086/01013] train_loss: 0.016292\n",
      "[086/01063] train_loss: 0.015162\n",
      "[086/01113] train_loss: 0.014533\n",
      "[086/01163] train_loss: 0.014462\n",
      "[086/01213] train_loss: 0.014929\n",
      "[087/00037] train_loss: 0.016045\n",
      "[087/00087] train_loss: 0.015264\n",
      "[087/00137] train_loss: 0.016878\n",
      "[087/00187] train_loss: 0.015671\n",
      "[087/00237] train_loss: 0.015006\n",
      "[087/00287] train_loss: 0.015982\n",
      "[087/00337] train_loss: 0.014839\n",
      "[087/00387] train_loss: 0.015015\n",
      "[087/00437] train_loss: 0.014885\n",
      "[087/00487] train_loss: 0.015072\n",
      "[087/00537] train_loss: 0.015628\n",
      "[087/00587] train_loss: 0.014886\n",
      "[087/00637] train_loss: 0.014479\n",
      "[087/00687] train_loss: 0.014624\n",
      "[087/00737] train_loss: 0.014024\n",
      "[087/00787] train_loss: 0.014775\n",
      "[087/00837] train_loss: 0.014114\n",
      "[087/00887] train_loss: 0.014348\n",
      "[087/00937] train_loss: 0.013886\n",
      "[087/00987] train_loss: 0.014588\n",
      "[087/01037] train_loss: 0.014591\n",
      "[087/01087] train_loss: 0.014656\n",
      "[087/01137] train_loss: 0.014730\n",
      "[087/01187] train_loss: 0.014763\n",
      "[088/00011] train_loss: 0.015161\n",
      "[088/00061] train_loss: 0.016317\n",
      "[088/00111] train_loss: 0.015938\n",
      "[088/00161] train_loss: 0.015271\n",
      "[088/00211] train_loss: 0.015781\n",
      "[088/00261] train_loss: 0.015226\n",
      "[088/00311] train_loss: 0.015686\n",
      "[088/00361] train_loss: 0.015407\n",
      "[088/00411] train_loss: 0.015096\n",
      "[088/00461] train_loss: 0.015854\n",
      "[088/00511] train_loss: 0.014899\n",
      "[088/00561] train_loss: 0.014769\n",
      "[088/00611] train_loss: 0.014787\n",
      "[088/00661] train_loss: 0.014620\n",
      "[088/00711] train_loss: 0.014870\n",
      "[088/00761] train_loss: 0.014975\n",
      "[088/00811] train_loss: 0.014294\n",
      "[088/00861] train_loss: 0.015090\n",
      "[088/00911] train_loss: 0.014871\n",
      "[088/00961] train_loss: 0.015210\n",
      "[088/01011] train_loss: 0.014325\n",
      "[088/01061] train_loss: 0.014706\n",
      "[088/01111] train_loss: 0.014550\n",
      "[088/01161] train_loss: 0.014186\n",
      "[088/01211] train_loss: 0.014407\n",
      "[089/00035] train_loss: 0.015538\n",
      "[089/00085] train_loss: 0.014942\n",
      "[089/00135] train_loss: 0.016410\n",
      "[089/00185] train_loss: 0.015824\n",
      "[089/00235] train_loss: 0.016299\n",
      "[089/00285] train_loss: 0.015670\n",
      "[089/00335] train_loss: 0.014940\n",
      "[089/00385] train_loss: 0.015489\n",
      "[089/00435] train_loss: 0.014048\n",
      "[089/00485] train_loss: 0.015413\n",
      "[089/00535] train_loss: 0.014111\n",
      "[089/00585] train_loss: 0.014541\n",
      "[089/00635] train_loss: 0.014629\n",
      "[089/00685] train_loss: 0.014802\n",
      "[089/00735] train_loss: 0.014689\n",
      "[089/00785] train_loss: 0.014285\n",
      "[089/00835] train_loss: 0.014435\n",
      "[089/00885] train_loss: 0.015276\n",
      "[089/00935] train_loss: 0.014194\n",
      "[089/00985] train_loss: 0.014909\n",
      "[089/01035] train_loss: 0.014144\n",
      "[089/01085] train_loss: 0.014381\n",
      "[089/01135] train_loss: 0.014448\n",
      "[089/01185] train_loss: 0.014098\n",
      "[090/00009] train_loss: 0.014762\n",
      "[090/00059] train_loss: 0.016531\n",
      "[090/00109] train_loss: 0.016022\n",
      "[090/00159] train_loss: 0.015859\n",
      "[090/00209] train_loss: 0.014804\n",
      "[090/00259] train_loss: 0.014935\n",
      "[090/00309] train_loss: 0.014058\n",
      "[090/00359] train_loss: 0.014790\n",
      "[090/00409] train_loss: 0.014232\n",
      "[090/00459] train_loss: 0.014387\n",
      "[090/00509] train_loss: 0.015025\n",
      "[090/00559] train_loss: 0.014864\n",
      "[090/00609] train_loss: 0.015703\n",
      "[090/00659] train_loss: 0.014893\n",
      "[090/00709] train_loss: 0.014438\n",
      "[090/00759] train_loss: 0.013894\n",
      "[090/00809] train_loss: 0.015266\n",
      "[090/00859] train_loss: 0.014490\n",
      "[090/00909] train_loss: 0.014739\n",
      "[090/00959] train_loss: 0.015298\n",
      "[090/01009] train_loss: 0.015448\n",
      "[090/01059] train_loss: 0.014727\n",
      "[090/01109] train_loss: 0.014296\n",
      "[090/01159] train_loss: 0.014853\n",
      "[090/01209] train_loss: 0.014223\n",
      "[091/00033] train_loss: 0.015776\n",
      "[091/00083] train_loss: 0.016148\n",
      "[091/00133] train_loss: 0.015643\n",
      "[091/00183] train_loss: 0.015419\n",
      "[091/00233] train_loss: 0.015344\n",
      "[091/00283] train_loss: 0.014826\n",
      "[091/00333] train_loss: 0.014418\n",
      "[091/00383] train_loss: 0.014550\n",
      "[091/00433] train_loss: 0.014983\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[091/00483] train_loss: 0.014565\n",
      "[091/00533] train_loss: 0.015124\n",
      "[091/00583] train_loss: 0.015000\n",
      "[091/00633] train_loss: 0.014902\n",
      "[091/00683] train_loss: 0.014490\n",
      "[091/00733] train_loss: 0.014421\n",
      "[091/00783] train_loss: 0.015341\n",
      "[091/00833] train_loss: 0.015272\n",
      "[091/00883] train_loss: 0.014981\n",
      "[091/00933] train_loss: 0.014721\n",
      "[091/00983] train_loss: 0.014386\n",
      "[091/01033] train_loss: 0.013810\n",
      "[091/01083] train_loss: 0.013569\n",
      "[091/01133] train_loss: 0.014575\n",
      "[091/01183] train_loss: 0.014236\n",
      "[092/00007] train_loss: 0.014839\n",
      "[092/00057] train_loss: 0.016363\n",
      "[092/00107] train_loss: 0.015341\n",
      "[092/00157] train_loss: 0.016145\n",
      "[092/00207] train_loss: 0.015570\n",
      "[092/00257] train_loss: 0.014938\n",
      "[092/00307] train_loss: 0.015660\n",
      "[092/00357] train_loss: 0.015298\n",
      "[092/00407] train_loss: 0.015532\n",
      "[092/00457] train_loss: 0.014926\n",
      "[092/00507] train_loss: 0.014820\n",
      "[092/00557] train_loss: 0.014561\n",
      "[092/00607] train_loss: 0.014971\n",
      "[092/00657] train_loss: 0.014787\n",
      "[092/00707] train_loss: 0.015395\n",
      "[092/00757] train_loss: 0.015638\n",
      "[092/00807] train_loss: 0.014851\n",
      "[092/00857] train_loss: 0.014945\n",
      "[092/00907] train_loss: 0.014440\n",
      "[092/00957] train_loss: 0.014002\n",
      "[092/01007] train_loss: 0.014849\n",
      "[092/01057] train_loss: 0.013959\n",
      "[092/01107] train_loss: 0.014436\n",
      "[092/01157] train_loss: 0.014042\n",
      "[092/01207] train_loss: 0.014514\n",
      "[093/00031] train_loss: 0.015678\n",
      "[093/00081] train_loss: 0.016229\n",
      "[093/00131] train_loss: 0.015171\n",
      "[093/00181] train_loss: 0.015396\n",
      "[093/00231] train_loss: 0.015663\n",
      "[093/00281] train_loss: 0.015286\n",
      "[093/00331] train_loss: 0.014384\n",
      "[093/00381] train_loss: 0.014648\n",
      "[093/00431] train_loss: 0.014326\n",
      "[093/00481] train_loss: 0.014440\n",
      "[093/00531] train_loss: 0.014865\n",
      "[093/00581] train_loss: 0.014044\n",
      "[093/00631] train_loss: 0.014615\n",
      "[093/00681] train_loss: 0.014522\n",
      "[093/00731] train_loss: 0.015188\n",
      "[093/00781] train_loss: 0.014407\n",
      "[093/00831] train_loss: 0.014829\n",
      "[093/00881] train_loss: 0.014861\n",
      "[093/00931] train_loss: 0.014961\n",
      "[093/00981] train_loss: 0.014125\n",
      "[093/01031] train_loss: 0.014002\n",
      "[093/01081] train_loss: 0.014360\n",
      "[093/01131] train_loss: 0.015181\n",
      "[093/01181] train_loss: 0.013720\n",
      "[094/00005] train_loss: 0.014530\n",
      "[094/00055] train_loss: 0.015954\n",
      "[094/00105] train_loss: 0.015525\n",
      "[094/00155] train_loss: 0.015720\n",
      "[094/00205] train_loss: 0.015669\n",
      "[094/00255] train_loss: 0.014733\n",
      "[094/00305] train_loss: 0.014403\n",
      "[094/00355] train_loss: 0.014741\n",
      "[094/00405] train_loss: 0.015459\n",
      "[094/00455] train_loss: 0.014872\n",
      "[094/00505] train_loss: 0.014288\n",
      "[094/00555] train_loss: 0.015112\n",
      "[094/00605] train_loss: 0.014700\n",
      "[094/00655] train_loss: 0.014549\n",
      "[094/00705] train_loss: 0.014351\n",
      "[094/00755] train_loss: 0.014631\n",
      "[094/00805] train_loss: 0.014244\n",
      "[094/00855] train_loss: 0.014455\n",
      "[094/00905] train_loss: 0.014592\n",
      "[094/00955] train_loss: 0.013772\n",
      "[094/01005] train_loss: 0.014701\n",
      "[094/01055] train_loss: 0.015363\n",
      "[094/01105] train_loss: 0.014687\n",
      "[094/01155] train_loss: 0.014707\n",
      "[094/01205] train_loss: 0.014140\n",
      "[095/00029] train_loss: 0.015605\n",
      "[095/00079] train_loss: 0.016426\n",
      "[095/00129] train_loss: 0.015238\n",
      "[095/00179] train_loss: 0.015575\n",
      "[095/00229] train_loss: 0.015407\n",
      "[095/00279] train_loss: 0.014627\n",
      "[095/00329] train_loss: 0.015046\n",
      "[095/00379] train_loss: 0.015171\n",
      "[095/00429] train_loss: 0.015738\n",
      "[095/00479] train_loss: 0.014420\n",
      "[095/00529] train_loss: 0.014404\n",
      "[095/00579] train_loss: 0.014488\n",
      "[095/00629] train_loss: 0.013944\n",
      "[095/00679] train_loss: 0.014612\n",
      "[095/00729] train_loss: 0.014773\n",
      "[095/00779] train_loss: 0.014327\n",
      "[095/00829] train_loss: 0.014603\n",
      "[095/00879] train_loss: 0.014388\n",
      "[095/00929] train_loss: 0.014632\n",
      "[095/00979] train_loss: 0.014017\n",
      "[095/01029] train_loss: 0.014331\n",
      "[095/01079] train_loss: 0.013755\n",
      "[095/01129] train_loss: 0.014972\n",
      "[095/01179] train_loss: 0.014677\n",
      "[096/00003] train_loss: 0.015144\n",
      "[096/00053] train_loss: 0.015712\n",
      "[096/00103] train_loss: 0.016479\n",
      "[096/00153] train_loss: 0.015694\n",
      "[096/00203] train_loss: 0.015781\n",
      "[096/00253] train_loss: 0.014758\n",
      "[096/00303] train_loss: 0.014895\n",
      "[096/00353] train_loss: 0.014650\n",
      "[096/00403] train_loss: 0.014415\n",
      "[096/00453] train_loss: 0.014726\n",
      "[096/00503] train_loss: 0.015325\n",
      "[096/00553] train_loss: 0.015316\n",
      "[096/00603] train_loss: 0.015316\n",
      "[096/00653] train_loss: 0.014783\n",
      "[096/00703] train_loss: 0.014022\n",
      "[096/00753] train_loss: 0.013649\n",
      "[096/00803] train_loss: 0.014052\n",
      "[096/00853] train_loss: 0.014424\n",
      "[096/00903] train_loss: 0.014334\n",
      "[096/00953] train_loss: 0.014495\n",
      "[096/01003] train_loss: 0.015085\n",
      "[096/01053] train_loss: 0.015130\n",
      "[096/01103] train_loss: 0.014844\n",
      "[096/01153] train_loss: 0.014433\n",
      "[096/01203] train_loss: 0.013796\n",
      "[097/00027] train_loss: 0.014795\n",
      "[097/00077] train_loss: 0.016767\n",
      "[097/00127] train_loss: 0.015704\n",
      "[097/00177] train_loss: 0.014724\n",
      "[097/00227] train_loss: 0.014529\n",
      "[097/00277] train_loss: 0.014746\n",
      "[097/00327] train_loss: 0.015184\n",
      "[097/00377] train_loss: 0.013983\n",
      "[097/00427] train_loss: 0.015146\n",
      "[097/00477] train_loss: 0.014659\n",
      "[097/00527] train_loss: 0.015390\n",
      "[097/00577] train_loss: 0.015193\n",
      "[097/00627] train_loss: 0.013853\n",
      "[097/00677] train_loss: 0.014738\n",
      "[097/00727] train_loss: 0.014414\n",
      "[097/00777] train_loss: 0.015041\n",
      "[097/00827] train_loss: 0.013524\n",
      "[097/00877] train_loss: 0.014674\n",
      "[097/00927] train_loss: 0.014460\n",
      "[097/00977] train_loss: 0.014954\n",
      "[097/01027] train_loss: 0.013911\n",
      "[097/01077] train_loss: 0.014592\n",
      "[097/01127] train_loss: 0.013954\n",
      "[097/01177] train_loss: 0.014122\n",
      "[098/00001] train_loss: 0.014516\n",
      "[098/00051] train_loss: 0.016030\n",
      "[098/00101] train_loss: 0.015723\n",
      "[098/00151] train_loss: 0.015775\n",
      "[098/00201] train_loss: 0.015802\n",
      "[098/00251] train_loss: 0.015640\n",
      "[098/00301] train_loss: 0.015379\n",
      "[098/00351] train_loss: 0.014460\n",
      "[098/00401] train_loss: 0.015380\n",
      "[098/00451] train_loss: 0.015018\n",
      "[098/00501] train_loss: 0.013959\n",
      "[098/00551] train_loss: 0.014168\n",
      "[098/00601] train_loss: 0.015242\n",
      "[098/00651] train_loss: 0.014242\n",
      "[098/00701] train_loss: 0.014138\n",
      "[098/00751] train_loss: 0.013988\n",
      "[098/00801] train_loss: 0.014120\n",
      "[098/00851] train_loss: 0.014683\n",
      "[098/00901] train_loss: 0.013887\n",
      "[098/00951] train_loss: 0.013798\n",
      "[098/01001] train_loss: 0.013856\n",
      "[098/01051] train_loss: 0.014756\n",
      "[098/01101] train_loss: 0.014262\n",
      "[098/01151] train_loss: 0.014686\n",
      "[098/01201] train_loss: 0.014668\n",
      "[099/00025] train_loss: 0.015864\n",
      "[099/00075] train_loss: 0.015687\n",
      "[099/00125] train_loss: 0.015666\n",
      "[099/00175] train_loss: 0.015812\n",
      "[099/00225] train_loss: 0.015246\n",
      "[099/00275] train_loss: 0.014431\n",
      "[099/00325] train_loss: 0.014221\n",
      "[099/00375] train_loss: 0.014695\n",
      "[099/00425] train_loss: 0.014706\n",
      "[099/00475] train_loss: 0.015205\n",
      "[099/00525] train_loss: 0.015893\n",
      "[099/00575] train_loss: 0.013762\n",
      "[099/00625] train_loss: 0.014539\n",
      "[099/00675] train_loss: 0.014009\n",
      "[099/00725] train_loss: 0.014329\n",
      "[099/00775] train_loss: 0.014213\n",
      "[099/00825] train_loss: 0.014741\n",
      "[099/00875] train_loss: 0.014656\n",
      "[099/00925] train_loss: 0.015091\n",
      "[099/00975] train_loss: 0.014194\n",
      "[099/01025] train_loss: 0.014797\n",
      "[099/01075] train_loss: 0.014323\n",
      "[099/01125] train_loss: 0.014646\n",
      "[099/01175] train_loss: 0.013347\n",
      "[099/01225] train_loss: 0.014029\n",
      "[100/00049] train_loss: 0.016577\n",
      "[100/00099] train_loss: 0.015382\n",
      "[100/00149] train_loss: 0.016173\n",
      "[100/00199] train_loss: 0.015371\n",
      "[100/00249] train_loss: 0.015122\n",
      "[100/00299] train_loss: 0.015141\n",
      "[100/00349] train_loss: 0.014636\n",
      "[100/00399] train_loss: 0.014534\n",
      "[100/00449] train_loss: 0.014305\n",
      "[100/00499] train_loss: 0.015207\n",
      "[100/00549] train_loss: 0.014302\n",
      "[100/00599] train_loss: 0.015570\n",
      "[100/00649] train_loss: 0.014610\n",
      "[100/00699] train_loss: 0.014869\n",
      "[100/00749] train_loss: 0.014885\n",
      "[100/00799] train_loss: 0.014374\n",
      "[100/00849] train_loss: 0.014267\n",
      "[100/00899] train_loss: 0.014316\n",
      "[100/00949] train_loss: 0.014173\n",
      "[100/00999] train_loss: 0.014164\n",
      "[100/01049] train_loss: 0.013312\n",
      "[100/01099] train_loss: 0.014590\n",
      "[100/01149] train_loss: 0.013885\n",
      "[100/01199] train_loss: 0.013456\n",
      "[101/00023] train_loss: 0.015382\n",
      "[101/00073] train_loss: 0.016352\n",
      "[101/00123] train_loss: 0.015417\n",
      "[101/00173] train_loss: 0.014736\n",
      "[101/00223] train_loss: 0.015238\n",
      "[101/00273] train_loss: 0.014798\n",
      "[101/00323] train_loss: 0.015152\n",
      "[101/00373] train_loss: 0.015461\n",
      "[101/00423] train_loss: 0.015379\n",
      "[101/00473] train_loss: 0.014359\n",
      "[101/00523] train_loss: 0.014234\n",
      "[101/00573] train_loss: 0.013732\n",
      "[101/00623] train_loss: 0.014660\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101/00673] train_loss: 0.014605\n",
      "[101/00723] train_loss: 0.014507\n",
      "[101/00773] train_loss: 0.014286\n",
      "[101/00823] train_loss: 0.014883\n",
      "[101/00873] train_loss: 0.015046\n",
      "[101/00923] train_loss: 0.014508\n",
      "[101/00973] train_loss: 0.014540\n",
      "[101/01023] train_loss: 0.014833\n",
      "[101/01073] train_loss: 0.015129\n",
      "[101/01123] train_loss: 0.013682\n",
      "[101/01173] train_loss: 0.014257\n",
      "[101/01223] train_loss: 0.014418\n",
      "[102/00047] train_loss: 0.017137\n",
      "[102/00097] train_loss: 0.016336\n",
      "[102/00147] train_loss: 0.015047\n",
      "[102/00197] train_loss: 0.015178\n",
      "[102/00247] train_loss: 0.015205\n",
      "[102/00297] train_loss: 0.014801\n",
      "[102/00347] train_loss: 0.014226\n",
      "[102/00397] train_loss: 0.014782\n",
      "[102/00447] train_loss: 0.014978\n",
      "[102/00497] train_loss: 0.014053\n",
      "[102/00547] train_loss: 0.014711\n",
      "[102/00597] train_loss: 0.013872\n",
      "[102/00647] train_loss: 0.014624\n",
      "[102/00697] train_loss: 0.013959\n",
      "[102/00747] train_loss: 0.014915\n",
      "[102/00797] train_loss: 0.014135\n",
      "[102/00847] train_loss: 0.013891\n",
      "[102/00897] train_loss: 0.013742\n",
      "[102/00947] train_loss: 0.015415\n",
      "[102/00997] train_loss: 0.014722\n",
      "[102/01047] train_loss: 0.013758\n",
      "[102/01097] train_loss: 0.014300\n",
      "[102/01147] train_loss: 0.014960\n",
      "[102/01197] train_loss: 0.014075\n",
      "[103/00021] train_loss: 0.015858\n",
      "[103/00071] train_loss: 0.015143\n",
      "[103/00121] train_loss: 0.015453\n",
      "[103/00171] train_loss: 0.015151\n",
      "[103/00221] train_loss: 0.014602\n",
      "[103/00271] train_loss: 0.014706\n",
      "[103/00321] train_loss: 0.015259\n",
      "[103/00371] train_loss: 0.014448\n",
      "[103/00421] train_loss: 0.014654\n",
      "[103/00471] train_loss: 0.014510\n",
      "[103/00521] train_loss: 0.014699\n",
      "[103/00571] train_loss: 0.014962\n",
      "[103/00621] train_loss: 0.015152\n",
      "[103/00671] train_loss: 0.014734\n",
      "[103/00721] train_loss: 0.015234\n",
      "[103/00771] train_loss: 0.014705\n",
      "[103/00821] train_loss: 0.014640\n",
      "[103/00871] train_loss: 0.014055\n",
      "[103/00921] train_loss: 0.014427\n",
      "[103/00971] train_loss: 0.014556\n",
      "[103/01021] train_loss: 0.015103\n",
      "[103/01071] train_loss: 0.014688\n",
      "[103/01121] train_loss: 0.014283\n",
      "[103/01171] train_loss: 0.014067\n",
      "[103/01221] train_loss: 0.014133\n",
      "[104/00045] train_loss: 0.015489\n",
      "[104/00095] train_loss: 0.015665\n",
      "[104/00145] train_loss: 0.015215\n",
      "[104/00195] train_loss: 0.014968\n",
      "[104/00245] train_loss: 0.015237\n",
      "[104/00295] train_loss: 0.015720\n",
      "[104/00345] train_loss: 0.014478\n",
      "[104/00395] train_loss: 0.013767\n",
      "[104/00445] train_loss: 0.014036\n",
      "[104/00495] train_loss: 0.014305\n",
      "[104/00545] train_loss: 0.014345\n",
      "[104/00595] train_loss: 0.014416\n",
      "[104/00645] train_loss: 0.014435\n",
      "[104/00695] train_loss: 0.014472\n",
      "[104/00745] train_loss: 0.015269\n",
      "[104/00795] train_loss: 0.014572\n",
      "[104/00845] train_loss: 0.014753\n",
      "[104/00895] train_loss: 0.014941\n",
      "[104/00945] train_loss: 0.014275\n",
      "[104/00995] train_loss: 0.014521\n",
      "[104/01045] train_loss: 0.014223\n",
      "[104/01095] train_loss: 0.014059\n",
      "[104/01145] train_loss: 0.013732\n",
      "[104/01195] train_loss: 0.014543\n",
      "[105/00019] train_loss: 0.014812\n",
      "[105/00069] train_loss: 0.016567\n",
      "[105/00119] train_loss: 0.015182\n",
      "[105/00169] train_loss: 0.015074\n",
      "[105/00219] train_loss: 0.015220\n",
      "[105/00269] train_loss: 0.015108\n",
      "[105/00319] train_loss: 0.015088\n",
      "[105/00369] train_loss: 0.013744\n",
      "[105/00419] train_loss: 0.015238\n",
      "[105/00469] train_loss: 0.014300\n",
      "[105/00519] train_loss: 0.013472\n",
      "[105/00569] train_loss: 0.014119\n",
      "[105/00619] train_loss: 0.014808\n",
      "[105/00669] train_loss: 0.013449\n",
      "[105/00719] train_loss: 0.015190\n",
      "[105/00769] train_loss: 0.013833\n",
      "[105/00819] train_loss: 0.014365\n",
      "[105/00869] train_loss: 0.014307\n",
      "[105/00919] train_loss: 0.013593\n",
      "[105/00969] train_loss: 0.015747\n",
      "[105/01019] train_loss: 0.014806\n",
      "[105/01069] train_loss: 0.015260\n",
      "[105/01119] train_loss: 0.013454\n",
      "[105/01169] train_loss: 0.013543\n",
      "[105/01219] train_loss: 0.013849\n",
      "[106/00043] train_loss: 0.014881\n",
      "[106/00093] train_loss: 0.015065\n",
      "[106/00143] train_loss: 0.016087\n",
      "[106/00193] train_loss: 0.014863\n",
      "[106/00243] train_loss: 0.014317\n",
      "[106/00293] train_loss: 0.014546\n",
      "[106/00343] train_loss: 0.014753\n",
      "[106/00393] train_loss: 0.015420\n",
      "[106/00443] train_loss: 0.015200\n",
      "[106/00493] train_loss: 0.015498\n",
      "[106/00543] train_loss: 0.014998\n",
      "[106/00593] train_loss: 0.014618\n",
      "[106/00643] train_loss: 0.014435\n",
      "[106/00693] train_loss: 0.014245\n",
      "[106/00743] train_loss: 0.014084\n",
      "[106/00793] train_loss: 0.014173\n",
      "[106/00843] train_loss: 0.014732\n",
      "[106/00893] train_loss: 0.014217\n",
      "[106/00943] train_loss: 0.013890\n",
      "[106/00993] train_loss: 0.014199\n",
      "[106/01043] train_loss: 0.014281\n",
      "[106/01093] train_loss: 0.014177\n",
      "[106/01143] train_loss: 0.014846\n",
      "[106/01193] train_loss: 0.014156\n",
      "[107/00017] train_loss: 0.014226\n",
      "[107/00067] train_loss: 0.016089\n",
      "[107/00117] train_loss: 0.016435\n",
      "[107/00167] train_loss: 0.015123\n",
      "[107/00217] train_loss: 0.014755\n",
      "[107/00267] train_loss: 0.014933\n",
      "[107/00317] train_loss: 0.014261\n",
      "[107/00367] train_loss: 0.014030\n",
      "[107/00417] train_loss: 0.014483\n",
      "[107/00467] train_loss: 0.014724\n",
      "[107/00517] train_loss: 0.013657\n",
      "[107/00567] train_loss: 0.015402\n",
      "[107/00617] train_loss: 0.013781\n",
      "[107/00667] train_loss: 0.015278\n",
      "[107/00717] train_loss: 0.014182\n",
      "[107/00767] train_loss: 0.014820\n",
      "[107/00817] train_loss: 0.014295\n",
      "[107/00867] train_loss: 0.014499\n",
      "[107/00917] train_loss: 0.014147\n",
      "[107/00967] train_loss: 0.014128\n",
      "[107/01017] train_loss: 0.013929\n",
      "[107/01067] train_loss: 0.013995\n",
      "[107/01117] train_loss: 0.013758\n",
      "[107/01167] train_loss: 0.014293\n",
      "[107/01217] train_loss: 0.013979\n",
      "[108/00041] train_loss: 0.016081\n",
      "[108/00091] train_loss: 0.015528\n",
      "[108/00141] train_loss: 0.015007\n",
      "[108/00191] train_loss: 0.015831\n",
      "[108/00241] train_loss: 0.015088\n",
      "[108/00291] train_loss: 0.015114\n",
      "[108/00341] train_loss: 0.014444\n",
      "[108/00391] train_loss: 0.014452\n",
      "[108/00441] train_loss: 0.014985\n",
      "[108/00491] train_loss: 0.014841\n",
      "[108/00541] train_loss: 0.014611\n",
      "[108/00591] train_loss: 0.014214\n",
      "[108/00641] train_loss: 0.014253\n",
      "[108/00691] train_loss: 0.014293\n",
      "[108/00741] train_loss: 0.014391\n",
      "[108/00791] train_loss: 0.013970\n",
      "[108/00841] train_loss: 0.014313\n",
      "[108/00891] train_loss: 0.014159\n",
      "[108/00941] train_loss: 0.014077\n",
      "[108/00991] train_loss: 0.014412\n",
      "[108/01041] train_loss: 0.013933\n",
      "[108/01091] train_loss: 0.013927\n",
      "[108/01141] train_loss: 0.013499\n",
      "[108/01191] train_loss: 0.014372\n",
      "[109/00015] train_loss: 0.013942\n",
      "[109/00065] train_loss: 0.015040\n",
      "[109/00115] train_loss: 0.014861\n",
      "[109/00165] train_loss: 0.015129\n",
      "[109/00215] train_loss: 0.014526\n",
      "[109/00265] train_loss: 0.014311\n",
      "[109/00315] train_loss: 0.015784\n",
      "[109/00365] train_loss: 0.014703\n",
      "[109/00415] train_loss: 0.014578\n",
      "[109/00465] train_loss: 0.015591\n",
      "[109/00515] train_loss: 0.014893\n",
      "[109/00565] train_loss: 0.013994\n",
      "[109/00615] train_loss: 0.014359\n",
      "[109/00665] train_loss: 0.014185\n",
      "[109/00715] train_loss: 0.014496\n",
      "[109/00765] train_loss: 0.014328\n",
      "[109/00815] train_loss: 0.014215\n",
      "[109/00865] train_loss: 0.013816\n",
      "[109/00915] train_loss: 0.015169\n",
      "[109/00965] train_loss: 0.014418\n",
      "[109/01015] train_loss: 0.014338\n",
      "[109/01065] train_loss: 0.014463\n",
      "[109/01115] train_loss: 0.014286\n",
      "[109/01165] train_loss: 0.014268\n",
      "[109/01215] train_loss: 0.014059\n",
      "[110/00039] train_loss: 0.015894\n",
      "[110/00089] train_loss: 0.015085\n",
      "[110/00139] train_loss: 0.014996\n",
      "[110/00189] train_loss: 0.015028\n",
      "[110/00239] train_loss: 0.015378\n",
      "[110/00289] train_loss: 0.014842\n",
      "[110/00339] train_loss: 0.014449\n",
      "[110/00389] train_loss: 0.014823\n",
      "[110/00439] train_loss: 0.014707\n",
      "[110/00489] train_loss: 0.014345\n",
      "[110/00539] train_loss: 0.014175\n",
      "[110/00589] train_loss: 0.014642\n",
      "[110/00639] train_loss: 0.014046\n",
      "[110/00689] train_loss: 0.014692\n",
      "[110/00739] train_loss: 0.014821\n",
      "[110/00789] train_loss: 0.013867\n",
      "[110/00839] train_loss: 0.014553\n",
      "[110/00889] train_loss: 0.013878\n",
      "[110/00939] train_loss: 0.014176\n",
      "[110/00989] train_loss: 0.014035\n",
      "[110/01039] train_loss: 0.013537\n",
      "[110/01089] train_loss: 0.013583\n",
      "[110/01139] train_loss: 0.014615\n",
      "[110/01189] train_loss: 0.014528\n",
      "[111/00013] train_loss: 0.014485\n",
      "[111/00063] train_loss: 0.015187\n",
      "[111/00113] train_loss: 0.015619\n",
      "[111/00163] train_loss: 0.014629\n",
      "[111/00213] train_loss: 0.015031\n",
      "[111/00263] train_loss: 0.014717\n",
      "[111/00313] train_loss: 0.014594\n",
      "[111/00363] train_loss: 0.014775\n",
      "[111/00413] train_loss: 0.015253\n",
      "[111/00463] train_loss: 0.013792\n",
      "[111/00513] train_loss: 0.013693\n",
      "[111/00563] train_loss: 0.014802\n",
      "[111/00613] train_loss: 0.014147\n",
      "[111/00663] train_loss: 0.014439\n",
      "[111/00713] train_loss: 0.014141\n",
      "[111/00763] train_loss: 0.013029\n",
      "[111/00813] train_loss: 0.013654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[111/00863] train_loss: 0.015096\n",
      "[111/00913] train_loss: 0.014206\n",
      "[111/00963] train_loss: 0.014053\n",
      "[111/01013] train_loss: 0.014382\n",
      "[111/01063] train_loss: 0.014656\n",
      "[111/01113] train_loss: 0.014136\n",
      "[111/01163] train_loss: 0.013708\n",
      "[111/01213] train_loss: 0.014810\n",
      "[112/00037] train_loss: 0.015586\n",
      "[112/00087] train_loss: 0.015185\n",
      "[112/00137] train_loss: 0.015371\n",
      "[112/00187] train_loss: 0.015058\n",
      "[112/00237] train_loss: 0.015036\n",
      "[112/00287] train_loss: 0.015189\n",
      "[112/00337] train_loss: 0.013777\n",
      "[112/00387] train_loss: 0.013768\n",
      "[112/00437] train_loss: 0.013953\n",
      "[112/00487] train_loss: 0.014570\n",
      "[112/00537] train_loss: 0.014846\n",
      "[112/00587] train_loss: 0.014049\n",
      "[112/00637] train_loss: 0.013951\n",
      "[112/00687] train_loss: 0.013687\n",
      "[112/00737] train_loss: 0.014570\n",
      "[112/00787] train_loss: 0.014226\n",
      "[112/00837] train_loss: 0.013845\n",
      "[112/00887] train_loss: 0.013804\n",
      "[112/00937] train_loss: 0.014150\n",
      "[112/00987] train_loss: 0.014399\n",
      "[112/01037] train_loss: 0.014064\n",
      "[112/01087] train_loss: 0.014700\n",
      "[112/01137] train_loss: 0.013631\n",
      "[112/01187] train_loss: 0.013432\n",
      "[113/00011] train_loss: 0.014796\n",
      "[113/00061] train_loss: 0.016895\n",
      "[113/00111] train_loss: 0.015677\n",
      "[113/00161] train_loss: 0.015990\n",
      "[113/00211] train_loss: 0.015451\n",
      "[113/00261] train_loss: 0.014511\n",
      "[113/00311] train_loss: 0.014305\n",
      "[113/00361] train_loss: 0.013523\n",
      "[113/00411] train_loss: 0.014857\n",
      "[113/00461] train_loss: 0.013991\n",
      "[113/00511] train_loss: 0.014309\n",
      "[113/00561] train_loss: 0.014667\n",
      "[113/00611] train_loss: 0.013544\n",
      "[113/00661] train_loss: 0.014178\n",
      "[113/00711] train_loss: 0.013681\n",
      "[113/00761] train_loss: 0.014345\n",
      "[113/00811] train_loss: 0.014106\n",
      "[113/00861] train_loss: 0.014062\n",
      "[113/00911] train_loss: 0.014665\n",
      "[113/00961] train_loss: 0.013642\n",
      "[113/01011] train_loss: 0.013816\n",
      "[113/01061] train_loss: 0.014041\n",
      "[113/01111] train_loss: 0.013486\n",
      "[113/01161] train_loss: 0.013554\n",
      "[113/01211] train_loss: 0.014714\n",
      "[114/00035] train_loss: 0.015681\n",
      "[114/00085] train_loss: 0.015248\n",
      "[114/00135] train_loss: 0.015375\n",
      "[114/00185] train_loss: 0.014701\n",
      "[114/00235] train_loss: 0.014921\n",
      "[114/00285] train_loss: 0.015675\n",
      "[114/00335] train_loss: 0.014768\n",
      "[114/00385] train_loss: 0.014239\n",
      "[114/00435] train_loss: 0.014196\n",
      "[114/00485] train_loss: 0.014022\n",
      "[114/00535] train_loss: 0.014201\n",
      "[114/00585] train_loss: 0.014618\n",
      "[114/00635] train_loss: 0.013266\n",
      "[114/00685] train_loss: 0.014124\n",
      "[114/00735] train_loss: 0.014441\n",
      "[114/00785] train_loss: 0.014086\n",
      "[114/00835] train_loss: 0.014690\n",
      "[114/00885] train_loss: 0.013910\n",
      "[114/00935] train_loss: 0.013990\n",
      "[114/00985] train_loss: 0.014831\n",
      "[114/01035] train_loss: 0.014628\n",
      "[114/01085] train_loss: 0.014544\n",
      "[114/01135] train_loss: 0.014073\n",
      "[114/01185] train_loss: 0.013269\n",
      "[115/00009] train_loss: 0.013511\n",
      "[115/00059] train_loss: 0.015472\n",
      "[115/00109] train_loss: 0.016070\n",
      "[115/00159] train_loss: 0.014762\n",
      "[115/00209] train_loss: 0.013778\n",
      "[115/00259] train_loss: 0.014297\n",
      "[115/00309] train_loss: 0.014410\n",
      "[115/00359] train_loss: 0.014772\n",
      "[115/00409] train_loss: 0.013585\n",
      "[115/00459] train_loss: 0.014270\n",
      "[115/00509] train_loss: 0.013676\n",
      "[115/00559] train_loss: 0.014180\n",
      "[115/00609] train_loss: 0.014248\n",
      "[115/00659] train_loss: 0.014378\n",
      "[115/00709] train_loss: 0.013626\n",
      "[115/00759] train_loss: 0.014222\n",
      "[115/00809] train_loss: 0.013603\n",
      "[115/00859] train_loss: 0.013813\n",
      "[115/00909] train_loss: 0.014167\n",
      "[115/00959] train_loss: 0.013160\n",
      "[115/01009] train_loss: 0.013774\n",
      "[115/01059] train_loss: 0.014228\n",
      "[115/01109] train_loss: 0.014139\n",
      "[115/01159] train_loss: 0.014426\n",
      "[115/01209] train_loss: 0.013479\n",
      "[116/00033] train_loss: 0.016132\n",
      "[116/00083] train_loss: 0.016331\n",
      "[116/00133] train_loss: 0.015042\n",
      "[116/00183] train_loss: 0.015271\n",
      "[116/00233] train_loss: 0.015012\n",
      "[116/00283] train_loss: 0.015324\n",
      "[116/00333] train_loss: 0.014745\n",
      "[116/00383] train_loss: 0.014632\n",
      "[116/00433] train_loss: 0.014484\n",
      "[116/00483] train_loss: 0.013640\n",
      "[116/00533] train_loss: 0.013546\n",
      "[116/00583] train_loss: 0.015628\n",
      "[116/00633] train_loss: 0.013886\n",
      "[116/00683] train_loss: 0.014006\n",
      "[116/00733] train_loss: 0.013957\n",
      "[116/00783] train_loss: 0.013620\n",
      "[116/00833] train_loss: 0.014406\n",
      "[116/00883] train_loss: 0.013731\n",
      "[116/00933] train_loss: 0.014395\n",
      "[116/00983] train_loss: 0.013436\n",
      "[116/01033] train_loss: 0.013479\n",
      "[116/01083] train_loss: 0.013755\n",
      "[116/01133] train_loss: 0.013417\n",
      "[116/01183] train_loss: 0.013635\n",
      "[117/00007] train_loss: 0.014143\n",
      "[117/00057] train_loss: 0.014968\n",
      "[117/00107] train_loss: 0.015430\n",
      "[117/00157] train_loss: 0.015085\n",
      "[117/00207] train_loss: 0.013887\n",
      "[117/00257] train_loss: 0.014245\n",
      "[117/00307] train_loss: 0.013948\n",
      "[117/00357] train_loss: 0.014424\n",
      "[117/00407] train_loss: 0.014402\n",
      "[117/00457] train_loss: 0.013717\n",
      "[117/00507] train_loss: 0.014319\n",
      "[117/00557] train_loss: 0.014379\n",
      "[117/00607] train_loss: 0.013961\n",
      "[117/00657] train_loss: 0.014255\n",
      "[117/00707] train_loss: 0.013867\n",
      "[117/00757] train_loss: 0.014772\n",
      "[117/00807] train_loss: 0.014282\n",
      "[117/00857] train_loss: 0.013738\n",
      "[117/00907] train_loss: 0.014683\n",
      "[117/00957] train_loss: 0.013666\n",
      "[117/01007] train_loss: 0.013117\n",
      "[117/01057] train_loss: 0.013998\n",
      "[117/01107] train_loss: 0.014515\n",
      "[117/01157] train_loss: 0.013577\n",
      "[117/01207] train_loss: 0.014250\n",
      "[118/00031] train_loss: 0.015004\n",
      "[118/00081] train_loss: 0.015526\n",
      "[118/00131] train_loss: 0.014459\n",
      "[118/00181] train_loss: 0.015079\n",
      "[118/00231] train_loss: 0.014446\n",
      "[118/00281] train_loss: 0.014865\n",
      "[118/00331] train_loss: 0.014130\n",
      "[118/00381] train_loss: 0.014704\n",
      "[118/00431] train_loss: 0.013898\n",
      "[118/00481] train_loss: 0.013602\n",
      "[118/00531] train_loss: 0.014824\n",
      "[118/00581] train_loss: 0.013566\n",
      "[118/00631] train_loss: 0.013301\n",
      "[118/00681] train_loss: 0.013476\n",
      "[118/00731] train_loss: 0.014256\n",
      "[118/00781] train_loss: 0.014008\n",
      "[118/00831] train_loss: 0.014409\n",
      "[118/00881] train_loss: 0.014097\n",
      "[118/00931] train_loss: 0.014073\n",
      "[118/00981] train_loss: 0.013558\n",
      "[118/01031] train_loss: 0.014574\n",
      "[118/01081] train_loss: 0.014168\n",
      "[118/01131] train_loss: 0.013859\n",
      "[118/01181] train_loss: 0.013662\n",
      "[119/00005] train_loss: 0.014580\n",
      "[119/00055] train_loss: 0.015558\n",
      "[119/00105] train_loss: 0.014889\n",
      "[119/00155] train_loss: 0.014829\n",
      "[119/00205] train_loss: 0.014156\n",
      "[119/00255] train_loss: 0.014753\n",
      "[119/00305] train_loss: 0.014568\n",
      "[119/00355] train_loss: 0.014735\n",
      "[119/00405] train_loss: 0.014336\n",
      "[119/00455] train_loss: 0.014396\n",
      "[119/00505] train_loss: 0.014583\n",
      "[119/00555] train_loss: 0.013328\n",
      "[119/00605] train_loss: 0.013614\n",
      "[119/00655] train_loss: 0.014359\n",
      "[119/00705] train_loss: 0.014409\n",
      "[119/00755] train_loss: 0.014179\n",
      "[119/00805] train_loss: 0.014177\n",
      "[119/00855] train_loss: 0.013885\n",
      "[119/00905] train_loss: 0.014128\n",
      "[119/00955] train_loss: 0.014010\n",
      "[119/01005] train_loss: 0.013558\n",
      "[119/01055] train_loss: 0.013776\n",
      "[119/01105] train_loss: 0.013768\n",
      "[119/01155] train_loss: 0.013058\n",
      "[119/01205] train_loss: 0.013024\n",
      "[120/00029] train_loss: 0.014796\n",
      "[120/00079] train_loss: 0.015722\n",
      "[120/00129] train_loss: 0.014646\n",
      "[120/00179] train_loss: 0.014855\n",
      "[120/00229] train_loss: 0.015117\n",
      "[120/00279] train_loss: 0.014639\n",
      "[120/00329] train_loss: 0.013762\n",
      "[120/00379] train_loss: 0.013982\n",
      "[120/00429] train_loss: 0.014498\n",
      "[120/00479] train_loss: 0.013091\n",
      "[120/00529] train_loss: 0.013877\n",
      "[120/00579] train_loss: 0.014218\n",
      "[120/00629] train_loss: 0.014579\n",
      "[120/00679] train_loss: 0.013914\n",
      "[120/00729] train_loss: 0.014514\n",
      "[120/00779] train_loss: 0.014327\n",
      "[120/00829] train_loss: 0.013538\n",
      "[120/00879] train_loss: 0.014263\n",
      "[120/00929] train_loss: 0.013991\n",
      "[120/00979] train_loss: 0.014606\n",
      "[120/01029] train_loss: 0.014010\n",
      "[120/01079] train_loss: 0.013102\n",
      "[120/01129] train_loss: 0.014094\n",
      "[120/01179] train_loss: 0.014688\n",
      "[121/00003] train_loss: 0.014002\n",
      "[121/00053] train_loss: 0.015758\n",
      "[121/00103] train_loss: 0.014894\n",
      "[121/00153] train_loss: 0.013948\n",
      "[121/00203] train_loss: 0.014293\n",
      "[121/00253] train_loss: 0.014233\n",
      "[121/00303] train_loss: 0.014559\n",
      "[121/00353] train_loss: 0.014114\n",
      "[121/00403] train_loss: 0.014092\n",
      "[121/00453] train_loss: 0.014334\n",
      "[121/00503] train_loss: 0.013482\n",
      "[121/00553] train_loss: 0.013359\n",
      "[121/00603] train_loss: 0.013845\n",
      "[121/00653] train_loss: 0.013970\n",
      "[121/00703] train_loss: 0.014354\n",
      "[121/00753] train_loss: 0.014364\n",
      "[121/00803] train_loss: 0.013815\n",
      "[121/00853] train_loss: 0.014498\n",
      "[121/00903] train_loss: 0.013673\n",
      "[121/00953] train_loss: 0.013569\n",
      "[121/01003] train_loss: 0.013412\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[121/01053] train_loss: 0.013809\n",
      "[121/01103] train_loss: 0.014419\n",
      "[121/01153] train_loss: 0.014165\n",
      "[121/01203] train_loss: 0.013129\n",
      "[122/00027] train_loss: 0.014502\n",
      "[122/00077] train_loss: 0.015770\n",
      "[122/00127] train_loss: 0.015069\n",
      "[122/00177] train_loss: 0.015823\n",
      "[122/00227] train_loss: 0.015371\n",
      "[122/00277] train_loss: 0.015011\n",
      "[122/00327] train_loss: 0.013950\n",
      "[122/00377] train_loss: 0.013604\n",
      "[122/00427] train_loss: 0.014138\n",
      "[122/00477] train_loss: 0.014326\n",
      "[122/00527] train_loss: 0.014117\n",
      "[122/00577] train_loss: 0.014322\n",
      "[122/00627] train_loss: 0.014414\n",
      "[122/00677] train_loss: 0.014024\n",
      "[122/00727] train_loss: 0.013590\n",
      "[122/00777] train_loss: 0.013887\n",
      "[122/00827] train_loss: 0.014688\n",
      "[122/00877] train_loss: 0.014239\n",
      "[122/00927] train_loss: 0.013894\n",
      "[122/00977] train_loss: 0.013060\n",
      "[122/01027] train_loss: 0.013478\n",
      "[122/01077] train_loss: 0.013785\n",
      "[122/01127] train_loss: 0.013553\n",
      "[122/01177] train_loss: 0.013757\n",
      "[123/00001] train_loss: 0.014186\n",
      "[123/00051] train_loss: 0.015886\n",
      "[123/00101] train_loss: 0.015576\n",
      "[123/00151] train_loss: 0.014712\n",
      "[123/00201] train_loss: 0.014442\n",
      "[123/00251] train_loss: 0.014441\n",
      "[123/00301] train_loss: 0.014192\n",
      "[123/00351] train_loss: 0.014115\n",
      "[123/00401] train_loss: 0.013895\n",
      "[123/00451] train_loss: 0.014222\n",
      "[123/00501] train_loss: 0.013730\n",
      "[123/00551] train_loss: 0.013896\n",
      "[123/00601] train_loss: 0.014308\n",
      "[123/00651] train_loss: 0.013172\n",
      "[123/00701] train_loss: 0.013530\n",
      "[123/00751] train_loss: 0.013945\n",
      "[123/00801] train_loss: 0.013779\n",
      "[123/00851] train_loss: 0.013827\n",
      "[123/00901] train_loss: 0.014645\n",
      "[123/00951] train_loss: 0.013146\n",
      "[123/01001] train_loss: 0.013869\n",
      "[123/01051] train_loss: 0.014314\n",
      "[123/01101] train_loss: 0.013775\n",
      "[123/01151] train_loss: 0.014134\n",
      "[123/01201] train_loss: 0.014162\n",
      "[124/00025] train_loss: 0.014547\n",
      "[124/00075] train_loss: 0.014843\n",
      "[124/00125] train_loss: 0.014333\n",
      "[124/00175] train_loss: 0.014219\n",
      "[124/00225] train_loss: 0.014348\n",
      "[124/00275] train_loss: 0.014583\n",
      "[124/00325] train_loss: 0.014578\n",
      "[124/00375] train_loss: 0.014648\n",
      "[124/00425] train_loss: 0.014977\n",
      "[124/00475] train_loss: 0.013386\n",
      "[124/00525] train_loss: 0.014285\n",
      "[124/00575] train_loss: 0.013736\n",
      "[124/00625] train_loss: 0.014605\n",
      "[124/00675] train_loss: 0.014267\n",
      "[124/00725] train_loss: 0.014349\n",
      "[124/00775] train_loss: 0.013885\n",
      "[124/00825] train_loss: 0.014136\n",
      "[124/00875] train_loss: 0.014563\n",
      "[124/00925] train_loss: 0.013941\n",
      "[124/00975] train_loss: 0.014684\n",
      "[124/01025] train_loss: 0.014716\n",
      "[124/01075] train_loss: 0.014238\n",
      "[124/01125] train_loss: 0.013637\n",
      "[124/01175] train_loss: 0.014068\n",
      "[124/01225] train_loss: 0.013868\n",
      "[125/00049] train_loss: 0.016138\n",
      "[125/00099] train_loss: 0.014909\n",
      "[125/00149] train_loss: 0.015508\n",
      "[125/00199] train_loss: 0.015128\n",
      "[125/00249] train_loss: 0.013580\n",
      "[125/00299] train_loss: 0.014659\n",
      "[125/00349] train_loss: 0.013877\n",
      "[125/00399] train_loss: 0.013076\n",
      "[125/00449] train_loss: 0.014135\n",
      "[125/00499] train_loss: 0.013958\n",
      "[125/00549] train_loss: 0.013916\n",
      "[125/00599] train_loss: 0.013763\n",
      "[125/00649] train_loss: 0.013860\n",
      "[125/00699] train_loss: 0.014189\n",
      "[125/00749] train_loss: 0.013454\n",
      "[125/00799] train_loss: 0.013101\n",
      "[125/00849] train_loss: 0.013624\n",
      "[125/00899] train_loss: 0.013801\n",
      "[125/00949] train_loss: 0.013386\n",
      "[125/00999] train_loss: 0.013758\n",
      "[125/01049] train_loss: 0.014995\n",
      "[125/01099] train_loss: 0.013379\n",
      "[125/01149] train_loss: 0.014242\n",
      "[125/01199] train_loss: 0.013910\n",
      "[126/00023] train_loss: 0.014265\n",
      "[126/00073] train_loss: 0.015529\n",
      "[126/00123] train_loss: 0.014762\n",
      "[126/00173] train_loss: 0.015483\n",
      "[126/00223] train_loss: 0.014543\n",
      "[126/00273] train_loss: 0.014342\n",
      "[126/00323] train_loss: 0.014136\n",
      "[126/00373] train_loss: 0.014829\n",
      "[126/00423] train_loss: 0.013722\n",
      "[126/00473] train_loss: 0.013252\n",
      "[126/00523] train_loss: 0.013594\n",
      "[126/00573] train_loss: 0.014454\n",
      "[126/00623] train_loss: 0.014304\n",
      "[126/00673] train_loss: 0.014593\n",
      "[126/00723] train_loss: 0.013702\n",
      "[126/00773] train_loss: 0.014052\n",
      "[126/00823] train_loss: 0.014391\n",
      "[126/00873] train_loss: 0.014518\n",
      "[126/00923] train_loss: 0.013653\n",
      "[126/00973] train_loss: 0.013515\n",
      "[126/01023] train_loss: 0.013217\n",
      "[126/01073] train_loss: 0.014298\n",
      "[126/01123] train_loss: 0.014077\n",
      "[126/01173] train_loss: 0.014175\n",
      "[126/01223] train_loss: 0.014330\n",
      "[127/00047] train_loss: 0.014232\n",
      "[127/00097] train_loss: 0.015402\n",
      "[127/00147] train_loss: 0.014212\n",
      "[127/00197] train_loss: 0.013525\n",
      "[127/00247] train_loss: 0.014116\n",
      "[127/00297] train_loss: 0.015010\n",
      "[127/00347] train_loss: 0.015042\n",
      "[127/00397] train_loss: 0.013754\n",
      "[127/00447] train_loss: 0.013754\n",
      "[127/00497] train_loss: 0.014634\n",
      "[127/00547] train_loss: 0.013881\n",
      "[127/00597] train_loss: 0.014058\n",
      "[127/00647] train_loss: 0.014501\n",
      "[127/00697] train_loss: 0.014431\n",
      "[127/00747] train_loss: 0.013657\n",
      "[127/00797] train_loss: 0.013949\n",
      "[127/00847] train_loss: 0.013519\n",
      "[127/00897] train_loss: 0.014005\n",
      "[127/00947] train_loss: 0.013779\n",
      "[127/00997] train_loss: 0.013370\n",
      "[127/01047] train_loss: 0.012498\n",
      "[127/01097] train_loss: 0.014012\n",
      "[127/01147] train_loss: 0.013805\n",
      "[127/01197] train_loss: 0.013787\n",
      "[128/00021] train_loss: 0.014215\n",
      "[128/00071] train_loss: 0.014959\n",
      "[128/00121] train_loss: 0.014339\n",
      "[128/00171] train_loss: 0.013957\n",
      "[128/00221] train_loss: 0.014004\n",
      "[128/00271] train_loss: 0.014976\n",
      "[128/00321] train_loss: 0.014850\n",
      "[128/00371] train_loss: 0.014022\n",
      "[128/00421] train_loss: 0.014016\n",
      "[128/00471] train_loss: 0.013477\n",
      "[128/00521] train_loss: 0.014362\n",
      "[128/00571] train_loss: 0.013957\n",
      "[128/00621] train_loss: 0.014215\n",
      "[128/00671] train_loss: 0.013883\n",
      "[128/00721] train_loss: 0.013670\n",
      "[128/00771] train_loss: 0.013810\n",
      "[128/00821] train_loss: 0.014712\n",
      "[128/00871] train_loss: 0.013875\n",
      "[128/00921] train_loss: 0.013083\n",
      "[128/00971] train_loss: 0.013368\n",
      "[128/01021] train_loss: 0.013953\n",
      "[128/01071] train_loss: 0.013875\n",
      "[128/01121] train_loss: 0.013548\n",
      "[128/01171] train_loss: 0.013828\n",
      "[128/01221] train_loss: 0.013724\n",
      "[129/00045] train_loss: 0.015284\n",
      "[129/00095] train_loss: 0.014772\n",
      "[129/00145] train_loss: 0.014310\n",
      "[129/00195] train_loss: 0.014352\n",
      "[129/00245] train_loss: 0.014095\n",
      "[129/00295] train_loss: 0.014172\n",
      "[129/00345] train_loss: 0.013549\n",
      "[129/00395] train_loss: 0.014532\n",
      "[129/00445] train_loss: 0.013481\n",
      "[129/00495] train_loss: 0.014256\n",
      "[129/00545] train_loss: 0.014075\n",
      "[129/00595] train_loss: 0.014294\n",
      "[129/00645] train_loss: 0.014857\n",
      "[129/00695] train_loss: 0.013515\n",
      "[129/00745] train_loss: 0.012884\n",
      "[129/00795] train_loss: 0.013114\n",
      "[129/00845] train_loss: 0.014616\n",
      "[129/00895] train_loss: 0.013530\n",
      "[129/00945] train_loss: 0.014531\n",
      "[129/00995] train_loss: 0.014249\n",
      "[129/01045] train_loss: 0.013431\n",
      "[129/01095] train_loss: 0.013582\n",
      "[129/01145] train_loss: 0.014394\n",
      "[129/01195] train_loss: 0.013602\n",
      "[130/00019] train_loss: 0.014305\n",
      "[130/00069] train_loss: 0.015923\n",
      "[130/00119] train_loss: 0.016168\n",
      "[130/00169] train_loss: 0.014708\n",
      "[130/00219] train_loss: 0.014639\n",
      "[130/00269] train_loss: 0.013677\n",
      "[130/00319] train_loss: 0.014155\n",
      "[130/00369] train_loss: 0.014016\n",
      "[130/00419] train_loss: 0.013535\n",
      "[130/00469] train_loss: 0.014544\n",
      "[130/00519] train_loss: 0.013769\n",
      "[130/00569] train_loss: 0.013658\n",
      "[130/00619] train_loss: 0.014500\n",
      "[130/00669] train_loss: 0.013380\n",
      "[130/00719] train_loss: 0.012774\n",
      "[130/00769] train_loss: 0.013462\n",
      "[130/00819] train_loss: 0.014203\n",
      "[130/00869] train_loss: 0.013544\n",
      "[130/00919] train_loss: 0.014004\n",
      "[130/00969] train_loss: 0.012976\n",
      "[130/01019] train_loss: 0.013664\n",
      "[130/01069] train_loss: 0.013457\n",
      "[130/01119] train_loss: 0.013330\n",
      "[130/01169] train_loss: 0.014219\n",
      "[130/01219] train_loss: 0.014522\n",
      "[131/00043] train_loss: 0.014860\n",
      "[131/00093] train_loss: 0.014935\n",
      "[131/00143] train_loss: 0.015021\n",
      "[131/00193] train_loss: 0.014778\n",
      "[131/00243] train_loss: 0.014456\n",
      "[131/00293] train_loss: 0.013496\n",
      "[131/00343] train_loss: 0.013561\n",
      "[131/00393] train_loss: 0.013770\n",
      "[131/00443] train_loss: 0.013488\n",
      "[131/00493] train_loss: 0.014108\n",
      "[131/00543] train_loss: 0.014330\n",
      "[131/00593] train_loss: 0.014323\n",
      "[131/00643] train_loss: 0.013437\n",
      "[131/00693] train_loss: 0.013594\n",
      "[131/00743] train_loss: 0.013842\n",
      "[131/00793] train_loss: 0.012824\n",
      "[131/00843] train_loss: 0.014323\n",
      "[131/00893] train_loss: 0.013716\n",
      "[131/00943] train_loss: 0.013332\n",
      "[131/00993] train_loss: 0.013236\n",
      "[131/01043] train_loss: 0.013942\n",
      "[131/01093] train_loss: 0.013184\n",
      "[131/01143] train_loss: 0.014213\n",
      "[131/01193] train_loss: 0.014090\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[132/00017] train_loss: 0.014947\n",
      "[132/00067] train_loss: 0.015625\n",
      "[132/00117] train_loss: 0.014516\n",
      "[132/00167] train_loss: 0.014926\n",
      "[132/00217] train_loss: 0.014953\n",
      "[132/00267] train_loss: 0.014336\n",
      "[132/00317] train_loss: 0.014174\n",
      "[132/00367] train_loss: 0.014498\n",
      "[132/00417] train_loss: 0.014398\n",
      "[132/00467] train_loss: 0.014415\n",
      "[132/00517] train_loss: 0.013148\n",
      "[132/00567] train_loss: 0.013492\n",
      "[132/00617] train_loss: 0.013245\n",
      "[132/00667] train_loss: 0.013082\n",
      "[132/00717] train_loss: 0.014623\n",
      "[132/00767] train_loss: 0.013384\n",
      "[132/00817] train_loss: 0.013490\n",
      "[132/00867] train_loss: 0.014062\n",
      "[132/00917] train_loss: 0.013602\n",
      "[132/00967] train_loss: 0.014179\n",
      "[132/01017] train_loss: 0.014271\n",
      "[132/01067] train_loss: 0.013828\n",
      "[132/01117] train_loss: 0.013705\n",
      "[132/01167] train_loss: 0.013410\n",
      "[132/01217] train_loss: 0.012922\n",
      "[133/00041] train_loss: 0.014856\n",
      "[133/00091] train_loss: 0.014978\n",
      "[133/00141] train_loss: 0.014473\n",
      "[133/00191] train_loss: 0.014577\n",
      "[133/00241] train_loss: 0.015080\n",
      "[133/00291] train_loss: 0.015378\n",
      "[133/00341] train_loss: 0.012995\n",
      "[133/00391] train_loss: 0.013843\n",
      "[133/00441] train_loss: 0.014096\n",
      "[133/00491] train_loss: 0.014039\n",
      "[133/00541] train_loss: 0.013628\n",
      "[133/00591] train_loss: 0.013936\n",
      "[133/00641] train_loss: 0.013378\n",
      "[133/00691] train_loss: 0.013604\n",
      "[133/00741] train_loss: 0.013589\n",
      "[133/00791] train_loss: 0.013856\n",
      "[133/00841] train_loss: 0.013673\n",
      "[133/00891] train_loss: 0.013785\n",
      "[133/00941] train_loss: 0.013501\n",
      "[133/00991] train_loss: 0.013989\n",
      "[133/01041] train_loss: 0.013402\n",
      "[133/01091] train_loss: 0.013617\n",
      "[133/01141] train_loss: 0.013139\n",
      "[133/01191] train_loss: 0.013558\n",
      "[134/00015] train_loss: 0.014414\n",
      "[134/00065] train_loss: 0.015661\n",
      "[134/00115] train_loss: 0.014746\n",
      "[134/00165] train_loss: 0.014503\n",
      "[134/00215] train_loss: 0.013751\n",
      "[134/00265] train_loss: 0.014417\n",
      "[134/00315] train_loss: 0.014030\n",
      "[134/00365] train_loss: 0.014071\n",
      "[134/00415] train_loss: 0.013787\n",
      "[134/00465] train_loss: 0.014145\n",
      "[134/00515] train_loss: 0.013920\n",
      "[134/00565] train_loss: 0.013716\n",
      "[134/00615] train_loss: 0.012792\n",
      "[134/00665] train_loss: 0.013019\n",
      "[134/00715] train_loss: 0.014866\n",
      "[134/00765] train_loss: 0.014522\n",
      "[134/00815] train_loss: 0.013443\n",
      "[134/00865] train_loss: 0.012888\n",
      "[134/00915] train_loss: 0.014119\n",
      "[134/00965] train_loss: 0.013861\n",
      "[134/01015] train_loss: 0.012923\n",
      "[134/01065] train_loss: 0.014176\n",
      "[134/01115] train_loss: 0.014657\n",
      "[134/01165] train_loss: 0.013058\n",
      "[134/01215] train_loss: 0.013473\n",
      "[135/00039] train_loss: 0.014834\n",
      "[135/00089] train_loss: 0.014731\n",
      "[135/00139] train_loss: 0.014454\n",
      "[135/00189] train_loss: 0.014527\n",
      "[135/00239] train_loss: 0.013587\n",
      "[135/00289] train_loss: 0.014680\n",
      "[135/00339] train_loss: 0.013638\n",
      "[135/00389] train_loss: 0.014197\n",
      "[135/00439] train_loss: 0.013715\n",
      "[135/00489] train_loss: 0.014072\n",
      "[135/00539] train_loss: 0.014438\n",
      "[135/00589] train_loss: 0.013788\n",
      "[135/00639] train_loss: 0.013741\n",
      "[135/00689] train_loss: 0.014458\n",
      "[135/00739] train_loss: 0.013424\n",
      "[135/00789] train_loss: 0.013967\n",
      "[135/00839] train_loss: 0.013460\n",
      "[135/00889] train_loss: 0.013978\n",
      "[135/00939] train_loss: 0.013450\n",
      "[135/00989] train_loss: 0.013882\n",
      "[135/01039] train_loss: 0.013334\n",
      "[135/01089] train_loss: 0.014143\n",
      "[135/01139] train_loss: 0.013588\n",
      "[135/01189] train_loss: 0.013037\n",
      "[136/00013] train_loss: 0.013347\n",
      "[136/00063] train_loss: 0.015222\n",
      "[136/00113] train_loss: 0.015076\n",
      "[136/00163] train_loss: 0.014620\n",
      "[136/00213] train_loss: 0.013609\n",
      "[136/00263] train_loss: 0.014395\n",
      "[136/00313] train_loss: 0.013754\n",
      "[136/00363] train_loss: 0.014462\n",
      "[136/00413] train_loss: 0.014388\n",
      "[136/00463] train_loss: 0.013775\n",
      "[136/00513] train_loss: 0.014599\n",
      "[136/00563] train_loss: 0.014122\n",
      "[136/00613] train_loss: 0.012973\n",
      "[136/00663] train_loss: 0.014090\n",
      "[136/00713] train_loss: 0.013496\n",
      "[136/00763] train_loss: 0.013044\n",
      "[136/00813] train_loss: 0.014248\n",
      "[136/00863] train_loss: 0.013749\n",
      "[136/00913] train_loss: 0.013627\n",
      "[136/00963] train_loss: 0.013370\n",
      "[136/01013] train_loss: 0.013854\n",
      "[136/01063] train_loss: 0.014795\n",
      "[136/01113] train_loss: 0.013635\n",
      "[136/01163] train_loss: 0.012927\n",
      "[136/01213] train_loss: 0.013174\n",
      "[137/00037] train_loss: 0.014212\n",
      "[137/00087] train_loss: 0.014296\n",
      "[137/00137] train_loss: 0.014732\n",
      "[137/00187] train_loss: 0.014221\n",
      "[137/00237] train_loss: 0.014373\n",
      "[137/00287] train_loss: 0.013562\n",
      "[137/00337] train_loss: 0.013722\n",
      "[137/00387] train_loss: 0.013432\n",
      "[137/00437] train_loss: 0.012695\n",
      "[137/00487] train_loss: 0.014145\n",
      "[137/00537] train_loss: 0.013468\n",
      "[137/00587] train_loss: 0.014125\n",
      "[137/00637] train_loss: 0.013983\n",
      "[137/00687] train_loss: 0.013538\n",
      "[137/00737] train_loss: 0.013332\n",
      "[137/00787] train_loss: 0.013862\n",
      "[137/00837] train_loss: 0.014077\n",
      "[137/00887] train_loss: 0.013130\n",
      "[137/00937] train_loss: 0.013442\n",
      "[137/00987] train_loss: 0.014147\n",
      "[137/01037] train_loss: 0.015023\n",
      "[137/01087] train_loss: 0.013333\n",
      "[137/01137] train_loss: 0.013685\n",
      "[137/01187] train_loss: 0.013299\n",
      "[138/00011] train_loss: 0.014156\n",
      "[138/00061] train_loss: 0.014953\n",
      "[138/00111] train_loss: 0.014196\n",
      "[138/00161] train_loss: 0.014147\n",
      "[138/00211] train_loss: 0.013743\n",
      "[138/00261] train_loss: 0.014500\n",
      "[138/00311] train_loss: 0.014146\n",
      "[138/00361] train_loss: 0.014363\n",
      "[138/00411] train_loss: 0.013706\n",
      "[138/00461] train_loss: 0.013524\n",
      "[138/00511] train_loss: 0.013854\n",
      "[138/00561] train_loss: 0.013436\n",
      "[138/00611] train_loss: 0.014881\n",
      "[138/00661] train_loss: 0.012870\n",
      "[138/00711] train_loss: 0.013409\n",
      "[138/00761] train_loss: 0.014024\n",
      "[138/00811] train_loss: 0.013549\n",
      "[138/00861] train_loss: 0.014619\n",
      "[138/00911] train_loss: 0.013822\n",
      "[138/00961] train_loss: 0.013252\n",
      "[138/01011] train_loss: 0.013504\n",
      "[138/01061] train_loss: 0.013030\n",
      "[138/01111] train_loss: 0.013887\n",
      "[138/01161] train_loss: 0.013523\n",
      "[138/01211] train_loss: 0.012977\n",
      "[139/00035] train_loss: 0.014193\n",
      "[139/00085] train_loss: 0.014385\n",
      "[139/00135] train_loss: 0.014171\n",
      "[139/00185] train_loss: 0.014692\n",
      "[139/00235] train_loss: 0.014110\n",
      "[139/00285] train_loss: 0.014100\n",
      "[139/00335] train_loss: 0.013631\n",
      "[139/00385] train_loss: 0.014252\n",
      "[139/00435] train_loss: 0.014322\n",
      "[139/00485] train_loss: 0.014113\n",
      "[139/00535] train_loss: 0.013489\n",
      "[139/00585] train_loss: 0.013217\n",
      "[139/00635] train_loss: 0.013276\n",
      "[139/00685] train_loss: 0.014775\n",
      "[139/00735] train_loss: 0.013472\n",
      "[139/00785] train_loss: 0.013536\n",
      "[139/00835] train_loss: 0.014001\n",
      "[139/00885] train_loss: 0.013301\n",
      "[139/00935] train_loss: 0.014082\n",
      "[139/00985] train_loss: 0.014015\n",
      "[139/01035] train_loss: 0.013524\n",
      "[139/01085] train_loss: 0.012414\n",
      "[139/01135] train_loss: 0.013543\n",
      "[139/01185] train_loss: 0.013656\n",
      "[140/00009] train_loss: 0.013609\n",
      "[140/00059] train_loss: 0.014528\n",
      "[140/00109] train_loss: 0.015088\n",
      "[140/00159] train_loss: 0.015621\n",
      "[140/00209] train_loss: 0.014090\n",
      "[140/00259] train_loss: 0.013213\n",
      "[140/00309] train_loss: 0.013378\n",
      "[140/00359] train_loss: 0.013262\n",
      "[140/00409] train_loss: 0.013730\n",
      "[140/00459] train_loss: 0.013738\n",
      "[140/00509] train_loss: 0.013765\n",
      "[140/00559] train_loss: 0.013329\n",
      "[140/00609] train_loss: 0.014048\n",
      "[140/00659] train_loss: 0.013470\n",
      "[140/00709] train_loss: 0.013793\n",
      "[140/00759] train_loss: 0.014004\n",
      "[140/00809] train_loss: 0.013186\n",
      "[140/00859] train_loss: 0.014198\n",
      "[140/00909] train_loss: 0.013880\n",
      "[140/00959] train_loss: 0.013849\n",
      "[140/01009] train_loss: 0.014402\n",
      "[140/01059] train_loss: 0.013701\n",
      "[140/01109] train_loss: 0.013373\n",
      "[140/01159] train_loss: 0.013700\n",
      "[140/01209] train_loss: 0.013259\n",
      "[141/00033] train_loss: 0.014629\n",
      "[141/00083] train_loss: 0.015013\n",
      "[141/00133] train_loss: 0.014669\n",
      "[141/00183] train_loss: 0.013643\n",
      "[141/00233] train_loss: 0.014615\n",
      "[141/00283] train_loss: 0.013857\n",
      "[141/00333] train_loss: 0.013098\n",
      "[141/00383] train_loss: 0.014197\n",
      "[141/00433] train_loss: 0.013676\n",
      "[141/00483] train_loss: 0.013066\n",
      "[141/00533] train_loss: 0.014235\n",
      "[141/00583] train_loss: 0.013272\n",
      "[141/00633] train_loss: 0.013333\n",
      "[141/00683] train_loss: 0.013105\n",
      "[141/00733] train_loss: 0.013594\n",
      "[141/00783] train_loss: 0.013386\n",
      "[141/00833] train_loss: 0.012746\n",
      "[141/00883] train_loss: 0.013249\n",
      "[141/00933] train_loss: 0.013654\n",
      "[141/00983] train_loss: 0.013750\n",
      "[141/01033] train_loss: 0.013594\n",
      "[141/01083] train_loss: 0.013646\n",
      "[141/01133] train_loss: 0.013496\n",
      "[141/01183] train_loss: 0.013813\n",
      "[142/00007] train_loss: 0.013244\n",
      "[142/00057] train_loss: 0.015129\n",
      "[142/00107] train_loss: 0.014652\n",
      "[142/00157] train_loss: 0.014229\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[142/00207] train_loss: 0.014967\n",
      "[142/00257] train_loss: 0.013699\n",
      "[142/00307] train_loss: 0.014117\n",
      "[142/00357] train_loss: 0.014051\n",
      "[142/00407] train_loss: 0.013418\n",
      "[142/00457] train_loss: 0.013673\n",
      "[142/00507] train_loss: 0.014311\n",
      "[142/00557] train_loss: 0.013952\n",
      "[142/00607] train_loss: 0.014069\n",
      "[142/00657] train_loss: 0.013710\n",
      "[142/00707] train_loss: 0.013092\n",
      "[142/00757] train_loss: 0.012952\n",
      "[142/00807] train_loss: 0.013629\n",
      "[142/00857] train_loss: 0.013316\n",
      "[142/00907] train_loss: 0.013374\n",
      "[142/00957] train_loss: 0.013132\n",
      "[142/01007] train_loss: 0.013231\n",
      "[142/01057] train_loss: 0.013044\n",
      "[142/01107] train_loss: 0.013836\n",
      "[142/01157] train_loss: 0.013851\n",
      "[142/01207] train_loss: 0.013912\n",
      "[143/00031] train_loss: 0.014930\n",
      "[143/00081] train_loss: 0.015078\n",
      "[143/00131] train_loss: 0.014954\n",
      "[143/00181] train_loss: 0.013349\n",
      "[143/00231] train_loss: 0.014314\n",
      "[143/00281] train_loss: 0.013953\n",
      "[143/00331] train_loss: 0.013564\n",
      "[143/00381] train_loss: 0.013727\n",
      "[143/00431] train_loss: 0.013096\n",
      "[143/00481] train_loss: 0.013815\n",
      "[143/00531] train_loss: 0.012883\n",
      "[143/00581] train_loss: 0.013682\n",
      "[143/00631] train_loss: 0.014009\n",
      "[143/00681] train_loss: 0.013902\n",
      "[143/00731] train_loss: 0.014246\n",
      "[143/00781] train_loss: 0.012896\n",
      "[143/00831] train_loss: 0.013437\n",
      "[143/00881] train_loss: 0.013568\n",
      "[143/00931] train_loss: 0.013247\n",
      "[143/00981] train_loss: 0.013592\n",
      "[143/01031] train_loss: 0.013094\n",
      "[143/01081] train_loss: 0.013900\n",
      "[143/01131] train_loss: 0.013903\n",
      "[143/01181] train_loss: 0.013209\n",
      "[144/00005] train_loss: 0.013079\n",
      "[144/00055] train_loss: 0.014735\n",
      "[144/00105] train_loss: 0.014372\n",
      "[144/00155] train_loss: 0.014368\n",
      "[144/00205] train_loss: 0.013781\n",
      "[144/00255] train_loss: 0.014185\n",
      "[144/00305] train_loss: 0.014561\n",
      "[144/00355] train_loss: 0.013930\n",
      "[144/00405] train_loss: 0.013672\n",
      "[144/00455] train_loss: 0.013781\n",
      "[144/00505] train_loss: 0.014499\n",
      "[144/00555] train_loss: 0.013079\n",
      "[144/00605] train_loss: 0.013226\n",
      "[144/00655] train_loss: 0.013214\n",
      "[144/00705] train_loss: 0.013143\n",
      "[144/00755] train_loss: 0.013373\n",
      "[144/00805] train_loss: 0.013171\n",
      "[144/00855] train_loss: 0.013482\n",
      "[144/00905] train_loss: 0.013753\n",
      "[144/00955] train_loss: 0.013322\n",
      "[144/01005] train_loss: 0.013216\n",
      "[144/01055] train_loss: 0.012745\n",
      "[144/01105] train_loss: 0.013449\n",
      "[144/01155] train_loss: 0.013775\n",
      "[144/01205] train_loss: 0.014120\n",
      "[145/00029] train_loss: 0.014278\n",
      "[145/00079] train_loss: 0.014304\n",
      "[145/00129] train_loss: 0.014370\n",
      "[145/00179] train_loss: 0.013682\n",
      "[145/00229] train_loss: 0.013593\n",
      "[145/00279] train_loss: 0.013495\n",
      "[145/00329] train_loss: 0.013941\n",
      "[145/00379] train_loss: 0.014051\n",
      "[145/00429] train_loss: 0.014431\n",
      "[145/00479] train_loss: 0.014158\n",
      "[145/00529] train_loss: 0.013799\n",
      "[145/00579] train_loss: 0.013804\n",
      "[145/00629] train_loss: 0.013145\n",
      "[145/00679] train_loss: 0.013042\n",
      "[145/00729] train_loss: 0.013238\n",
      "[145/00779] train_loss: 0.012754\n",
      "[145/00829] train_loss: 0.013863\n",
      "[145/00879] train_loss: 0.013318\n",
      "[145/00929] train_loss: 0.013774\n",
      "[145/00979] train_loss: 0.013653\n",
      "[145/01029] train_loss: 0.013637\n",
      "[145/01079] train_loss: 0.013897\n",
      "[145/01129] train_loss: 0.012814\n",
      "[145/01179] train_loss: 0.013012\n",
      "[146/00003] train_loss: 0.014073\n",
      "[146/00053] train_loss: 0.015485\n",
      "[146/00103] train_loss: 0.014005\n",
      "[146/00153] train_loss: 0.014000\n",
      "[146/00203] train_loss: 0.014233\n",
      "[146/00253] train_loss: 0.014019\n",
      "[146/00303] train_loss: 0.013642\n",
      "[146/00353] train_loss: 0.013589\n",
      "[146/00403] train_loss: 0.013851\n",
      "[146/00453] train_loss: 0.013637\n",
      "[146/00503] train_loss: 0.013403\n",
      "[146/00553] train_loss: 0.013475\n",
      "[146/00603] train_loss: 0.014636\n",
      "[146/00653] train_loss: 0.013270\n",
      "[146/00703] train_loss: 0.013211\n",
      "[146/00753] train_loss: 0.012824\n",
      "[146/00803] train_loss: 0.013771\n",
      "[146/00853] train_loss: 0.014207\n",
      "[146/00903] train_loss: 0.013252\n",
      "[146/00953] train_loss: 0.013617\n",
      "[146/01003] train_loss: 0.014773\n",
      "[146/01053] train_loss: 0.012865\n",
      "[146/01103] train_loss: 0.013623\n",
      "[146/01153] train_loss: 0.013028\n",
      "[146/01203] train_loss: 0.013167\n",
      "[147/00027] train_loss: 0.014666\n",
      "[147/00077] train_loss: 0.014034\n",
      "[147/00127] train_loss: 0.013855\n",
      "[147/00177] train_loss: 0.013527\n",
      "[147/00227] train_loss: 0.013325\n",
      "[147/00277] train_loss: 0.013906\n",
      "[147/00327] train_loss: 0.014254\n",
      "[147/00377] train_loss: 0.013825\n",
      "[147/00427] train_loss: 0.013214\n",
      "[147/00477] train_loss: 0.013720\n",
      "[147/00527] train_loss: 0.013475\n",
      "[147/00577] train_loss: 0.014233\n",
      "[147/00627] train_loss: 0.013922\n",
      "[147/00677] train_loss: 0.013022\n",
      "[147/00727] train_loss: 0.013584\n",
      "[147/00777] train_loss: 0.013405\n",
      "[147/00827] train_loss: 0.014200\n",
      "[147/00877] train_loss: 0.013984\n",
      "[147/00927] train_loss: 0.013689\n",
      "[147/00977] train_loss: 0.012987\n",
      "[147/01027] train_loss: 0.013958\n",
      "[147/01077] train_loss: 0.013923\n",
      "[147/01127] train_loss: 0.013735\n",
      "[147/01177] train_loss: 0.013380\n",
      "[148/00001] train_loss: 0.013449\n",
      "[148/00051] train_loss: 0.015471\n",
      "[148/00101] train_loss: 0.015030\n",
      "[148/00151] train_loss: 0.014133\n",
      "[148/00201] train_loss: 0.014466\n",
      "[148/00251] train_loss: 0.013977\n",
      "[148/00301] train_loss: 0.014010\n",
      "[148/00351] train_loss: 0.013899\n",
      "[148/00401] train_loss: 0.013157\n",
      "[148/00451] train_loss: 0.013627\n",
      "[148/00501] train_loss: 0.013315\n",
      "[148/00551] train_loss: 0.013423\n",
      "[148/00601] train_loss: 0.012715\n",
      "[148/00651] train_loss: 0.015065\n",
      "[148/00701] train_loss: 0.012524\n",
      "[148/00751] train_loss: 0.013916\n",
      "[148/00801] train_loss: 0.013484\n",
      "[148/00851] train_loss: 0.013078\n",
      "[148/00901] train_loss: 0.013662\n",
      "[148/00951] train_loss: 0.013616\n",
      "[148/01001] train_loss: 0.013967\n",
      "[148/01051] train_loss: 0.014014\n",
      "[148/01101] train_loss: 0.014395\n",
      "[148/01151] train_loss: 0.014200\n",
      "[148/01201] train_loss: 0.013819\n",
      "[149/00025] train_loss: 0.014365\n",
      "[149/00075] train_loss: 0.013672\n",
      "[149/00125] train_loss: 0.014424\n",
      "[149/00175] train_loss: 0.014459\n",
      "[149/00225] train_loss: 0.013545\n",
      "[149/00275] train_loss: 0.014526\n",
      "[149/00325] train_loss: 0.013318\n",
      "[149/00375] train_loss: 0.014121\n",
      "[149/00425] train_loss: 0.012934\n",
      "[149/00475] train_loss: 0.013381\n",
      "[149/00525] train_loss: 0.013498\n",
      "[149/00575] train_loss: 0.013813\n",
      "[149/00625] train_loss: 0.013667\n",
      "[149/00675] train_loss: 0.013526\n",
      "[149/00725] train_loss: 0.013510\n",
      "[149/00775] train_loss: 0.013571\n",
      "[149/00825] train_loss: 0.013445\n",
      "[149/00875] train_loss: 0.014013\n",
      "[149/00925] train_loss: 0.013813\n",
      "[149/00975] train_loss: 0.013276\n",
      "[149/01025] train_loss: 0.013287\n",
      "[149/01075] train_loss: 0.013430\n",
      "[149/01125] train_loss: 0.013498\n",
      "[149/01175] train_loss: 0.013523\n",
      "[149/01225] train_loss: 0.012937\n",
      "[150/00049] train_loss: 0.014849\n",
      "[150/00099] train_loss: 0.014750\n",
      "[150/00149] train_loss: 0.014437\n",
      "[150/00199] train_loss: 0.014305\n",
      "[150/00249] train_loss: 0.014367\n",
      "[150/00299] train_loss: 0.013667\n",
      "[150/00349] train_loss: 0.013680\n",
      "[150/00399] train_loss: 0.014117\n",
      "[150/00449] train_loss: 0.013826\n",
      "[150/00499] train_loss: 0.013828\n",
      "[150/00549] train_loss: 0.013249\n",
      "[150/00599] train_loss: 0.012911\n",
      "[150/00649] train_loss: 0.013725\n",
      "[150/00699] train_loss: 0.013705\n",
      "[150/00749] train_loss: 0.013116\n",
      "[150/00799] train_loss: 0.013461\n",
      "[150/00849] train_loss: 0.013375\n",
      "[150/00899] train_loss: 0.012431\n",
      "[150/00949] train_loss: 0.013713\n",
      "[150/00999] train_loss: 0.012817\n",
      "[150/01049] train_loss: 0.013762\n",
      "[150/01099] train_loss: 0.013703\n",
      "[150/01149] train_loss: 0.014113\n",
      "[150/01199] train_loss: 0.013827\n",
      "[151/00023] train_loss: 0.013852\n",
      "[151/00073] train_loss: 0.014737\n",
      "[151/00123] train_loss: 0.014768\n",
      "[151/00173] train_loss: 0.013861\n",
      "[151/00223] train_loss: 0.013883\n",
      "[151/00273] train_loss: 0.014239\n",
      "[151/00323] train_loss: 0.014059\n",
      "[151/00373] train_loss: 0.014344\n",
      "[151/00423] train_loss: 0.014148\n",
      "[151/00473] train_loss: 0.012932\n",
      "[151/00523] train_loss: 0.013108\n",
      "[151/00573] train_loss: 0.013008\n",
      "[151/00623] train_loss: 0.013076\n",
      "[151/00673] train_loss: 0.013570\n",
      "[151/00723] train_loss: 0.013466\n",
      "[151/00773] train_loss: 0.013585\n",
      "[151/00823] train_loss: 0.013484\n",
      "[151/00873] train_loss: 0.012507\n",
      "[151/00923] train_loss: 0.012531\n",
      "[151/00973] train_loss: 0.013823\n",
      "[151/01023] train_loss: 0.012957\n",
      "[151/01073] train_loss: 0.013955\n",
      "[151/01123] train_loss: 0.013602\n",
      "[151/01173] train_loss: 0.013766\n",
      "[151/01223] train_loss: 0.013248\n",
      "[152/00047] train_loss: 0.014425\n",
      "[152/00097] train_loss: 0.014481\n",
      "[152/00147] train_loss: 0.014851\n",
      "[152/00197] train_loss: 0.013955\n",
      "[152/00247] train_loss: 0.014902\n",
      "[152/00297] train_loss: 0.013131\n",
      "[152/00347] train_loss: 0.014326\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[152/00397] train_loss: 0.013589\n",
      "[152/00447] train_loss: 0.013185\n",
      "[152/00497] train_loss: 0.013787\n",
      "[152/00547] train_loss: 0.013051\n",
      "[152/00597] train_loss: 0.013146\n",
      "[152/00647] train_loss: 0.013946\n",
      "[152/00697] train_loss: 0.013477\n",
      "[152/00747] train_loss: 0.013497\n",
      "[152/00797] train_loss: 0.013134\n",
      "[152/00847] train_loss: 0.013733\n",
      "[152/00897] train_loss: 0.013669\n",
      "[152/00947] train_loss: 0.013284\n",
      "[152/00997] train_loss: 0.012502\n",
      "[152/01047] train_loss: 0.012727\n",
      "[152/01097] train_loss: 0.013017\n",
      "[152/01147] train_loss: 0.013405\n",
      "[152/01197] train_loss: 0.013582\n",
      "[153/00021] train_loss: 0.013825\n",
      "[153/00071] train_loss: 0.013909\n",
      "[153/00121] train_loss: 0.014149\n",
      "[153/00171] train_loss: 0.013603\n",
      "[153/00221] train_loss: 0.013678\n",
      "[153/00271] train_loss: 0.014059\n",
      "[153/00321] train_loss: 0.014109\n",
      "[153/00371] train_loss: 0.014471\n",
      "[153/00421] train_loss: 0.013280\n",
      "[153/00471] train_loss: 0.013449\n",
      "[153/00521] train_loss: 0.013258\n",
      "[153/00571] train_loss: 0.014145\n",
      "[153/00621] train_loss: 0.013834\n",
      "[153/00671] train_loss: 0.012761\n",
      "[153/00721] train_loss: 0.013467\n",
      "[153/00771] train_loss: 0.012818\n",
      "[153/00821] train_loss: 0.013121\n",
      "[153/00871] train_loss: 0.013895\n",
      "[153/00921] train_loss: 0.013327\n",
      "[153/00971] train_loss: 0.013103\n",
      "[153/01021] train_loss: 0.014319\n",
      "[153/01071] train_loss: 0.012939\n",
      "[153/01121] train_loss: 0.013602\n",
      "[153/01171] train_loss: 0.014150\n",
      "[153/01221] train_loss: 0.013323\n",
      "[154/00045] train_loss: 0.015397\n",
      "[154/00095] train_loss: 0.013699\n",
      "[154/00145] train_loss: 0.013671\n",
      "[154/00195] train_loss: 0.013837\n",
      "[154/00245] train_loss: 0.013399\n",
      "[154/00295] train_loss: 0.014161\n",
      "[154/00345] train_loss: 0.013631\n",
      "[154/00395] train_loss: 0.013746\n",
      "[154/00445] train_loss: 0.013589\n",
      "[154/00495] train_loss: 0.013123\n",
      "[154/00545] train_loss: 0.013530\n",
      "[154/00595] train_loss: 0.014512\n",
      "[154/00645] train_loss: 0.013737\n",
      "[154/00695] train_loss: 0.013524\n",
      "[154/00745] train_loss: 0.013307\n",
      "[154/00795] train_loss: 0.013932\n",
      "[154/00845] train_loss: 0.012808\n",
      "[154/00895] train_loss: 0.013758\n",
      "[154/00945] train_loss: 0.013596\n",
      "[154/00995] train_loss: 0.012623\n",
      "[154/01045] train_loss: 0.013249\n",
      "[154/01095] train_loss: 0.012495\n",
      "[154/01145] train_loss: 0.013954\n",
      "[154/01195] train_loss: 0.013672\n",
      "[155/00019] train_loss: 0.013626\n",
      "[155/00069] train_loss: 0.014396\n",
      "[155/00119] train_loss: 0.014171\n",
      "[155/00169] train_loss: 0.012634\n",
      "[155/00219] train_loss: 0.013845\n",
      "[155/00269] train_loss: 0.014364\n",
      "[155/00319] train_loss: 0.013854\n",
      "[155/00369] train_loss: 0.012774\n",
      "[155/00419] train_loss: 0.012828\n",
      "[155/00469] train_loss: 0.013522\n",
      "[155/00519] train_loss: 0.013038\n",
      "[155/00569] train_loss: 0.013508\n",
      "[155/00619] train_loss: 0.013651\n",
      "[155/00669] train_loss: 0.013729\n",
      "[155/00719] train_loss: 0.013641\n",
      "[155/00769] train_loss: 0.012993\n",
      "[155/00819] train_loss: 0.014281\n",
      "[155/00869] train_loss: 0.012707\n",
      "[155/00919] train_loss: 0.013358\n",
      "[155/00969] train_loss: 0.013198\n",
      "[155/01019] train_loss: 0.014053\n",
      "[155/01069] train_loss: 0.013119\n",
      "[155/01119] train_loss: 0.013705\n",
      "[155/01169] train_loss: 0.014184\n",
      "[155/01219] train_loss: 0.013901\n",
      "[156/00043] train_loss: 0.014855\n",
      "[156/00093] train_loss: 0.013369\n",
      "[156/00143] train_loss: 0.013505\n",
      "[156/00193] train_loss: 0.014307\n",
      "[156/00243] train_loss: 0.014322\n",
      "[156/00293] train_loss: 0.013813\n",
      "[156/00343] train_loss: 0.014922\n",
      "[156/00393] train_loss: 0.014202\n",
      "[156/00443] train_loss: 0.014270\n",
      "[156/00493] train_loss: 0.013002\n",
      "[156/00543] train_loss: 0.014069\n",
      "[156/00593] train_loss: 0.012799\n",
      "[156/00643] train_loss: 0.013114\n",
      "[156/00693] train_loss: 0.012943\n",
      "[156/00743] train_loss: 0.013372\n",
      "[156/00793] train_loss: 0.013270\n",
      "[156/00843] train_loss: 0.013582\n",
      "[156/00893] train_loss: 0.012952\n",
      "[156/00943] train_loss: 0.013256\n",
      "[156/00993] train_loss: 0.012574\n",
      "[156/01043] train_loss: 0.013277\n",
      "[156/01093] train_loss: 0.013173\n",
      "[156/01143] train_loss: 0.013476\n",
      "[156/01193] train_loss: 0.013116\n",
      "[157/00017] train_loss: 0.014151\n",
      "[157/00067] train_loss: 0.014404\n",
      "[157/00117] train_loss: 0.014461\n",
      "[157/00167] train_loss: 0.013287\n",
      "[157/00217] train_loss: 0.013622\n",
      "[157/00267] train_loss: 0.013186\n",
      "[157/00317] train_loss: 0.013853\n",
      "[157/00367] train_loss: 0.013525\n",
      "[157/00417] train_loss: 0.013004\n",
      "[157/00467] train_loss: 0.013667\n",
      "[157/00517] train_loss: 0.013819\n",
      "[157/00567] train_loss: 0.013322\n",
      "[157/00617] train_loss: 0.012993\n",
      "[157/00667] train_loss: 0.013803\n",
      "[157/00717] train_loss: 0.012670\n",
      "[157/00767] train_loss: 0.013003\n",
      "[157/00817] train_loss: 0.013584\n",
      "[157/00867] train_loss: 0.012656\n",
      "[157/00917] train_loss: 0.012720\n",
      "[157/00967] train_loss: 0.013964\n",
      "[157/01017] train_loss: 0.013420\n",
      "[157/01067] train_loss: 0.013682\n",
      "[157/01117] train_loss: 0.012843\n",
      "[157/01167] train_loss: 0.013469\n",
      "[157/01217] train_loss: 0.012821\n",
      "[158/00041] train_loss: 0.015096\n",
      "[158/00091] train_loss: 0.014712\n",
      "[158/00141] train_loss: 0.014277\n",
      "[158/00191] train_loss: 0.013966\n",
      "[158/00241] train_loss: 0.013442\n",
      "[158/00291] train_loss: 0.013977\n",
      "[158/00341] train_loss: 0.013192\n",
      "[158/00391] train_loss: 0.012873\n",
      "[158/00441] train_loss: 0.013485\n",
      "[158/00491] train_loss: 0.013399\n",
      "[158/00541] train_loss: 0.013748\n",
      "[158/00591] train_loss: 0.013190\n",
      "[158/00641] train_loss: 0.012892\n",
      "[158/00691] train_loss: 0.013592\n",
      "[158/00741] train_loss: 0.012867\n",
      "[158/00791] train_loss: 0.013258\n",
      "[158/00841] train_loss: 0.013939\n",
      "[158/00891] train_loss: 0.013580\n",
      "[158/00941] train_loss: 0.012510\n",
      "[158/00991] train_loss: 0.013322\n",
      "[158/01041] train_loss: 0.012703\n",
      "[158/01091] train_loss: 0.012906\n",
      "[158/01141] train_loss: 0.013994\n",
      "[158/01191] train_loss: 0.013681\n",
      "[159/00015] train_loss: 0.013884\n",
      "[159/00065] train_loss: 0.014034\n",
      "[159/00115] train_loss: 0.014409\n",
      "[159/00165] train_loss: 0.014043\n",
      "[159/00215] train_loss: 0.012691\n",
      "[159/00265] train_loss: 0.012805\n",
      "[159/00315] train_loss: 0.014145\n",
      "[159/00365] train_loss: 0.014436\n",
      "[159/00415] train_loss: 0.014531\n",
      "[159/00465] train_loss: 0.012726\n",
      "[159/00515] train_loss: 0.013843\n",
      "[159/00565] train_loss: 0.013582\n",
      "[159/00615] train_loss: 0.013326\n",
      "[159/00665] train_loss: 0.013419\n",
      "[159/00715] train_loss: 0.012788\n",
      "[159/00765] train_loss: 0.013227\n",
      "[159/00815] train_loss: 0.012114\n",
      "[159/00865] train_loss: 0.012170\n",
      "[159/00915] train_loss: 0.013183\n",
      "[159/00965] train_loss: 0.013066\n",
      "[159/01015] train_loss: 0.013412\n",
      "[159/01065] train_loss: 0.012619\n",
      "[159/01115] train_loss: 0.013341\n",
      "[159/01165] train_loss: 0.013107\n",
      "[159/01215] train_loss: 0.013442\n",
      "[160/00039] train_loss: 0.013899\n",
      "[160/00089] train_loss: 0.013858\n",
      "[160/00139] train_loss: 0.014206\n",
      "[160/00189] train_loss: 0.013801\n",
      "[160/00239] train_loss: 0.013439\n",
      "[160/00289] train_loss: 0.013876\n",
      "[160/00339] train_loss: 0.014159\n",
      "[160/00389] train_loss: 0.013932\n",
      "[160/00439] train_loss: 0.012954\n",
      "[160/00489] train_loss: 0.013184\n",
      "[160/00539] train_loss: 0.013234\n",
      "[160/00589] train_loss: 0.012907\n",
      "[160/00639] train_loss: 0.013328\n",
      "[160/00689] train_loss: 0.013619\n",
      "[160/00739] train_loss: 0.014283\n",
      "[160/00789] train_loss: 0.013824\n",
      "[160/00839] train_loss: 0.013203\n",
      "[160/00889] train_loss: 0.013850\n",
      "[160/00939] train_loss: 0.013440\n",
      "[160/00989] train_loss: 0.012923\n",
      "[160/01039] train_loss: 0.012435\n",
      "[160/01089] train_loss: 0.013090\n",
      "[160/01139] train_loss: 0.013288\n",
      "[160/01189] train_loss: 0.012707\n",
      "[161/00013] train_loss: 0.014119\n",
      "[161/00063] train_loss: 0.013580\n",
      "[161/00113] train_loss: 0.014133\n",
      "[161/00163] train_loss: 0.013917\n",
      "[161/00213] train_loss: 0.013313\n",
      "[161/00263] train_loss: 0.013429\n",
      "[161/00313] train_loss: 0.012820\n",
      "[161/00363] train_loss: 0.013749\n",
      "[161/00413] train_loss: 0.013560\n",
      "[161/00463] train_loss: 0.013332\n",
      "[161/00513] train_loss: 0.012782\n",
      "[161/00563] train_loss: 0.012826\n",
      "[161/00613] train_loss: 0.012954\n",
      "[161/00663] train_loss: 0.012943\n",
      "[161/00713] train_loss: 0.013756\n",
      "[161/00763] train_loss: 0.013585\n",
      "[161/00813] train_loss: 0.013070\n",
      "[161/00863] train_loss: 0.013562\n",
      "[161/00913] train_loss: 0.013057\n",
      "[161/00963] train_loss: 0.013148\n",
      "[161/01013] train_loss: 0.013264\n",
      "[161/01063] train_loss: 0.013248\n",
      "[161/01113] train_loss: 0.012833\n",
      "[161/01163] train_loss: 0.012992\n",
      "[161/01213] train_loss: 0.012886\n",
      "[162/00037] train_loss: 0.014159\n",
      "[162/00087] train_loss: 0.014233\n",
      "[162/00137] train_loss: 0.014308\n",
      "[162/00187] train_loss: 0.013947\n",
      "[162/00237] train_loss: 0.014236\n",
      "[162/00287] train_loss: 0.014071\n",
      "[162/00337] train_loss: 0.013302\n",
      "[162/00387] train_loss: 0.013554\n",
      "[162/00437] train_loss: 0.013524\n",
      "[162/00487] train_loss: 0.013996\n",
      "[162/00537] train_loss: 0.013293\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[162/00587] train_loss: 0.014021\n",
      "[162/00637] train_loss: 0.013124\n",
      "[162/00687] train_loss: 0.013931\n",
      "[162/00737] train_loss: 0.013180\n",
      "[162/00787] train_loss: 0.013022\n",
      "[162/00837] train_loss: 0.013131\n",
      "[162/00887] train_loss: 0.013532\n",
      "[162/00937] train_loss: 0.013133\n",
      "[162/00987] train_loss: 0.013463\n",
      "[162/01037] train_loss: 0.012112\n",
      "[162/01087] train_loss: 0.013063\n",
      "[162/01137] train_loss: 0.012482\n",
      "[162/01187] train_loss: 0.013168\n",
      "[163/00011] train_loss: 0.013580\n",
      "[163/00061] train_loss: 0.014576\n",
      "[163/00111] train_loss: 0.013821\n",
      "[163/00161] train_loss: 0.013864\n",
      "[163/00211] train_loss: 0.013846\n",
      "[163/00261] train_loss: 0.013076\n",
      "[163/00311] train_loss: 0.013892\n",
      "[163/00361] train_loss: 0.013095\n",
      "[163/00411] train_loss: 0.013051\n",
      "[163/00461] train_loss: 0.013136\n",
      "[163/00511] train_loss: 0.013813\n",
      "[163/00561] train_loss: 0.013004\n",
      "[163/00611] train_loss: 0.012924\n",
      "[163/00661] train_loss: 0.013696\n",
      "[163/00711] train_loss: 0.013639\n",
      "[163/00761] train_loss: 0.012916\n",
      "[163/00811] train_loss: 0.013817\n",
      "[163/00861] train_loss: 0.013337\n",
      "[163/00911] train_loss: 0.013135\n",
      "[163/00961] train_loss: 0.013649\n",
      "[163/01011] train_loss: 0.013095\n",
      "[163/01061] train_loss: 0.012176\n",
      "[163/01111] train_loss: 0.012538\n",
      "[163/01161] train_loss: 0.012675\n",
      "[163/01211] train_loss: 0.012755\n",
      "[164/00035] train_loss: 0.013980\n",
      "[164/00085] train_loss: 0.014875\n",
      "[164/00135] train_loss: 0.013821\n",
      "[164/00185] train_loss: 0.013360\n",
      "[164/00235] train_loss: 0.013372\n",
      "[164/00285] train_loss: 0.013378\n",
      "[164/00335] train_loss: 0.014725\n",
      "[164/00385] train_loss: 0.014039\n",
      "[164/00435] train_loss: 0.013259\n",
      "[164/00485] train_loss: 0.012999\n",
      "[164/00535] train_loss: 0.012882\n",
      "[164/00585] train_loss: 0.013160\n",
      "[164/00635] train_loss: 0.013667\n",
      "[164/00685] train_loss: 0.013261\n",
      "[164/00735] train_loss: 0.013362\n",
      "[164/00785] train_loss: 0.013142\n",
      "[164/00835] train_loss: 0.012559\n",
      "[164/00885] train_loss: 0.013071\n",
      "[164/00935] train_loss: 0.012927\n",
      "[164/00985] train_loss: 0.013356\n",
      "[164/01035] train_loss: 0.012840\n",
      "[164/01085] train_loss: 0.014386\n",
      "[164/01135] train_loss: 0.012973\n",
      "[164/01185] train_loss: 0.013021\n",
      "[165/00009] train_loss: 0.013507\n",
      "[165/00059] train_loss: 0.014280\n",
      "[165/00109] train_loss: 0.014529\n",
      "[165/00159] train_loss: 0.013620\n",
      "[165/00209] train_loss: 0.012881\n",
      "[165/00259] train_loss: 0.012510\n",
      "[165/00309] train_loss: 0.013419\n",
      "[165/00359] train_loss: 0.013122\n",
      "[165/00409] train_loss: 0.013664\n",
      "[165/00459] train_loss: 0.014390\n",
      "[165/00509] train_loss: 0.014224\n",
      "[165/00559] train_loss: 0.013023\n",
      "[165/00609] train_loss: 0.012380\n",
      "[165/00659] train_loss: 0.013440\n",
      "[165/00709] train_loss: 0.013620\n",
      "[165/00759] train_loss: 0.013325\n",
      "[165/00809] train_loss: 0.013044\n",
      "[165/00859] train_loss: 0.013409\n",
      "[165/00909] train_loss: 0.013229\n",
      "[165/00959] train_loss: 0.013132\n",
      "[165/01009] train_loss: 0.013167\n",
      "[165/01059] train_loss: 0.013458\n",
      "[165/01109] train_loss: 0.013614\n",
      "[165/01159] train_loss: 0.013176\n",
      "[165/01209] train_loss: 0.012850\n",
      "[166/00033] train_loss: 0.015215\n",
      "[166/00083] train_loss: 0.014179\n",
      "[166/00133] train_loss: 0.014593\n",
      "[166/00183] train_loss: 0.013742\n",
      "[166/00233] train_loss: 0.014555\n",
      "[166/00283] train_loss: 0.013376\n",
      "[166/00333] train_loss: 0.013825\n",
      "[166/00383] train_loss: 0.013420\n",
      "[166/00433] train_loss: 0.013533\n",
      "[166/00483] train_loss: 0.013818\n",
      "[166/00533] train_loss: 0.013107\n",
      "[166/00583] train_loss: 0.013116\n",
      "[166/00633] train_loss: 0.013824\n",
      "[166/00683] train_loss: 0.013364\n",
      "[166/00733] train_loss: 0.013103\n",
      "[166/00783] train_loss: 0.012759\n",
      "[166/00833] train_loss: 0.012499\n",
      "[166/00883] train_loss: 0.012744\n",
      "[166/00933] train_loss: 0.013284\n",
      "[166/00983] train_loss: 0.013138\n",
      "[166/01033] train_loss: 0.013384\n",
      "[166/01083] train_loss: 0.013151\n",
      "[166/01133] train_loss: 0.012449\n",
      "[166/01183] train_loss: 0.013064\n",
      "[167/00007] train_loss: 0.013126\n",
      "[167/00057] train_loss: 0.015036\n",
      "[167/00107] train_loss: 0.014419\n",
      "[167/00157] train_loss: 0.013738\n",
      "[167/00207] train_loss: 0.013804\n",
      "[167/00257] train_loss: 0.013561\n",
      "[167/00307] train_loss: 0.012893\n",
      "[167/00357] train_loss: 0.013473\n",
      "[167/00407] train_loss: 0.013158\n",
      "[167/00457] train_loss: 0.014029\n",
      "[167/00507] train_loss: 0.013483\n",
      "[167/00557] train_loss: 0.012928\n",
      "[167/00607] train_loss: 0.012990\n",
      "[167/00657] train_loss: 0.012377\n",
      "[167/00707] train_loss: 0.013386\n",
      "[167/00757] train_loss: 0.013036\n",
      "[167/00807] train_loss: 0.012425\n",
      "[167/00857] train_loss: 0.013426\n",
      "[167/00907] train_loss: 0.012382\n",
      "[167/00957] train_loss: 0.012691\n",
      "[167/01007] train_loss: 0.012961\n",
      "[167/01057] train_loss: 0.012769\n",
      "[167/01107] train_loss: 0.013408\n",
      "[167/01157] train_loss: 0.013464\n",
      "[167/01207] train_loss: 0.013217\n",
      "[168/00031] train_loss: 0.014199\n",
      "[168/00081] train_loss: 0.013732\n",
      "[168/00131] train_loss: 0.014311\n",
      "[168/00181] train_loss: 0.013845\n",
      "[168/00231] train_loss: 0.013205\n",
      "[168/00281] train_loss: 0.013000\n",
      "[168/00331] train_loss: 0.013549\n",
      "[168/00381] train_loss: 0.014027\n",
      "[168/00431] train_loss: 0.013738\n",
      "[168/00481] train_loss: 0.013351\n",
      "[168/00531] train_loss: 0.013523\n",
      "[168/00581] train_loss: 0.013699\n",
      "[168/00631] train_loss: 0.013390\n",
      "[168/00681] train_loss: 0.012838\n",
      "[168/00731] train_loss: 0.013233\n",
      "[168/00781] train_loss: 0.013270\n",
      "[168/00831] train_loss: 0.012082\n",
      "[168/00881] train_loss: 0.012929\n",
      "[168/00931] train_loss: 0.013028\n",
      "[168/00981] train_loss: 0.013159\n",
      "[168/01031] train_loss: 0.012690\n",
      "[168/01081] train_loss: 0.012731\n",
      "[168/01131] train_loss: 0.012853\n",
      "[168/01181] train_loss: 0.012675\n",
      "[169/00005] train_loss: 0.013669\n",
      "[169/00055] train_loss: 0.014112\n",
      "[169/00105] train_loss: 0.013788\n",
      "[169/00155] train_loss: 0.014051\n",
      "[169/00205] train_loss: 0.013225\n",
      "[169/00255] train_loss: 0.013962\n",
      "[169/00305] train_loss: 0.013501\n",
      "[169/00355] train_loss: 0.013449\n",
      "[169/00405] train_loss: 0.013763\n",
      "[169/00455] train_loss: 0.013758\n",
      "[169/00505] train_loss: 0.014044\n",
      "[169/00555] train_loss: 0.012478\n",
      "[169/00605] train_loss: 0.012764\n",
      "[169/00655] train_loss: 0.012760\n",
      "[169/00705] train_loss: 0.013622\n",
      "[169/00755] train_loss: 0.012583\n",
      "[169/00805] train_loss: 0.012799\n",
      "[169/00855] train_loss: 0.012795\n",
      "[169/00905] train_loss: 0.013315\n",
      "[169/00955] train_loss: 0.013032\n",
      "[169/01005] train_loss: 0.012660\n",
      "[169/01055] train_loss: 0.012725\n",
      "[169/01105] train_loss: 0.012676\n",
      "[169/01155] train_loss: 0.012682\n",
      "[169/01205] train_loss: 0.013528\n",
      "[170/00029] train_loss: 0.014065\n",
      "[170/00079] train_loss: 0.014389\n",
      "[170/00129] train_loss: 0.013469\n",
      "[170/00179] train_loss: 0.013471\n",
      "[170/00229] train_loss: 0.013796\n",
      "[170/00279] train_loss: 0.013566\n",
      "[170/00329] train_loss: 0.012910\n",
      "[170/00379] train_loss: 0.013223\n",
      "[170/00429] train_loss: 0.013117\n",
      "[170/00479] train_loss: 0.013356\n",
      "[170/00529] train_loss: 0.014112\n",
      "[170/00579] train_loss: 0.012952\n",
      "[170/00629] train_loss: 0.013168\n",
      "[170/00679] train_loss: 0.013419\n",
      "[170/00729] train_loss: 0.012504\n",
      "[170/00779] train_loss: 0.012855\n",
      "[170/00829] train_loss: 0.013638\n",
      "[170/00879] train_loss: 0.012968\n",
      "[170/00929] train_loss: 0.012673\n",
      "[170/00979] train_loss: 0.013016\n",
      "[170/01029] train_loss: 0.013159\n",
      "[170/01079] train_loss: 0.013544\n",
      "[170/01129] train_loss: 0.012233\n",
      "[170/01179] train_loss: 0.012559\n",
      "[171/00003] train_loss: 0.012708\n",
      "[171/00053] train_loss: 0.013487\n",
      "[171/00103] train_loss: 0.013660\n",
      "[171/00153] train_loss: 0.013966\n",
      "[171/00203] train_loss: 0.013547\n",
      "[171/00253] train_loss: 0.013925\n",
      "[171/00303] train_loss: 0.013247\n",
      "[171/00353] train_loss: 0.012942\n",
      "[171/00403] train_loss: 0.013903\n",
      "[171/00453] train_loss: 0.013122\n",
      "[171/00503] train_loss: 0.013065\n",
      "[171/00553] train_loss: 0.012607\n",
      "[171/00603] train_loss: 0.013559\n",
      "[171/00653] train_loss: 0.013121\n",
      "[171/00703] train_loss: 0.012704\n",
      "[171/00753] train_loss: 0.012825\n",
      "[171/00803] train_loss: 0.013106\n",
      "[171/00853] train_loss: 0.013296\n",
      "[171/00903] train_loss: 0.013178\n",
      "[171/00953] train_loss: 0.012485\n",
      "[171/01003] train_loss: 0.012628\n",
      "[171/01053] train_loss: 0.013671\n",
      "[171/01103] train_loss: 0.012384\n",
      "[171/01153] train_loss: 0.013320\n",
      "[171/01203] train_loss: 0.012691\n",
      "[172/00027] train_loss: 0.014044\n",
      "[172/00077] train_loss: 0.014080\n",
      "[172/00127] train_loss: 0.013603\n",
      "[172/00177] train_loss: 0.012722\n",
      "[172/00227] train_loss: 0.013333\n",
      "[172/00277] train_loss: 0.013253\n",
      "[172/00327] train_loss: 0.013817\n",
      "[172/00377] train_loss: 0.012995\n",
      "[172/00427] train_loss: 0.013164\n",
      "[172/00477] train_loss: 0.013129\n",
      "[172/00527] train_loss: 0.013126\n",
      "[172/00577] train_loss: 0.013440\n",
      "[172/00627] train_loss: 0.013027\n",
      "[172/00677] train_loss: 0.013641\n",
      "[172/00727] train_loss: 0.012659\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[172/00777] train_loss: 0.013404\n",
      "[172/00827] train_loss: 0.013055\n",
      "[172/00877] train_loss: 0.013648\n",
      "[172/00927] train_loss: 0.012361\n",
      "[172/00977] train_loss: 0.012832\n",
      "[172/01027] train_loss: 0.012911\n",
      "[172/01077] train_loss: 0.012857\n",
      "[172/01127] train_loss: 0.013293\n",
      "[172/01177] train_loss: 0.012509\n",
      "[173/00001] train_loss: 0.012580\n",
      "[173/00051] train_loss: 0.014347\n",
      "[173/00101] train_loss: 0.013290\n",
      "[173/00151] train_loss: 0.013927\n",
      "[173/00201] train_loss: 0.013741\n",
      "[173/00251] train_loss: 0.012412\n",
      "[173/00301] train_loss: 0.012511\n",
      "[173/00351] train_loss: 0.013399\n",
      "[173/00401] train_loss: 0.014186\n",
      "[173/00451] train_loss: 0.013660\n",
      "[173/00501] train_loss: 0.013076\n",
      "[173/00551] train_loss: 0.013138\n",
      "[173/00601] train_loss: 0.012475\n",
      "[173/00651] train_loss: 0.013208\n",
      "[173/00701] train_loss: 0.013111\n",
      "[173/00751] train_loss: 0.013045\n",
      "[173/00801] train_loss: 0.013584\n",
      "[173/00851] train_loss: 0.013076\n",
      "[173/00901] train_loss: 0.012987\n",
      "[173/00951] train_loss: 0.013084\n",
      "[173/01001] train_loss: 0.012918\n",
      "[173/01051] train_loss: 0.013578\n",
      "[173/01101] train_loss: 0.013272\n",
      "[173/01151] train_loss: 0.013445\n",
      "[173/01201] train_loss: 0.012690\n",
      "[174/00025] train_loss: 0.013378\n",
      "[174/00075] train_loss: 0.013421\n",
      "[174/00125] train_loss: 0.013882\n",
      "[174/00175] train_loss: 0.013774\n",
      "[174/00225] train_loss: 0.013394\n",
      "[174/00275] train_loss: 0.013785\n",
      "[174/00325] train_loss: 0.014072\n",
      "[174/00375] train_loss: 0.012945\n",
      "[174/00425] train_loss: 0.013453\n",
      "[174/00475] train_loss: 0.012942\n",
      "[174/00525] train_loss: 0.012834\n",
      "[174/00575] train_loss: 0.013802\n",
      "[174/00625] train_loss: 0.013183\n",
      "[174/00675] train_loss: 0.012623\n",
      "[174/00725] train_loss: 0.012813\n",
      "[174/00775] train_loss: 0.012738\n",
      "[174/00825] train_loss: 0.013547\n",
      "[174/00875] train_loss: 0.012486\n",
      "[174/00925] train_loss: 0.012561\n",
      "[174/00975] train_loss: 0.013033\n",
      "[174/01025] train_loss: 0.013263\n",
      "[174/01075] train_loss: 0.013560\n",
      "[174/01125] train_loss: 0.012775\n",
      "[174/01175] train_loss: 0.012549\n",
      "[174/01225] train_loss: 0.012998\n",
      "[175/00049] train_loss: 0.015070\n",
      "[175/00099] train_loss: 0.013476\n",
      "[175/00149] train_loss: 0.013414\n",
      "[175/00199] train_loss: 0.013088\n",
      "[175/00249] train_loss: 0.013221\n",
      "[175/00299] train_loss: 0.013189\n",
      "[175/00349] train_loss: 0.014022\n",
      "[175/00399] train_loss: 0.012826\n",
      "[175/00449] train_loss: 0.012916\n",
      "[175/00499] train_loss: 0.013117\n",
      "[175/00549] train_loss: 0.013435\n",
      "[175/00599] train_loss: 0.013307\n",
      "[175/00649] train_loss: 0.013553\n",
      "[175/00699] train_loss: 0.012681\n",
      "[175/00749] train_loss: 0.012517\n",
      "[175/00799] train_loss: 0.012810\n",
      "[175/00849] train_loss: 0.013750\n",
      "[175/00899] train_loss: 0.013979\n",
      "[175/00949] train_loss: 0.013193\n",
      "[175/00999] train_loss: 0.011978\n",
      "[175/01049] train_loss: 0.013923\n",
      "[175/01099] train_loss: 0.013252\n",
      "[175/01149] train_loss: 0.013114\n",
      "[175/01199] train_loss: 0.012689\n",
      "[176/00023] train_loss: 0.014027\n",
      "[176/00073] train_loss: 0.014322\n",
      "[176/00123] train_loss: 0.013602\n",
      "[176/00173] train_loss: 0.013720\n",
      "[176/00223] train_loss: 0.014054\n",
      "[176/00273] train_loss: 0.013520\n",
      "[176/00323] train_loss: 0.013392\n",
      "[176/00373] train_loss: 0.013429\n",
      "[176/00423] train_loss: 0.012747\n",
      "[176/00473] train_loss: 0.013461\n",
      "[176/00523] train_loss: 0.012901\n",
      "[176/00573] train_loss: 0.013136\n",
      "[176/00623] train_loss: 0.012863\n",
      "[176/00673] train_loss: 0.012826\n",
      "[176/00723] train_loss: 0.013154\n",
      "[176/00773] train_loss: 0.012833\n",
      "[176/00823] train_loss: 0.012986\n",
      "[176/00873] train_loss: 0.012806\n",
      "[176/00923] train_loss: 0.012952\n",
      "[176/00973] train_loss: 0.012674\n",
      "[176/01023] train_loss: 0.013268\n",
      "[176/01073] train_loss: 0.012973\n",
      "[176/01123] train_loss: 0.013496\n",
      "[176/01173] train_loss: 0.013437\n",
      "[176/01223] train_loss: 0.013691\n",
      "[177/00047] train_loss: 0.013815\n",
      "[177/00097] train_loss: 0.012909\n",
      "[177/00147] train_loss: 0.013845\n",
      "[177/00197] train_loss: 0.013327\n",
      "[177/00247] train_loss: 0.013685\n",
      "[177/00297] train_loss: 0.013330\n",
      "[177/00347] train_loss: 0.013945\n",
      "[177/00397] train_loss: 0.013642\n",
      "[177/00447] train_loss: 0.012823\n",
      "[177/00497] train_loss: 0.012730\n",
      "[177/00547] train_loss: 0.013150\n",
      "[177/00597] train_loss: 0.012708\n",
      "[177/00647] train_loss: 0.013158\n",
      "[177/00697] train_loss: 0.012781\n",
      "[177/00747] train_loss: 0.012203\n",
      "[177/00797] train_loss: 0.012323\n",
      "[177/00847] train_loss: 0.012911\n",
      "[177/00897] train_loss: 0.013631\n",
      "[177/00947] train_loss: 0.013074\n",
      "[177/00997] train_loss: 0.013225\n",
      "[177/01047] train_loss: 0.014059\n",
      "[177/01097] train_loss: 0.012086\n",
      "[177/01147] train_loss: 0.012542\n",
      "[177/01197] train_loss: 0.013258\n",
      "[178/00021] train_loss: 0.013307\n",
      "[178/00071] train_loss: 0.014539\n",
      "[178/00121] train_loss: 0.014104\n",
      "[178/00171] train_loss: 0.013462\n",
      "[178/00221] train_loss: 0.014319\n",
      "[178/00271] train_loss: 0.012958\n",
      "[178/00321] train_loss: 0.012944\n",
      "[178/00371] train_loss: 0.013324\n",
      "[178/00421] train_loss: 0.013172\n",
      "[178/00471] train_loss: 0.013120\n",
      "[178/00521] train_loss: 0.013022\n",
      "[178/00571] train_loss: 0.013376\n",
      "[178/00621] train_loss: 0.013647\n",
      "[178/00671] train_loss: 0.012791\n",
      "[178/00721] train_loss: 0.012256\n",
      "[178/00771] train_loss: 0.012579\n",
      "[178/00821] train_loss: 0.013595\n",
      "[178/00871] train_loss: 0.014179\n",
      "[178/00921] train_loss: 0.012976\n",
      "[178/00971] train_loss: 0.013156\n",
      "[178/01021] train_loss: 0.012520\n",
      "[178/01071] train_loss: 0.013346\n",
      "[178/01121] train_loss: 0.012815\n",
      "[178/01171] train_loss: 0.013583\n",
      "[178/01221] train_loss: 0.012686\n",
      "[179/00045] train_loss: 0.014160\n",
      "[179/00095] train_loss: 0.013990\n",
      "[179/00145] train_loss: 0.013453\n",
      "[179/00195] train_loss: 0.013660\n",
      "[179/00245] train_loss: 0.013803\n",
      "[179/00295] train_loss: 0.013819\n",
      "[179/00345] train_loss: 0.013538\n",
      "[179/00395] train_loss: 0.013534\n",
      "[179/00445] train_loss: 0.013008\n",
      "[179/00495] train_loss: 0.012869\n",
      "[179/00545] train_loss: 0.013044\n",
      "[179/00595] train_loss: 0.012621\n",
      "[179/00645] train_loss: 0.013106\n",
      "[179/00695] train_loss: 0.012787\n",
      "[179/00745] train_loss: 0.012840\n",
      "[179/00795] train_loss: 0.013151\n",
      "[179/00845] train_loss: 0.012203\n",
      "[179/00895] train_loss: 0.013208\n",
      "[179/00945] train_loss: 0.012232\n",
      "[179/00995] train_loss: 0.012069\n",
      "[179/01045] train_loss: 0.012752\n",
      "[179/01095] train_loss: 0.012943\n",
      "[179/01145] train_loss: 0.012630\n",
      "[179/01195] train_loss: 0.012878\n",
      "[180/00019] train_loss: 0.013670\n",
      "[180/00069] train_loss: 0.013651\n",
      "[180/00119] train_loss: 0.013838\n",
      "[180/00169] train_loss: 0.013431\n",
      "[180/00219] train_loss: 0.013342\n",
      "[180/00269] train_loss: 0.013156\n",
      "[180/00319] train_loss: 0.012510\n",
      "[180/00369] train_loss: 0.013572\n",
      "[180/00419] train_loss: 0.013031\n",
      "[180/00469] train_loss: 0.012884\n",
      "[180/00519] train_loss: 0.012845\n",
      "[180/00569] train_loss: 0.013757\n",
      "[180/00619] train_loss: 0.013305\n",
      "[180/00669] train_loss: 0.012992\n",
      "[180/00719] train_loss: 0.013348\n",
      "[180/00769] train_loss: 0.013386\n",
      "[180/00819] train_loss: 0.012747\n",
      "[180/00869] train_loss: 0.013447\n",
      "[180/00919] train_loss: 0.012571\n",
      "[180/00969] train_loss: 0.012791\n",
      "[180/01019] train_loss: 0.012863\n",
      "[180/01069] train_loss: 0.013853\n",
      "[180/01119] train_loss: 0.013612\n",
      "[180/01169] train_loss: 0.012502\n",
      "[180/01219] train_loss: 0.012367\n",
      "[181/00043] train_loss: 0.013667\n",
      "[181/00093] train_loss: 0.014595\n",
      "[181/00143] train_loss: 0.013244\n",
      "[181/00193] train_loss: 0.013877\n",
      "[181/00243] train_loss: 0.014050\n",
      "[181/00293] train_loss: 0.012839\n",
      "[181/00343] train_loss: 0.012892\n",
      "[181/00393] train_loss: 0.012517\n",
      "[181/00443] train_loss: 0.012680\n",
      "[181/00493] train_loss: 0.012321\n",
      "[181/00543] train_loss: 0.012618\n",
      "[181/00593] train_loss: 0.014231\n",
      "[181/00643] train_loss: 0.013189\n",
      "[181/00693] train_loss: 0.012458\n",
      "[181/00743] train_loss: 0.012571\n",
      "[181/00793] train_loss: 0.012897\n",
      "[181/00843] train_loss: 0.012488\n",
      "[181/00893] train_loss: 0.012719\n",
      "[181/00943] train_loss: 0.012729\n",
      "[181/00993] train_loss: 0.013167\n",
      "[181/01043] train_loss: 0.012643\n",
      "[181/01093] train_loss: 0.012486\n",
      "[181/01143] train_loss: 0.012973\n",
      "[181/01193] train_loss: 0.012215\n",
      "[182/00017] train_loss: 0.013972\n",
      "[182/00067] train_loss: 0.013916\n",
      "[182/00117] train_loss: 0.012861\n",
      "[182/00167] train_loss: 0.013479\n",
      "[182/00217] train_loss: 0.013190\n",
      "[182/00267] train_loss: 0.012942\n",
      "[182/00317] train_loss: 0.013190\n",
      "[182/00367] train_loss: 0.012493\n",
      "[182/00417] train_loss: 0.013504\n",
      "[182/00467] train_loss: 0.012239\n",
      "[182/00517] train_loss: 0.013299\n",
      "[182/00567] train_loss: 0.012510\n",
      "[182/00617] train_loss: 0.012283\n",
      "[182/00667] train_loss: 0.012339\n",
      "[182/00717] train_loss: 0.013108\n",
      "[182/00767] train_loss: 0.013582\n",
      "[182/00817] train_loss: 0.013261\n",
      "[182/00867] train_loss: 0.013516\n",
      "[182/00917] train_loss: 0.012701\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[182/00967] train_loss: 0.012476\n",
      "[182/01017] train_loss: 0.012970\n",
      "[182/01067] train_loss: 0.013191\n",
      "[182/01117] train_loss: 0.013672\n",
      "[182/01167] train_loss: 0.013290\n",
      "[182/01217] train_loss: 0.013268\n",
      "[183/00041] train_loss: 0.014446\n",
      "[183/00091] train_loss: 0.013429\n",
      "[183/00141] train_loss: 0.013940\n",
      "[183/00191] train_loss: 0.013359\n",
      "[183/00241] train_loss: 0.013537\n",
      "[183/00291] train_loss: 0.013418\n",
      "[183/00341] train_loss: 0.012665\n",
      "[183/00391] train_loss: 0.013064\n",
      "[183/00441] train_loss: 0.013490\n",
      "[183/00491] train_loss: 0.013629\n",
      "[183/00541] train_loss: 0.012634\n",
      "[183/00591] train_loss: 0.013940\n",
      "[183/00641] train_loss: 0.013104\n",
      "[183/00691] train_loss: 0.012563\n",
      "[183/00741] train_loss: 0.013161\n",
      "[183/00791] train_loss: 0.012566\n",
      "[183/00841] train_loss: 0.012685\n",
      "[183/00891] train_loss: 0.013237\n",
      "[183/00941] train_loss: 0.012875\n",
      "[183/00991] train_loss: 0.012843\n",
      "[183/01041] train_loss: 0.012646\n",
      "[183/01091] train_loss: 0.012490\n",
      "[183/01141] train_loss: 0.011538\n",
      "[183/01191] train_loss: 0.013002\n",
      "[184/00015] train_loss: 0.014771\n",
      "[184/00065] train_loss: 0.013018\n",
      "[184/00115] train_loss: 0.013886\n",
      "[184/00165] train_loss: 0.013876\n",
      "[184/00215] train_loss: 0.013354\n",
      "[184/00265] train_loss: 0.013389\n",
      "[184/00315] train_loss: 0.013089\n",
      "[184/00365] train_loss: 0.013217\n",
      "[184/00415] train_loss: 0.013202\n",
      "[184/00465] train_loss: 0.013851\n",
      "[184/00515] train_loss: 0.013926\n",
      "[184/00565] train_loss: 0.012710\n",
      "[184/00615] train_loss: 0.012074\n",
      "[184/00665] train_loss: 0.012719\n",
      "[184/00715] train_loss: 0.012644\n",
      "[184/00765] train_loss: 0.013275\n",
      "[184/00815] train_loss: 0.013147\n",
      "[184/00865] train_loss: 0.012333\n",
      "[184/00915] train_loss: 0.012880\n",
      "[184/00965] train_loss: 0.013038\n",
      "[184/01015] train_loss: 0.013232\n",
      "[184/01065] train_loss: 0.012801\n",
      "[184/01115] train_loss: 0.013150\n",
      "[184/01165] train_loss: 0.012463\n",
      "[184/01215] train_loss: 0.013142\n",
      "[185/00039] train_loss: 0.013716\n",
      "[185/00089] train_loss: 0.015335\n",
      "[185/00139] train_loss: 0.013089\n",
      "[185/00189] train_loss: 0.013666\n",
      "[185/00239] train_loss: 0.013172\n",
      "[185/00289] train_loss: 0.012630\n",
      "[185/00339] train_loss: 0.013448\n",
      "[185/00389] train_loss: 0.013863\n",
      "[185/00439] train_loss: 0.012445\n",
      "[185/00489] train_loss: 0.012936\n",
      "[185/00539] train_loss: 0.012532\n",
      "[185/00589] train_loss: 0.012265\n",
      "[185/00639] train_loss: 0.012356\n",
      "[185/00689] train_loss: 0.013602\n",
      "[185/00739] train_loss: 0.013043\n",
      "[185/00789] train_loss: 0.012955\n",
      "[185/00839] train_loss: 0.012941\n",
      "[185/00889] train_loss: 0.012508\n",
      "[185/00939] train_loss: 0.013336\n",
      "[185/00989] train_loss: 0.013234\n",
      "[185/01039] train_loss: 0.012837\n",
      "[185/01089] train_loss: 0.012784\n",
      "[185/01139] train_loss: 0.012083\n",
      "[185/01189] train_loss: 0.012476\n",
      "[186/00013] train_loss: 0.013665\n",
      "[186/00063] train_loss: 0.014039\n",
      "[186/00113] train_loss: 0.012636\n",
      "[186/00163] train_loss: 0.014784\n",
      "[186/00213] train_loss: 0.012803\n",
      "[186/00263] train_loss: 0.013479\n",
      "[186/00313] train_loss: 0.013017\n",
      "[186/00363] train_loss: 0.013026\n",
      "[186/00413] train_loss: 0.013269\n",
      "[186/00463] train_loss: 0.013011\n",
      "[186/00513] train_loss: 0.012220\n",
      "[186/00563] train_loss: 0.013419\n",
      "[186/00613] train_loss: 0.013134\n",
      "[186/00663] train_loss: 0.012993\n",
      "[186/00713] train_loss: 0.012794\n",
      "[186/00763] train_loss: 0.013341\n",
      "[186/00813] train_loss: 0.012809\n",
      "[186/00863] train_loss: 0.012836\n",
      "[186/00913] train_loss: 0.012206\n",
      "[186/00963] train_loss: 0.012536\n",
      "[186/01013] train_loss: 0.012670\n",
      "[186/01063] train_loss: 0.012169\n",
      "[186/01113] train_loss: 0.013517\n",
      "[186/01163] train_loss: 0.012528\n",
      "[186/01213] train_loss: 0.012092\n",
      "[187/00037] train_loss: 0.014446\n",
      "[187/00087] train_loss: 0.014212\n",
      "[187/00137] train_loss: 0.013684\n",
      "[187/00187] train_loss: 0.013623\n",
      "[187/00237] train_loss: 0.013592\n",
      "[187/00287] train_loss: 0.012444\n",
      "[187/00337] train_loss: 0.013053\n",
      "[187/00387] train_loss: 0.013168\n",
      "[187/00437] train_loss: 0.013408\n",
      "[187/00487] train_loss: 0.012912\n",
      "[187/00537] train_loss: 0.013283\n",
      "[187/00587] train_loss: 0.013081\n",
      "[187/00637] train_loss: 0.013114\n",
      "[187/00687] train_loss: 0.012832\n",
      "[187/00737] train_loss: 0.012717\n",
      "[187/00787] train_loss: 0.012559\n",
      "[187/00837] train_loss: 0.013590\n",
      "[187/00887] train_loss: 0.013295\n",
      "[187/00937] train_loss: 0.012789\n",
      "[187/00987] train_loss: 0.012912\n",
      "[187/01037] train_loss: 0.012266\n",
      "[187/01087] train_loss: 0.011788\n",
      "[187/01137] train_loss: 0.013682\n",
      "[187/01187] train_loss: 0.013034\n",
      "[188/00011] train_loss: 0.012901\n",
      "[188/00061] train_loss: 0.014044\n",
      "[188/00111] train_loss: 0.013884\n",
      "[188/00161] train_loss: 0.013525\n",
      "[188/00211] train_loss: 0.013292\n",
      "[188/00261] train_loss: 0.013369\n",
      "[188/00311] train_loss: 0.012801\n",
      "[188/00361] train_loss: 0.013050\n",
      "[188/00411] train_loss: 0.013294\n",
      "[188/00461] train_loss: 0.013853\n",
      "[188/00511] train_loss: 0.012362\n",
      "[188/00561] train_loss: 0.012963\n",
      "[188/00611] train_loss: 0.012918\n",
      "[188/00661] train_loss: 0.012570\n",
      "[188/00711] train_loss: 0.012674\n",
      "[188/00761] train_loss: 0.013144\n",
      "[188/00811] train_loss: 0.012447\n",
      "[188/00861] train_loss: 0.012797\n",
      "[188/00911] train_loss: 0.012185\n",
      "[188/00961] train_loss: 0.013211\n",
      "[188/01011] train_loss: 0.012892\n",
      "[188/01061] train_loss: 0.013408\n",
      "[188/01111] train_loss: 0.012177\n",
      "[188/01161] train_loss: 0.012566\n",
      "[188/01211] train_loss: 0.013591\n",
      "[189/00035] train_loss: 0.014039\n",
      "[189/00085] train_loss: 0.013765\n",
      "[189/00135] train_loss: 0.013303\n",
      "[189/00185] train_loss: 0.013570\n",
      "[189/00235] train_loss: 0.013186\n",
      "[189/00285] train_loss: 0.012360\n",
      "[189/00335] train_loss: 0.013473\n",
      "[189/00385] train_loss: 0.013175\n",
      "[189/00435] train_loss: 0.012250\n",
      "[189/00485] train_loss: 0.013430\n",
      "[189/00535] train_loss: 0.012933\n",
      "[189/00585] train_loss: 0.013041\n",
      "[189/00635] train_loss: 0.012841\n",
      "[189/00685] train_loss: 0.012588\n",
      "[189/00735] train_loss: 0.012954\n",
      "[189/00785] train_loss: 0.012700\n",
      "[189/00835] train_loss: 0.012969\n",
      "[189/00885] train_loss: 0.012415\n",
      "[189/00935] train_loss: 0.012868\n",
      "[189/00985] train_loss: 0.012509\n",
      "[189/01035] train_loss: 0.013805\n",
      "[189/01085] train_loss: 0.013047\n",
      "[189/01135] train_loss: 0.012744\n",
      "[189/01185] train_loss: 0.012678\n",
      "[190/00009] train_loss: 0.013049\n",
      "[190/00059] train_loss: 0.013680\n",
      "[190/00109] train_loss: 0.013514\n",
      "[190/00159] train_loss: 0.013393\n",
      "[190/00209] train_loss: 0.013112\n",
      "[190/00259] train_loss: 0.013193\n",
      "[190/00309] train_loss: 0.013843\n",
      "[190/00359] train_loss: 0.013887\n",
      "[190/00409] train_loss: 0.012966\n",
      "[190/00459] train_loss: 0.013159\n",
      "[190/00509] train_loss: 0.013797\n",
      "[190/00559] train_loss: 0.013528\n",
      "[190/00609] train_loss: 0.013002\n",
      "[190/00659] train_loss: 0.012037\n",
      "[190/00709] train_loss: 0.012551\n",
      "[190/00759] train_loss: 0.013256\n",
      "[190/00809] train_loss: 0.013254\n",
      "[190/00859] train_loss: 0.012690\n",
      "[190/00909] train_loss: 0.012233\n",
      "[190/00959] train_loss: 0.012255\n",
      "[190/01009] train_loss: 0.012439\n",
      "[190/01059] train_loss: 0.013095\n",
      "[190/01109] train_loss: 0.012997\n",
      "[190/01159] train_loss: 0.012467\n",
      "[190/01209] train_loss: 0.012394\n",
      "[191/00033] train_loss: 0.013711\n",
      "[191/00083] train_loss: 0.013810\n",
      "[191/00133] train_loss: 0.013746\n",
      "[191/00183] train_loss: 0.012610\n",
      "[191/00233] train_loss: 0.012810\n",
      "[191/00283] train_loss: 0.012905\n",
      "[191/00333] train_loss: 0.013228\n",
      "[191/00383] train_loss: 0.012970\n",
      "[191/00433] train_loss: 0.012749\n",
      "[191/00483] train_loss: 0.012579\n",
      "[191/00533] train_loss: 0.012879\n",
      "[191/00583] train_loss: 0.013344\n",
      "[191/00633] train_loss: 0.013218\n",
      "[191/00683] train_loss: 0.012760\n",
      "[191/00733] train_loss: 0.013547\n",
      "[191/00783] train_loss: 0.013158\n",
      "[191/00833] train_loss: 0.012777\n",
      "[191/00883] train_loss: 0.013251\n",
      "[191/00933] train_loss: 0.011878\n",
      "[191/00983] train_loss: 0.012986\n",
      "[191/01033] train_loss: 0.013038\n",
      "[191/01083] train_loss: 0.012333\n",
      "[191/01133] train_loss: 0.012258\n",
      "[191/01183] train_loss: 0.013628\n",
      "[192/00007] train_loss: 0.012335\n",
      "[192/00057] train_loss: 0.013891\n",
      "[192/00107] train_loss: 0.013328\n",
      "[192/00157] train_loss: 0.013193\n",
      "[192/00207] train_loss: 0.012959\n",
      "[192/00257] train_loss: 0.013354\n",
      "[192/00307] train_loss: 0.013589\n",
      "[192/00357] train_loss: 0.013065\n",
      "[192/00407] train_loss: 0.013543\n",
      "[192/00457] train_loss: 0.013442\n",
      "[192/00507] train_loss: 0.013112\n",
      "[192/00557] train_loss: 0.013033\n",
      "[192/00607] train_loss: 0.012925\n",
      "[192/00657] train_loss: 0.013214\n",
      "[192/00707] train_loss: 0.012969\n",
      "[192/00757] train_loss: 0.012745\n",
      "[192/00807] train_loss: 0.013237\n",
      "[192/00857] train_loss: 0.013447\n",
      "[192/00907] train_loss: 0.012684\n",
      "[192/00957] train_loss: 0.012684\n",
      "[192/01007] train_loss: 0.013281\n",
      "[192/01057] train_loss: 0.012702\n",
      "[192/01107] train_loss: 0.012854\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[192/01157] train_loss: 0.012586\n",
      "[192/01207] train_loss: 0.013006\n",
      "[193/00031] train_loss: 0.013920\n",
      "[193/00081] train_loss: 0.013843\n",
      "[193/00131] train_loss: 0.013294\n",
      "[193/00181] train_loss: 0.013467\n",
      "[193/00231] train_loss: 0.013778\n",
      "[193/00281] train_loss: 0.013151\n",
      "[193/00331] train_loss: 0.013820\n",
      "[193/00381] train_loss: 0.012702\n",
      "[193/00431] train_loss: 0.012510\n",
      "[193/00481] train_loss: 0.013440\n",
      "[193/00531] train_loss: 0.012589\n",
      "[193/00581] train_loss: 0.013050\n",
      "[193/00631] train_loss: 0.012840\n",
      "[193/00681] train_loss: 0.013491\n",
      "[193/00731] train_loss: 0.012144\n",
      "[193/00781] train_loss: 0.013065\n",
      "[193/00831] train_loss: 0.012912\n",
      "[193/00881] train_loss: 0.012995\n",
      "[193/00931] train_loss: 0.013000\n",
      "[193/00981] train_loss: 0.012776\n",
      "[193/01031] train_loss: 0.011501\n",
      "[193/01081] train_loss: 0.013084\n",
      "[193/01131] train_loss: 0.013145\n",
      "[193/01181] train_loss: 0.012410\n",
      "[194/00005] train_loss: 0.012038\n",
      "[194/00055] train_loss: 0.013184\n",
      "[194/00105] train_loss: 0.013487\n",
      "[194/00155] train_loss: 0.013203\n",
      "[194/00205] train_loss: 0.013278\n",
      "[194/00255] train_loss: 0.013187\n",
      "[194/00305] train_loss: 0.012979\n",
      "[194/00355] train_loss: 0.013705\n",
      "[194/00405] train_loss: 0.011796\n",
      "[194/00455] train_loss: 0.013022\n",
      "[194/00505] train_loss: 0.013025\n",
      "[194/00555] train_loss: 0.013730\n",
      "[194/00605] train_loss: 0.014056\n",
      "[194/00655] train_loss: 0.013154\n",
      "[194/00705] train_loss: 0.013445\n",
      "[194/00755] train_loss: 0.012526\n",
      "[194/00805] train_loss: 0.012136\n",
      "[194/00855] train_loss: 0.012730\n",
      "[194/00905] train_loss: 0.012898\n",
      "[194/00955] train_loss: 0.013330\n",
      "[194/01005] train_loss: 0.012837\n",
      "[194/01055] train_loss: 0.012895\n",
      "[194/01105] train_loss: 0.012251\n",
      "[194/01155] train_loss: 0.012428\n",
      "[194/01205] train_loss: 0.012332\n",
      "[195/00029] train_loss: 0.013674\n",
      "[195/00079] train_loss: 0.013627\n",
      "[195/00129] train_loss: 0.013939\n",
      "[195/00179] train_loss: 0.013165\n",
      "[195/00229] train_loss: 0.012905\n",
      "[195/00279] train_loss: 0.013432\n",
      "[195/00329] train_loss: 0.013285\n",
      "[195/00379] train_loss: 0.013325\n",
      "[195/00429] train_loss: 0.012557\n",
      "[195/00479] train_loss: 0.011976\n",
      "[195/00529] train_loss: 0.012679\n",
      "[195/00579] train_loss: 0.012384\n",
      "[195/00629] train_loss: 0.013194\n",
      "[195/00679] train_loss: 0.012910\n",
      "[195/00729] train_loss: 0.011746\n",
      "[195/00779] train_loss: 0.012977\n",
      "[195/00829] train_loss: 0.012855\n",
      "[195/00879] train_loss: 0.013073\n",
      "[195/00929] train_loss: 0.013416\n",
      "[195/00979] train_loss: 0.012541\n",
      "[195/01029] train_loss: 0.013011\n",
      "[195/01079] train_loss: 0.013613\n",
      "[195/01129] train_loss: 0.013297\n",
      "[195/01179] train_loss: 0.012686\n",
      "[196/00003] train_loss: 0.012591\n",
      "[196/00053] train_loss: 0.014524\n",
      "[196/00103] train_loss: 0.013792\n",
      "[196/00153] train_loss: 0.013359\n",
      "[196/00203] train_loss: 0.013227\n",
      "[196/00253] train_loss: 0.012768\n",
      "[196/00303] train_loss: 0.013282\n",
      "[196/00353] train_loss: 0.013530\n",
      "[196/00403] train_loss: 0.013385\n",
      "[196/00453] train_loss: 0.012757\n",
      "[196/00503] train_loss: 0.012816\n",
      "[196/00553] train_loss: 0.012473\n",
      "[196/00603] train_loss: 0.012547\n",
      "[196/00653] train_loss: 0.012366\n",
      "[196/00703] train_loss: 0.012946\n",
      "[196/00753] train_loss: 0.012818\n",
      "[196/00803] train_loss: 0.013468\n",
      "[196/00853] train_loss: 0.013506\n",
      "[196/00903] train_loss: 0.011814\n",
      "[196/00953] train_loss: 0.012612\n",
      "[196/01003] train_loss: 0.012404\n",
      "[196/01053] train_loss: 0.013371\n",
      "[196/01103] train_loss: 0.012718\n",
      "[196/01153] train_loss: 0.012472\n",
      "[196/01203] train_loss: 0.012134\n",
      "[197/00027] train_loss: 0.012983\n",
      "[197/00077] train_loss: 0.013818\n",
      "[197/00127] train_loss: 0.012992\n",
      "[197/00177] train_loss: 0.013894\n",
      "[197/00227] train_loss: 0.013337\n",
      "[197/00277] train_loss: 0.012590\n",
      "[197/00327] train_loss: 0.012765\n",
      "[197/00377] train_loss: 0.013647\n",
      "[197/00427] train_loss: 0.012290\n",
      "[197/00477] train_loss: 0.013234\n",
      "[197/00527] train_loss: 0.012811\n",
      "[197/00577] train_loss: 0.012546\n",
      "[197/00627] train_loss: 0.011313\n",
      "[197/00677] train_loss: 0.013444\n",
      "[197/00727] train_loss: 0.012357\n",
      "[197/00777] train_loss: 0.013187\n",
      "[197/00827] train_loss: 0.012618\n",
      "[197/00877] train_loss: 0.011842\n",
      "[197/00927] train_loss: 0.013628\n",
      "[197/00977] train_loss: 0.012124\n",
      "[197/01027] train_loss: 0.012860\n",
      "[197/01077] train_loss: 0.012985\n",
      "[197/01127] train_loss: 0.012497\n",
      "[197/01177] train_loss: 0.012572\n",
      "[198/00001] train_loss: 0.012744\n",
      "[198/00051] train_loss: 0.013687\n",
      "[198/00101] train_loss: 0.013905\n",
      "[198/00151] train_loss: 0.013259\n",
      "[198/00201] train_loss: 0.013464\n",
      "[198/00251] train_loss: 0.013617\n",
      "[198/00301] train_loss: 0.013678\n",
      "[198/00351] train_loss: 0.012734\n",
      "[198/00401] train_loss: 0.012466\n",
      "[198/00451] train_loss: 0.013231\n",
      "[198/00501] train_loss: 0.013346\n",
      "[198/00551] train_loss: 0.011977\n",
      "[198/00601] train_loss: 0.012035\n",
      "[198/00651] train_loss: 0.012644\n",
      "[198/00701] train_loss: 0.013221\n",
      "[198/00751] train_loss: 0.012800\n",
      "[198/00801] train_loss: 0.012643\n",
      "[198/00851] train_loss: 0.012939\n",
      "[198/00901] train_loss: 0.012641\n",
      "[198/00951] train_loss: 0.012699\n",
      "[198/01001] train_loss: 0.012765\n",
      "[198/01051] train_loss: 0.012711\n",
      "[198/01101] train_loss: 0.012569\n",
      "[198/01151] train_loss: 0.012859\n",
      "[198/01201] train_loss: 0.013172\n",
      "[199/00025] train_loss: 0.013255\n",
      "[199/00075] train_loss: 0.015001\n",
      "[199/00125] train_loss: 0.013686\n",
      "[199/00175] train_loss: 0.012999\n",
      "[199/00225] train_loss: 0.012413\n",
      "[199/00275] train_loss: 0.013682\n",
      "[199/00325] train_loss: 0.012731\n",
      "[199/00375] train_loss: 0.012652\n",
      "[199/00425] train_loss: 0.012403\n",
      "[199/00475] train_loss: 0.013432\n",
      "[199/00525] train_loss: 0.013222\n",
      "[199/00575] train_loss: 0.012350\n",
      "[199/00625] train_loss: 0.012098\n",
      "[199/00675] train_loss: 0.012226\n",
      "[199/00725] train_loss: 0.013431\n",
      "[199/00775] train_loss: 0.013160\n",
      "[199/00825] train_loss: 0.012700\n",
      "[199/00875] train_loss: 0.012914\n",
      "[199/00925] train_loss: 0.012855\n",
      "[199/00975] train_loss: 0.012464\n",
      "[199/01025] train_loss: 0.011700\n",
      "[199/01075] train_loss: 0.012963\n",
      "[199/01125] train_loss: 0.012820\n",
      "[199/01175] train_loss: 0.012359\n",
      "[199/01225] train_loss: 0.012150\n",
      "[200/00049] train_loss: 0.014088\n",
      "[200/00099] train_loss: 0.013472\n",
      "[200/00149] train_loss: 0.013682\n",
      "[200/00199] train_loss: 0.013069\n",
      "[200/00249] train_loss: 0.012856\n",
      "[200/00299] train_loss: 0.012725\n",
      "[200/00349] train_loss: 0.013505\n",
      "[200/00399] train_loss: 0.013229\n",
      "[200/00449] train_loss: 0.013692\n",
      "[200/00499] train_loss: 0.012534\n",
      "[200/00549] train_loss: 0.013757\n",
      "[200/00599] train_loss: 0.012767\n",
      "[200/00649] train_loss: 0.012772\n",
      "[200/00699] train_loss: 0.012466\n",
      "[200/00749] train_loss: 0.012701\n",
      "[200/00799] train_loss: 0.012644\n",
      "[200/00849] train_loss: 0.012047\n",
      "[200/00899] train_loss: 0.013005\n",
      "[200/00949] train_loss: 0.012311\n",
      "[200/00999] train_loss: 0.012629\n",
      "[200/01049] train_loss: 0.012844\n",
      "[200/01099] train_loss: 0.013042\n",
      "[200/01149] train_loss: 0.012680\n",
      "[200/01199] train_loss: 0.012402\n",
      "[201/00023] train_loss: 0.013127\n",
      "[201/00073] train_loss: 0.013207\n",
      "[201/00123] train_loss: 0.013791\n",
      "[201/00173] train_loss: 0.013603\n",
      "[201/00223] train_loss: 0.013338\n",
      "[201/00273] train_loss: 0.013909\n",
      "[201/00323] train_loss: 0.013302\n",
      "[201/00373] train_loss: 0.012388\n",
      "[201/00423] train_loss: 0.012474\n",
      "[201/00473] train_loss: 0.012432\n",
      "[201/00523] train_loss: 0.012685\n",
      "[201/00573] train_loss: 0.012601\n",
      "[201/00623] train_loss: 0.012381\n",
      "[201/00673] train_loss: 0.012435\n",
      "[201/00723] train_loss: 0.012300\n",
      "[201/00773] train_loss: 0.013398\n",
      "[201/00823] train_loss: 0.013304\n",
      "[201/00873] train_loss: 0.012971\n",
      "[201/00923] train_loss: 0.012846\n",
      "[201/00973] train_loss: 0.012943\n",
      "[201/01023] train_loss: 0.013053\n",
      "[201/01073] train_loss: 0.012144\n",
      "[201/01123] train_loss: 0.011854\n",
      "[201/01173] train_loss: 0.013037\n",
      "[201/01223] train_loss: 0.012500\n",
      "[202/00047] train_loss: 0.014028\n",
      "[202/00097] train_loss: 0.013038\n",
      "[202/00147] train_loss: 0.013334\n",
      "[202/00197] train_loss: 0.012948\n",
      "[202/00247] train_loss: 0.013343\n",
      "[202/00297] train_loss: 0.012669\n",
      "[202/00347] train_loss: 0.013832\n",
      "[202/00397] train_loss: 0.013330\n",
      "[202/00447] train_loss: 0.012740\n",
      "[202/00497] train_loss: 0.013071\n",
      "[202/00547] train_loss: 0.012389\n",
      "[202/00597] train_loss: 0.012819\n",
      "[202/00647] train_loss: 0.012746\n",
      "[202/00697] train_loss: 0.012875\n",
      "[202/00747] train_loss: 0.013277\n",
      "[202/00797] train_loss: 0.013818\n",
      "[202/00847] train_loss: 0.013032\n",
      "[202/00897] train_loss: 0.013235\n",
      "[202/00947] train_loss: 0.012595\n",
      "[202/00997] train_loss: 0.012242\n",
      "[202/01047] train_loss: 0.012185\n",
      "[202/01097] train_loss: 0.012316\n",
      "[202/01147] train_loss: 0.011975\n",
      "[202/01197] train_loss: 0.012989\n",
      "[203/00021] train_loss: 0.012990\n",
      "[203/00071] train_loss: 0.013876\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[203/00121] train_loss: 0.014062\n",
      "[203/00171] train_loss: 0.012452\n",
      "[203/00221] train_loss: 0.012316\n",
      "[203/00271] train_loss: 0.013173\n",
      "[203/00321] train_loss: 0.013046\n",
      "[203/00371] train_loss: 0.012972\n",
      "[203/00421] train_loss: 0.012655\n",
      "[203/00471] train_loss: 0.012404\n",
      "[203/00521] train_loss: 0.012675\n",
      "[203/00571] train_loss: 0.012607\n",
      "[203/00621] train_loss: 0.011644\n",
      "[203/00671] train_loss: 0.013119\n",
      "[203/00721] train_loss: 0.012636\n",
      "[203/00771] train_loss: 0.011931\n",
      "[203/00821] train_loss: 0.012393\n",
      "[203/00871] train_loss: 0.013454\n",
      "[203/00921] train_loss: 0.012670\n",
      "[203/00971] train_loss: 0.012529\n",
      "[203/01021] train_loss: 0.012895\n",
      "[203/01071] train_loss: 0.012699\n",
      "[203/01121] train_loss: 0.012547\n",
      "[203/01171] train_loss: 0.012464\n",
      "[203/01221] train_loss: 0.012559\n",
      "[204/00045] train_loss: 0.013892\n",
      "[204/00095] train_loss: 0.013549\n",
      "[204/00145] train_loss: 0.014172\n",
      "[204/00195] train_loss: 0.013659\n",
      "[204/00245] train_loss: 0.012776\n",
      "[204/00295] train_loss: 0.013742\n",
      "[204/00345] train_loss: 0.012648\n",
      "[204/00395] train_loss: 0.013478\n",
      "[204/00445] train_loss: 0.012031\n",
      "[204/00495] train_loss: 0.012718\n",
      "[204/00545] train_loss: 0.012584\n",
      "[204/00595] train_loss: 0.013599\n",
      "[204/00645] train_loss: 0.013057\n",
      "[204/00695] train_loss: 0.012804\n",
      "[204/00745] train_loss: 0.012144\n",
      "[204/00795] train_loss: 0.012440\n",
      "[204/00845] train_loss: 0.012853\n",
      "[204/00895] train_loss: 0.013045\n",
      "[204/00945] train_loss: 0.012946\n",
      "[204/00995] train_loss: 0.012871\n",
      "[204/01045] train_loss: 0.012245\n",
      "[204/01095] train_loss: 0.012802\n",
      "[204/01145] train_loss: 0.012306\n",
      "[204/01195] train_loss: 0.012335\n",
      "[205/00019] train_loss: 0.012592\n",
      "[205/00069] train_loss: 0.013278\n",
      "[205/00119] train_loss: 0.013139\n",
      "[205/00169] train_loss: 0.012839\n",
      "[205/00219] train_loss: 0.012491\n",
      "[205/00269] train_loss: 0.013265\n",
      "[205/00319] train_loss: 0.013330\n",
      "[205/00369] train_loss: 0.012994\n",
      "[205/00419] train_loss: 0.012245\n",
      "[205/00469] train_loss: 0.012051\n",
      "[205/00519] train_loss: 0.013114\n",
      "[205/00569] train_loss: 0.012113\n",
      "[205/00619] train_loss: 0.013181\n",
      "[205/00669] train_loss: 0.012443\n",
      "[205/00719] train_loss: 0.012177\n",
      "[205/00769] train_loss: 0.013196\n",
      "[205/00819] train_loss: 0.012952\n",
      "[205/00869] train_loss: 0.012442\n",
      "[205/00919] train_loss: 0.012424\n",
      "[205/00969] train_loss: 0.013055\n",
      "[205/01019] train_loss: 0.013219\n",
      "[205/01069] train_loss: 0.012676\n",
      "[205/01119] train_loss: 0.013161\n",
      "[205/01169] train_loss: 0.012515\n",
      "[205/01219] train_loss: 0.012069\n",
      "[206/00043] train_loss: 0.013795\n",
      "[206/00093] train_loss: 0.013927\n",
      "[206/00143] train_loss: 0.012987\n",
      "[206/00193] train_loss: 0.013492\n",
      "[206/00243] train_loss: 0.013391\n",
      "[206/00293] train_loss: 0.013342\n",
      "[206/00343] train_loss: 0.012710\n",
      "[206/00393] train_loss: 0.012832\n",
      "[206/00443] train_loss: 0.011728\n",
      "[206/00493] train_loss: 0.012813\n",
      "[206/00543] train_loss: 0.012227\n",
      "[206/00593] train_loss: 0.012585\n",
      "[206/00643] train_loss: 0.012641\n",
      "[206/00693] train_loss: 0.012161\n",
      "[206/00743] train_loss: 0.013915\n",
      "[206/00793] train_loss: 0.013043\n",
      "[206/00843] train_loss: 0.012303\n",
      "[206/00893] train_loss: 0.012959\n",
      "[206/00943] train_loss: 0.013070\n",
      "[206/00993] train_loss: 0.012656\n",
      "[206/01043] train_loss: 0.012192\n",
      "[206/01093] train_loss: 0.012712\n",
      "[206/01143] train_loss: 0.012604\n",
      "[206/01193] train_loss: 0.012754\n",
      "[207/00017] train_loss: 0.013117\n",
      "[207/00067] train_loss: 0.013428\n",
      "[207/00117] train_loss: 0.013564\n",
      "[207/00167] train_loss: 0.013254\n",
      "[207/00217] train_loss: 0.013207\n",
      "[207/00267] train_loss: 0.013886\n",
      "[207/00317] train_loss: 0.012543\n",
      "[207/00367] train_loss: 0.011919\n",
      "[207/00417] train_loss: 0.012728\n",
      "[207/00467] train_loss: 0.012761\n",
      "[207/00517] train_loss: 0.012109\n",
      "[207/00567] train_loss: 0.013100\n",
      "[207/00617] train_loss: 0.012654\n",
      "[207/00667] train_loss: 0.013249\n",
      "[207/00717] train_loss: 0.012051\n",
      "[207/00767] train_loss: 0.012795\n",
      "[207/00817] train_loss: 0.012968\n",
      "[207/00867] train_loss: 0.011872\n",
      "[207/00917] train_loss: 0.012502\n",
      "[207/00967] train_loss: 0.012890\n",
      "[207/01017] train_loss: 0.012537\n",
      "[207/01067] train_loss: 0.012686\n",
      "[207/01117] train_loss: 0.012527\n",
      "[207/01167] train_loss: 0.012598\n",
      "[207/01217] train_loss: 0.011937\n",
      "[208/00041] train_loss: 0.013187\n",
      "[208/00091] train_loss: 0.013311\n",
      "[208/00141] train_loss: 0.013207\n",
      "[208/00191] train_loss: 0.012191\n",
      "[208/00241] train_loss: 0.012978\n",
      "[208/00291] train_loss: 0.013109\n",
      "[208/00341] train_loss: 0.013400\n",
      "[208/00391] train_loss: 0.012739\n",
      "[208/00441] train_loss: 0.012394\n",
      "[208/00491] train_loss: 0.012233\n",
      "[208/00541] train_loss: 0.013411\n",
      "[208/00591] train_loss: 0.013058\n",
      "[208/00641] train_loss: 0.013353\n",
      "[208/00691] train_loss: 0.012517\n",
      "[208/00741] train_loss: 0.012375\n",
      "[208/00791] train_loss: 0.011881\n",
      "[208/00841] train_loss: 0.012951\n",
      "[208/00891] train_loss: 0.012954\n",
      "[208/00941] train_loss: 0.012528\n",
      "[208/00991] train_loss: 0.012578\n",
      "[208/01041] train_loss: 0.012629\n",
      "[208/01091] train_loss: 0.012710\n",
      "[208/01141] train_loss: 0.012572\n",
      "[208/01191] train_loss: 0.012574\n",
      "[209/00015] train_loss: 0.012816\n",
      "[209/00065] train_loss: 0.013494\n",
      "[209/00115] train_loss: 0.013123\n",
      "[209/00165] train_loss: 0.012612\n",
      "[209/00215] train_loss: 0.012680\n",
      "[209/00265] train_loss: 0.012919\n",
      "[209/00315] train_loss: 0.012323\n",
      "[209/00365] train_loss: 0.013148\n",
      "[209/00415] train_loss: 0.013275\n",
      "[209/00465] train_loss: 0.012188\n",
      "[209/00515] train_loss: 0.013009\n",
      "[209/00565] train_loss: 0.012077\n",
      "[209/00615] train_loss: 0.013530\n",
      "[209/00665] train_loss: 0.012469\n",
      "[209/00715] train_loss: 0.012883\n",
      "[209/00765] train_loss: 0.012779\n",
      "[209/00815] train_loss: 0.012546\n",
      "[209/00865] train_loss: 0.012670\n",
      "[209/00915] train_loss: 0.013518\n",
      "[209/00965] train_loss: 0.012713\n",
      "[209/01015] train_loss: 0.012497\n",
      "[209/01065] train_loss: 0.011840\n",
      "[209/01115] train_loss: 0.012771\n",
      "[209/01165] train_loss: 0.013086\n",
      "[209/01215] train_loss: 0.012939\n",
      "[210/00039] train_loss: 0.014588\n",
      "[210/00089] train_loss: 0.013707\n",
      "[210/00139] train_loss: 0.013868\n",
      "[210/00189] train_loss: 0.012559\n",
      "[210/00239] train_loss: 0.012704\n",
      "[210/00289] train_loss: 0.012251\n",
      "[210/00339] train_loss: 0.012922\n",
      "[210/00389] train_loss: 0.012517\n",
      "[210/00439] train_loss: 0.013450\n",
      "[210/00489] train_loss: 0.013091\n",
      "[210/00539] train_loss: 0.013325\n",
      "[210/00589] train_loss: 0.013231\n",
      "[210/00639] train_loss: 0.013130\n",
      "[210/00689] train_loss: 0.012497\n",
      "[210/00739] train_loss: 0.012331\n",
      "[210/00789] train_loss: 0.012383\n",
      "[210/00839] train_loss: 0.013186\n",
      "[210/00889] train_loss: 0.012487\n",
      "[210/00939] train_loss: 0.012635\n",
      "[210/00989] train_loss: 0.013095\n",
      "[210/01039] train_loss: 0.012715\n",
      "[210/01089] train_loss: 0.011089\n",
      "[210/01139] train_loss: 0.011751\n",
      "[210/01189] train_loss: 0.013010\n",
      "[211/00013] train_loss: 0.013104\n",
      "[211/00063] train_loss: 0.012914\n",
      "[211/00113] train_loss: 0.012855\n",
      "[211/00163] train_loss: 0.012218\n",
      "[211/00213] train_loss: 0.013149\n",
      "[211/00263] train_loss: 0.013014\n",
      "[211/00313] train_loss: 0.012557\n",
      "[211/00363] train_loss: 0.012676\n",
      "[211/00413] train_loss: 0.012792\n",
      "[211/00463] train_loss: 0.013659\n",
      "[211/00513] train_loss: 0.012398\n",
      "[211/00563] train_loss: 0.012586\n",
      "[211/00613] train_loss: 0.012418\n",
      "[211/00663] train_loss: 0.013069\n",
      "[211/00713] train_loss: 0.012764\n",
      "[211/00763] train_loss: 0.013713\n",
      "[211/00813] train_loss: 0.013206\n",
      "[211/00863] train_loss: 0.012565\n",
      "[211/00913] train_loss: 0.012480\n",
      "[211/00963] train_loss: 0.012295\n",
      "[211/01013] train_loss: 0.012716\n",
      "[211/01063] train_loss: 0.012655\n",
      "[211/01113] train_loss: 0.013120\n",
      "[211/01163] train_loss: 0.012164\n",
      "[211/01213] train_loss: 0.012490\n",
      "[212/00037] train_loss: 0.013189\n",
      "[212/00087] train_loss: 0.013430\n",
      "[212/00137] train_loss: 0.014007\n",
      "[212/00187] train_loss: 0.013106\n",
      "[212/00237] train_loss: 0.012261\n",
      "[212/00287] train_loss: 0.013433\n",
      "[212/00337] train_loss: 0.012471\n",
      "[212/00387] train_loss: 0.012922\n",
      "[212/00437] train_loss: 0.012452\n",
      "[212/00487] train_loss: 0.012357\n",
      "[212/00537] train_loss: 0.012823\n",
      "[212/00587] train_loss: 0.013072\n",
      "[212/00637] train_loss: 0.012533\n",
      "[212/00687] train_loss: 0.012538\n",
      "[212/00737] train_loss: 0.012720\n",
      "[212/00787] train_loss: 0.012392\n",
      "[212/00837] train_loss: 0.012653\n",
      "[212/00887] train_loss: 0.012898\n",
      "[212/00937] train_loss: 0.012261\n",
      "[212/00987] train_loss: 0.012234\n",
      "[212/01037] train_loss: 0.012029\n",
      "[212/01087] train_loss: 0.012451\n",
      "[212/01137] train_loss: 0.012223\n",
      "[212/01187] train_loss: 0.013172\n",
      "[213/00011] train_loss: 0.012749\n",
      "[213/00061] train_loss: 0.013371\n",
      "[213/00111] train_loss: 0.013812\n",
      "[213/00161] train_loss: 0.013402\n",
      "[213/00211] train_loss: 0.012927\n",
      "[213/00261] train_loss: 0.012944\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[213/00311] train_loss: 0.012844\n",
      "[213/00361] train_loss: 0.012795\n",
      "[213/00411] train_loss: 0.012801\n",
      "[213/00461] train_loss: 0.012582\n",
      "[213/00511] train_loss: 0.013845\n",
      "[213/00561] train_loss: 0.012354\n",
      "[213/00611] train_loss: 0.012878\n",
      "[213/00661] train_loss: 0.012284\n",
      "[213/00711] train_loss: 0.012162\n",
      "[213/00761] train_loss: 0.012892\n",
      "[213/00811] train_loss: 0.012629\n",
      "[213/00861] train_loss: 0.012555\n",
      "[213/00911] train_loss: 0.012643\n",
      "[213/00961] train_loss: 0.012210\n",
      "[213/01011] train_loss: 0.012331\n",
      "[213/01061] train_loss: 0.011962\n",
      "[213/01111] train_loss: 0.012326\n",
      "[213/01161] train_loss: 0.012100\n",
      "[213/01211] train_loss: 0.012114\n",
      "[214/00035] train_loss: 0.012933\n",
      "[214/00085] train_loss: 0.013481\n",
      "[214/00135] train_loss: 0.013249\n",
      "[214/00185] train_loss: 0.012735\n",
      "[214/00235] train_loss: 0.012700\n",
      "[214/00285] train_loss: 0.013343\n",
      "[214/00335] train_loss: 0.013153\n",
      "[214/00385] train_loss: 0.013492\n",
      "[214/00435] train_loss: 0.013238\n",
      "[214/00485] train_loss: 0.012505\n",
      "[214/00535] train_loss: 0.013116\n",
      "[214/00585] train_loss: 0.012845\n",
      "[214/00635] train_loss: 0.012527\n",
      "[214/00685] train_loss: 0.012508\n",
      "[214/00735] train_loss: 0.012221\n",
      "[214/00785] train_loss: 0.012743\n",
      "[214/00835] train_loss: 0.012594\n",
      "[214/00885] train_loss: 0.012500\n",
      "[214/00935] train_loss: 0.012467\n",
      "[214/00985] train_loss: 0.011217\n",
      "[214/01035] train_loss: 0.012127\n",
      "[214/01085] train_loss: 0.012964\n",
      "[214/01135] train_loss: 0.013011\n",
      "[214/01185] train_loss: 0.012519\n",
      "[215/00009] train_loss: 0.012700\n",
      "[215/00059] train_loss: 0.013408\n",
      "[215/00109] train_loss: 0.012999\n",
      "[215/00159] train_loss: 0.013128\n",
      "[215/00209] train_loss: 0.013201\n",
      "[215/00259] train_loss: 0.012539\n",
      "[215/00309] train_loss: 0.012524\n",
      "[215/00359] train_loss: 0.012138\n",
      "[215/00409] train_loss: 0.013360\n",
      "[215/00459] train_loss: 0.013136\n",
      "[215/00509] train_loss: 0.012542\n",
      "[215/00559] train_loss: 0.013075\n",
      "[215/00609] train_loss: 0.012469\n",
      "[215/00659] train_loss: 0.012169\n",
      "[215/00709] train_loss: 0.012418\n",
      "[215/00759] train_loss: 0.012464\n",
      "[215/00809] train_loss: 0.013138\n",
      "[215/00859] train_loss: 0.013287\n",
      "[215/00909] train_loss: 0.013325\n",
      "[215/00959] train_loss: 0.012619\n",
      "[215/01009] train_loss: 0.012847\n",
      "[215/01059] train_loss: 0.012526\n",
      "[215/01109] train_loss: 0.012767\n",
      "[215/01159] train_loss: 0.012382\n",
      "[215/01209] train_loss: 0.011866\n",
      "[216/00033] train_loss: 0.012864\n",
      "[216/00083] train_loss: 0.013728\n",
      "[216/00133] train_loss: 0.014021\n",
      "[216/00183] train_loss: 0.013519\n",
      "[216/00233] train_loss: 0.012721\n",
      "[216/00283] train_loss: 0.013141\n",
      "[216/00333] train_loss: 0.012958\n",
      "[216/00383] train_loss: 0.012914\n",
      "[216/00433] train_loss: 0.012101\n",
      "[216/00483] train_loss: 0.012895\n",
      "[216/00533] train_loss: 0.012325\n",
      "[216/00583] train_loss: 0.012732\n",
      "[216/00633] train_loss: 0.012255\n",
      "[216/00683] train_loss: 0.012559\n",
      "[216/00733] train_loss: 0.012929\n",
      "[216/00783] train_loss: 0.012878\n",
      "[216/00833] train_loss: 0.012326\n",
      "[216/00883] train_loss: 0.012207\n",
      "[216/00933] train_loss: 0.012163\n",
      "[216/00983] train_loss: 0.012841\n",
      "[216/01033] train_loss: 0.012405\n",
      "[216/01083] train_loss: 0.012484\n",
      "[216/01133] train_loss: 0.012400\n",
      "[216/01183] train_loss: 0.012253\n",
      "[217/00007] train_loss: 0.012618\n",
      "[217/00057] train_loss: 0.012651\n",
      "[217/00107] train_loss: 0.013015\n",
      "[217/00157] train_loss: 0.012469\n",
      "[217/00207] train_loss: 0.012716\n",
      "[217/00257] train_loss: 0.012622\n",
      "[217/00307] train_loss: 0.012781\n",
      "[217/00357] train_loss: 0.012893\n",
      "[217/00407] train_loss: 0.013042\n",
      "[217/00457] train_loss: 0.012601\n",
      "[217/00507] train_loss: 0.012899\n",
      "[217/00557] train_loss: 0.012296\n",
      "[217/00607] train_loss: 0.012711\n",
      "[217/00657] train_loss: 0.011869\n",
      "[217/00707] train_loss: 0.012981\n",
      "[217/00757] train_loss: 0.013053\n",
      "[217/00807] train_loss: 0.012787\n",
      "[217/00857] train_loss: 0.012988\n",
      "[217/00907] train_loss: 0.012570\n",
      "[217/00957] train_loss: 0.012944\n",
      "[217/01007] train_loss: 0.012625\n",
      "[217/01057] train_loss: 0.012203\n",
      "[217/01107] train_loss: 0.012676\n",
      "[217/01157] train_loss: 0.012840\n",
      "[217/01207] train_loss: 0.012274\n",
      "[218/00031] train_loss: 0.012747\n",
      "[218/00081] train_loss: 0.013644\n",
      "[218/00131] train_loss: 0.012563\n",
      "[218/00181] train_loss: 0.012933\n",
      "[218/00231] train_loss: 0.012251\n",
      "[218/00281] train_loss: 0.012664\n",
      "[218/00331] train_loss: 0.012339\n",
      "[218/00381] train_loss: 0.012665\n",
      "[218/00431] train_loss: 0.012756\n",
      "[218/00481] train_loss: 0.012914\n",
      "[218/00531] train_loss: 0.013028\n",
      "[218/00581] train_loss: 0.012466\n",
      "[218/00631] train_loss: 0.012557\n",
      "[218/00681] train_loss: 0.012322\n",
      "[218/00731] train_loss: 0.012879\n",
      "[218/00781] train_loss: 0.012310\n",
      "[218/00831] train_loss: 0.012739\n",
      "[218/00881] train_loss: 0.013801\n",
      "[218/00931] train_loss: 0.011542\n",
      "[218/00981] train_loss: 0.012610\n",
      "[218/01031] train_loss: 0.012153\n",
      "[218/01081] train_loss: 0.013226\n",
      "[218/01131] train_loss: 0.012645\n",
      "[218/01181] train_loss: 0.012851\n",
      "[219/00005] train_loss: 0.012494\n",
      "[219/00055] train_loss: 0.013286\n",
      "[219/00105] train_loss: 0.014089\n",
      "[219/00155] train_loss: 0.013029\n",
      "[219/00205] train_loss: 0.014135\n",
      "[219/00255] train_loss: 0.012863\n",
      "[219/00305] train_loss: 0.012488\n",
      "[219/00355] train_loss: 0.012623\n",
      "[219/00405] train_loss: 0.013023\n",
      "[219/00455] train_loss: 0.012173\n",
      "[219/00505] train_loss: 0.012068\n",
      "[219/00555] train_loss: 0.012866\n",
      "[219/00605] train_loss: 0.012524\n",
      "[219/00655] train_loss: 0.012400\n",
      "[219/00705] train_loss: 0.011764\n",
      "[219/00755] train_loss: 0.012677\n",
      "[219/00805] train_loss: 0.012076\n",
      "[219/00855] train_loss: 0.012771\n",
      "[219/00905] train_loss: 0.012431\n",
      "[219/00955] train_loss: 0.012127\n",
      "[219/01005] train_loss: 0.012176\n",
      "[219/01055] train_loss: 0.012475\n",
      "[219/01105] train_loss: 0.012190\n",
      "[219/01155] train_loss: 0.012009\n",
      "[219/01205] train_loss: 0.012144\n",
      "[220/00029] train_loss: 0.013714\n",
      "[220/00079] train_loss: 0.012949\n",
      "[220/00129] train_loss: 0.012636\n",
      "[220/00179] train_loss: 0.012420\n",
      "[220/00229] train_loss: 0.012817\n",
      "[220/00279] train_loss: 0.013579\n",
      "[220/00329] train_loss: 0.012684\n",
      "[220/00379] train_loss: 0.013402\n",
      "[220/00429] train_loss: 0.012554\n",
      "[220/00479] train_loss: 0.011984\n",
      "[220/00529] train_loss: 0.012510\n",
      "[220/00579] train_loss: 0.013221\n",
      "[220/00629] train_loss: 0.012379\n",
      "[220/00679] train_loss: 0.012709\n",
      "[220/00729] train_loss: 0.012952\n",
      "[220/00779] train_loss: 0.012396\n",
      "[220/00829] train_loss: 0.012154\n",
      "[220/00879] train_loss: 0.012311\n",
      "[220/00929] train_loss: 0.012718\n",
      "[220/00979] train_loss: 0.012425\n",
      "[220/01029] train_loss: 0.012904\n",
      "[220/01079] train_loss: 0.012426\n",
      "[220/01129] train_loss: 0.012616\n",
      "[220/01179] train_loss: 0.012156\n",
      "[221/00003] train_loss: 0.012774\n",
      "[221/00053] train_loss: 0.013197\n",
      "[221/00103] train_loss: 0.013303\n",
      "[221/00153] train_loss: 0.012702\n",
      "[221/00203] train_loss: 0.013567\n",
      "[221/00253] train_loss: 0.013028\n",
      "[221/00303] train_loss: 0.012936\n",
      "[221/00353] train_loss: 0.011888\n",
      "[221/00403] train_loss: 0.013713\n",
      "[221/00453] train_loss: 0.012199\n",
      "[221/00503] train_loss: 0.012970\n",
      "[221/00553] train_loss: 0.012395\n",
      "[221/00603] train_loss: 0.012252\n",
      "[221/00653] train_loss: 0.012033\n",
      "[221/00703] train_loss: 0.012335\n",
      "[221/00753] train_loss: 0.012271\n",
      "[221/00803] train_loss: 0.012959\n",
      "[221/00853] train_loss: 0.012243\n",
      "[221/00903] train_loss: 0.012168\n",
      "[221/00953] train_loss: 0.012108\n",
      "[221/01003] train_loss: 0.012574\n",
      "[221/01053] train_loss: 0.012535\n",
      "[221/01103] train_loss: 0.013007\n",
      "[221/01153] train_loss: 0.012366\n",
      "[221/01203] train_loss: 0.012521\n",
      "[222/00027] train_loss: 0.013090\n",
      "[222/00077] train_loss: 0.013268\n",
      "[222/00127] train_loss: 0.012634\n",
      "[222/00177] train_loss: 0.012565\n",
      "[222/00227] train_loss: 0.013451\n",
      "[222/00277] train_loss: 0.013444\n",
      "[222/00327] train_loss: 0.012201\n",
      "[222/00377] train_loss: 0.012761\n",
      "[222/00427] train_loss: 0.012837\n",
      "[222/00477] train_loss: 0.012544\n",
      "[222/00527] train_loss: 0.012097\n",
      "[222/00577] train_loss: 0.012839\n",
      "[222/00627] train_loss: 0.012849\n",
      "[222/00677] train_loss: 0.013008\n",
      "[222/00727] train_loss: 0.012782\n",
      "[222/00777] train_loss: 0.012242\n",
      "[222/00827] train_loss: 0.011967\n",
      "[222/00877] train_loss: 0.012350\n",
      "[222/00927] train_loss: 0.012248\n",
      "[222/00977] train_loss: 0.012765\n",
      "[222/01027] train_loss: 0.012127\n",
      "[222/01077] train_loss: 0.011645\n",
      "[222/01127] train_loss: 0.012626\n",
      "[222/01177] train_loss: 0.012201\n",
      "[223/00001] train_loss: 0.012331\n",
      "[223/00051] train_loss: 0.013383\n",
      "[223/00101] train_loss: 0.013591\n",
      "[223/00151] train_loss: 0.014085\n",
      "[223/00201] train_loss: 0.012669\n",
      "[223/00251] train_loss: 0.012038\n",
      "[223/00301] train_loss: 0.012434\n",
      "[223/00351] train_loss: 0.012920\n",
      "[223/00401] train_loss: 0.013859\n",
      "[223/00451] train_loss: 0.012153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[223/00501] train_loss: 0.012914\n",
      "[223/00551] train_loss: 0.012264\n",
      "[223/00601] train_loss: 0.012643\n",
      "[223/00651] train_loss: 0.012694\n",
      "[223/00701] train_loss: 0.013147\n",
      "[223/00751] train_loss: 0.012690\n",
      "[223/00801] train_loss: 0.013448\n",
      "[223/00851] train_loss: 0.012703\n",
      "[223/00901] train_loss: 0.012807\n",
      "[223/00951] train_loss: 0.012063\n",
      "[223/01001] train_loss: 0.012481\n",
      "[223/01051] train_loss: 0.011756\n",
      "[223/01101] train_loss: 0.011589\n",
      "[223/01151] train_loss: 0.012745\n",
      "[223/01201] train_loss: 0.012255\n",
      "[224/00025] train_loss: 0.012680\n",
      "[224/00075] train_loss: 0.012928\n",
      "[224/00125] train_loss: 0.012371\n",
      "[224/00175] train_loss: 0.012142\n",
      "[224/00225] train_loss: 0.012937\n",
      "[224/00275] train_loss: 0.012474\n",
      "[224/00325] train_loss: 0.012569\n",
      "[224/00375] train_loss: 0.012966\n",
      "[224/00425] train_loss: 0.012760\n",
      "[224/00475] train_loss: 0.013670\n",
      "[224/00525] train_loss: 0.012302\n",
      "[224/00575] train_loss: 0.012630\n",
      "[224/00625] train_loss: 0.013348\n",
      "[224/00675] train_loss: 0.012485\n",
      "[224/00725] train_loss: 0.012187\n",
      "[224/00775] train_loss: 0.012536\n",
      "[224/00825] train_loss: 0.012835\n",
      "[224/00875] train_loss: 0.012144\n",
      "[224/00925] train_loss: 0.012491\n",
      "[224/00975] train_loss: 0.012454\n",
      "[224/01025] train_loss: 0.012480\n",
      "[224/01075] train_loss: 0.013141\n",
      "[224/01125] train_loss: 0.012639\n",
      "[224/01175] train_loss: 0.013054\n",
      "[224/01225] train_loss: 0.012424\n",
      "[225/00049] train_loss: 0.013119\n",
      "[225/00099] train_loss: 0.013074\n",
      "[225/00149] train_loss: 0.013654\n",
      "[225/00199] train_loss: 0.012125\n",
      "[225/00249] train_loss: 0.012287\n",
      "[225/00299] train_loss: 0.012757\n",
      "[225/00349] train_loss: 0.012834\n",
      "[225/00399] train_loss: 0.012813\n",
      "[225/00449] train_loss: 0.012690\n",
      "[225/00499] train_loss: 0.012397\n",
      "[225/00549] train_loss: 0.012102\n",
      "[225/00599] train_loss: 0.012690\n",
      "[225/00649] train_loss: 0.012990\n",
      "[225/00699] train_loss: 0.012413\n",
      "[225/00749] train_loss: 0.012849\n",
      "[225/00799] train_loss: 0.012808\n",
      "[225/00849] train_loss: 0.011350\n",
      "[225/00899] train_loss: 0.013568\n",
      "[225/00949] train_loss: 0.012565\n",
      "[225/00999] train_loss: 0.012154\n",
      "[225/01049] train_loss: 0.011929\n",
      "[225/01099] train_loss: 0.012672\n",
      "[225/01149] train_loss: 0.012149\n",
      "[225/01199] train_loss: 0.012280\n",
      "[226/00023] train_loss: 0.012632\n",
      "[226/00073] train_loss: 0.013169\n",
      "[226/00123] train_loss: 0.013313\n",
      "[226/00173] train_loss: 0.012743\n",
      "[226/00223] train_loss: 0.013227\n",
      "[226/00273] train_loss: 0.012467\n",
      "[226/00323] train_loss: 0.012715\n",
      "[226/00373] train_loss: 0.012827\n",
      "[226/00423] train_loss: 0.012382\n",
      "[226/00473] train_loss: 0.012468\n",
      "[226/00523] train_loss: 0.012724\n",
      "[226/00573] train_loss: 0.012609\n",
      "[226/00623] train_loss: 0.013350\n",
      "[226/00673] train_loss: 0.011680\n",
      "[226/00723] train_loss: 0.012892\n",
      "[226/00773] train_loss: 0.012235\n",
      "[226/00823] train_loss: 0.012864\n",
      "[226/00873] train_loss: 0.012568\n",
      "[226/00923] train_loss: 0.013126\n",
      "[226/00973] train_loss: 0.012876\n",
      "[226/01023] train_loss: 0.012698\n",
      "[226/01073] train_loss: 0.013588\n",
      "[226/01123] train_loss: 0.011557\n",
      "[226/01173] train_loss: 0.012181\n",
      "[226/01223] train_loss: 0.012588\n",
      "[227/00047] train_loss: 0.013874\n",
      "[227/00097] train_loss: 0.012394\n",
      "[227/00147] train_loss: 0.013962\n",
      "[227/00197] train_loss: 0.013481\n",
      "[227/00247] train_loss: 0.013125\n",
      "[227/00297] train_loss: 0.013040\n",
      "[227/00347] train_loss: 0.012943\n",
      "[227/00397] train_loss: 0.012382\n",
      "[227/00447] train_loss: 0.012886\n",
      "[227/00497] train_loss: 0.012448\n",
      "[227/00547] train_loss: 0.012830\n",
      "[227/00597] train_loss: 0.012523\n",
      "[227/00647] train_loss: 0.012012\n",
      "[227/00697] train_loss: 0.012039\n",
      "[227/00747] train_loss: 0.012141\n",
      "[227/00797] train_loss: 0.011968\n",
      "[227/00847] train_loss: 0.012747\n",
      "[227/00897] train_loss: 0.011772\n",
      "[227/00947] train_loss: 0.012451\n",
      "[227/00997] train_loss: 0.012614\n",
      "[227/01047] train_loss: 0.012022\n",
      "[227/01097] train_loss: 0.012322\n",
      "[227/01147] train_loss: 0.012257\n",
      "[227/01197] train_loss: 0.012285\n",
      "[228/00021] train_loss: 0.012790\n",
      "[228/00071] train_loss: 0.012557\n",
      "[228/00121] train_loss: 0.012482\n",
      "[228/00171] train_loss: 0.013018\n",
      "[228/00221] train_loss: 0.013323\n",
      "[228/00271] train_loss: 0.013401\n",
      "[228/00321] train_loss: 0.012319\n",
      "[228/00371] train_loss: 0.012504\n",
      "[228/00421] train_loss: 0.012827\n",
      "[228/00471] train_loss: 0.012496\n",
      "[228/00521] train_loss: 0.012349\n",
      "[228/00571] train_loss: 0.012820\n",
      "[228/00621] train_loss: 0.012253\n",
      "[228/00671] train_loss: 0.012124\n",
      "[228/00721] train_loss: 0.012769\n",
      "[228/00771] train_loss: 0.012358\n",
      "[228/00821] train_loss: 0.011937\n",
      "[228/00871] train_loss: 0.011881\n",
      "[228/00921] train_loss: 0.012659\n",
      "[228/00971] train_loss: 0.012495\n",
      "[228/01021] train_loss: 0.012693\n",
      "[228/01071] train_loss: 0.012770\n",
      "[228/01121] train_loss: 0.012571\n",
      "[228/01171] train_loss: 0.012390\n",
      "[228/01221] train_loss: 0.012628\n",
      "[229/00045] train_loss: 0.012602\n",
      "[229/00095] train_loss: 0.012838\n",
      "[229/00145] train_loss: 0.013041\n",
      "[229/00195] train_loss: 0.012486\n",
      "[229/00245] train_loss: 0.013022\n",
      "[229/00295] train_loss: 0.011985\n",
      "[229/00345] train_loss: 0.013536\n",
      "[229/00395] train_loss: 0.011679\n",
      "[229/00445] train_loss: 0.012782\n",
      "[229/00495] train_loss: 0.012861\n",
      "[229/00545] train_loss: 0.013237\n",
      "[229/00595] train_loss: 0.012111\n",
      "[229/00645] train_loss: 0.012757\n",
      "[229/00695] train_loss: 0.012697\n",
      "[229/00745] train_loss: 0.012507\n",
      "[229/00795] train_loss: 0.012863\n",
      "[229/00845] train_loss: 0.012228\n",
      "[229/00895] train_loss: 0.011897\n",
      "[229/00945] train_loss: 0.011982\n",
      "[229/00995] train_loss: 0.012733\n",
      "[229/01045] train_loss: 0.011648\n",
      "[229/01095] train_loss: 0.012344\n",
      "[229/01145] train_loss: 0.011941\n",
      "[229/01195] train_loss: 0.012454\n",
      "[230/00019] train_loss: 0.012837\n",
      "[230/00069] train_loss: 0.013511\n",
      "[230/00119] train_loss: 0.013250\n",
      "[230/00169] train_loss: 0.012678\n",
      "[230/00219] train_loss: 0.012970\n",
      "[230/00269] train_loss: 0.013004\n",
      "[230/00319] train_loss: 0.011857\n",
      "[230/00369] train_loss: 0.013120\n",
      "[230/00419] train_loss: 0.012806\n",
      "[230/00469] train_loss: 0.012543\n",
      "[230/00519] train_loss: 0.012507\n",
      "[230/00569] train_loss: 0.012861\n",
      "[230/00619] train_loss: 0.012204\n",
      "[230/00669] train_loss: 0.013078\n",
      "[230/00719] train_loss: 0.012428\n",
      "[230/00769] train_loss: 0.012161\n",
      "[230/00819] train_loss: 0.011943\n",
      "[230/00869] train_loss: 0.012234\n",
      "[230/00919] train_loss: 0.012199\n",
      "[230/00969] train_loss: 0.012459\n",
      "[230/01019] train_loss: 0.012560\n",
      "[230/01069] train_loss: 0.013002\n",
      "[230/01119] train_loss: 0.013162\n",
      "[230/01169] train_loss: 0.012536\n",
      "[230/01219] train_loss: 0.011266\n",
      "[231/00043] train_loss: 0.012743\n",
      "[231/00093] train_loss: 0.012973\n",
      "[231/00143] train_loss: 0.013083\n",
      "[231/00193] train_loss: 0.012179\n",
      "[231/00243] train_loss: 0.012714\n",
      "[231/00293] train_loss: 0.013106\n",
      "[231/00343] train_loss: 0.013046\n",
      "[231/00393] train_loss: 0.012852\n",
      "[231/00443] train_loss: 0.012551\n",
      "[231/00493] train_loss: 0.012553\n",
      "[231/00543] train_loss: 0.013069\n",
      "[231/00593] train_loss: 0.012340\n",
      "[231/00643] train_loss: 0.012494\n",
      "[231/00693] train_loss: 0.012087\n",
      "[231/00743] train_loss: 0.013109\n",
      "[231/00793] train_loss: 0.012696\n",
      "[231/00843] train_loss: 0.012334\n",
      "[231/00893] train_loss: 0.011708\n",
      "[231/00943] train_loss: 0.012681\n",
      "[231/00993] train_loss: 0.012259\n",
      "[231/01043] train_loss: 0.011991\n",
      "[231/01093] train_loss: 0.012197\n",
      "[231/01143] train_loss: 0.011989\n",
      "[231/01193] train_loss: 0.012062\n",
      "[232/00017] train_loss: 0.012911\n",
      "[232/00067] train_loss: 0.013201\n",
      "[232/00117] train_loss: 0.012730\n",
      "[232/00167] train_loss: 0.013299\n",
      "[232/00217] train_loss: 0.013126\n",
      "[232/00267] train_loss: 0.012807\n",
      "[232/00317] train_loss: 0.013316\n",
      "[232/00367] train_loss: 0.012982\n",
      "[232/00417] train_loss: 0.013000\n",
      "[232/00467] train_loss: 0.013176\n",
      "[232/00517] train_loss: 0.012560\n",
      "[232/00567] train_loss: 0.012652\n",
      "[232/00617] train_loss: 0.012275\n",
      "[232/00667] train_loss: 0.012229\n",
      "[232/00717] train_loss: 0.012353\n",
      "[232/00767] train_loss: 0.012925\n",
      "[232/00817] train_loss: 0.012505\n",
      "[232/00867] train_loss: 0.011598\n",
      "[232/00917] train_loss: 0.012314\n",
      "[232/00967] train_loss: 0.012485\n",
      "[232/01017] train_loss: 0.012493\n",
      "[232/01067] train_loss: 0.012208\n",
      "[232/01117] train_loss: 0.011816\n",
      "[232/01167] train_loss: 0.012002\n",
      "[232/01217] train_loss: 0.012593\n",
      "[233/00041] train_loss: 0.013554\n",
      "[233/00091] train_loss: 0.013428\n",
      "[233/00141] train_loss: 0.012777\n",
      "[233/00191] train_loss: 0.012572\n",
      "[233/00241] train_loss: 0.012959\n",
      "[233/00291] train_loss: 0.013185\n",
      "[233/00341] train_loss: 0.012445\n",
      "[233/00391] train_loss: 0.012096\n",
      "[233/00441] train_loss: 0.012316\n",
      "[233/00491] train_loss: 0.012846\n",
      "[233/00541] train_loss: 0.012497\n",
      "[233/00591] train_loss: 0.012572\n",
      "[233/00641] train_loss: 0.012704\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[233/00691] train_loss: 0.012215\n",
      "[233/00741] train_loss: 0.012724\n",
      "[233/00791] train_loss: 0.012837\n",
      "[233/00841] train_loss: 0.012788\n",
      "[233/00891] train_loss: 0.012376\n",
      "[233/00941] train_loss: 0.012088\n",
      "[233/00991] train_loss: 0.011940\n",
      "[233/01041] train_loss: 0.012742\n",
      "[233/01091] train_loss: 0.011891\n",
      "[233/01141] train_loss: 0.011925\n",
      "[233/01191] train_loss: 0.011604\n",
      "[234/00015] train_loss: 0.012510\n",
      "[234/00065] train_loss: 0.013097\n",
      "[234/00115] train_loss: 0.012184\n",
      "[234/00165] train_loss: 0.013509\n",
      "[234/00215] train_loss: 0.013055\n",
      "[234/00265] train_loss: 0.012370\n",
      "[234/00315] train_loss: 0.012740\n",
      "[234/00365] train_loss: 0.012707\n",
      "[234/00415] train_loss: 0.011696\n",
      "[234/00465] train_loss: 0.012634\n",
      "[234/00515] train_loss: 0.012851\n",
      "[234/00565] train_loss: 0.012347\n",
      "[234/00615] train_loss: 0.013250\n",
      "[234/00665] train_loss: 0.012209\n",
      "[234/00715] train_loss: 0.011745\n",
      "[234/00765] train_loss: 0.012810\n",
      "[234/00815] train_loss: 0.012510\n",
      "[234/00865] train_loss: 0.012505\n",
      "[234/00915] train_loss: 0.012780\n",
      "[234/00965] train_loss: 0.012399\n",
      "[234/01015] train_loss: 0.011431\n",
      "[234/01065] train_loss: 0.012479\n",
      "[234/01115] train_loss: 0.012881\n",
      "[234/01165] train_loss: 0.011849\n",
      "[234/01215] train_loss: 0.012249\n",
      "[235/00039] train_loss: 0.012595\n",
      "[235/00089] train_loss: 0.013069\n",
      "[235/00139] train_loss: 0.013058\n",
      "[235/00189] train_loss: 0.012924\n",
      "[235/00239] train_loss: 0.012778\n",
      "[235/00289] train_loss: 0.011722\n",
      "[235/00339] train_loss: 0.012735\n",
      "[235/00389] train_loss: 0.012654\n",
      "[235/00439] train_loss: 0.012319\n",
      "[235/00489] train_loss: 0.013283\n",
      "[235/00539] train_loss: 0.011447\n",
      "[235/00589] train_loss: 0.013336\n",
      "[235/00639] train_loss: 0.012324\n",
      "[235/00689] train_loss: 0.012471\n",
      "[235/00739] train_loss: 0.012125\n",
      "[235/00789] train_loss: 0.012464\n",
      "[235/00839] train_loss: 0.011487\n",
      "[235/00889] train_loss: 0.012551\n",
      "[235/00939] train_loss: 0.012431\n",
      "[235/00989] train_loss: 0.012168\n",
      "[235/01039] train_loss: 0.012258\n",
      "[235/01089] train_loss: 0.013034\n",
      "[235/01139] train_loss: 0.012692\n",
      "[235/01189] train_loss: 0.012400\n",
      "[236/00013] train_loss: 0.013361\n",
      "[236/00063] train_loss: 0.013198\n",
      "[236/00113] train_loss: 0.012641\n",
      "[236/00163] train_loss: 0.012381\n",
      "[236/00213] train_loss: 0.013195\n",
      "[236/00263] train_loss: 0.012925\n",
      "[236/00313] train_loss: 0.012415\n",
      "[236/00363] train_loss: 0.013507\n",
      "[236/00413] train_loss: 0.011769\n",
      "[236/00463] train_loss: 0.012730\n",
      "[236/00513] train_loss: 0.011757\n",
      "[236/00563] train_loss: 0.012335\n",
      "[236/00613] train_loss: 0.012666\n",
      "[236/00663] train_loss: 0.012620\n",
      "[236/00713] train_loss: 0.012322\n",
      "[236/00763] train_loss: 0.012802\n",
      "[236/00813] train_loss: 0.013062\n",
      "[236/00863] train_loss: 0.012488\n",
      "[236/00913] train_loss: 0.012615\n",
      "[236/00963] train_loss: 0.012717\n",
      "[236/01013] train_loss: 0.012269\n",
      "[236/01063] train_loss: 0.012701\n",
      "[236/01113] train_loss: 0.011928\n",
      "[236/01163] train_loss: 0.012087\n",
      "[236/01213] train_loss: 0.012042\n",
      "[237/00037] train_loss: 0.012842\n",
      "[237/00087] train_loss: 0.012992\n",
      "[237/00137] train_loss: 0.013151\n",
      "[237/00187] train_loss: 0.011948\n",
      "[237/00237] train_loss: 0.011912\n",
      "[237/00287] train_loss: 0.012141\n",
      "[237/00337] train_loss: 0.013441\n",
      "[237/00387] train_loss: 0.012576\n",
      "[237/00437] train_loss: 0.012817\n",
      "[237/00487] train_loss: 0.012293\n",
      "[237/00537] train_loss: 0.012154\n",
      "[237/00587] train_loss: 0.013079\n",
      "[237/00637] train_loss: 0.013127\n",
      "[237/00687] train_loss: 0.011820\n",
      "[237/00737] train_loss: 0.012349\n",
      "[237/00787] train_loss: 0.012378\n",
      "[237/00837] train_loss: 0.012826\n",
      "[237/00887] train_loss: 0.012051\n",
      "[237/00937] train_loss: 0.012314\n",
      "[237/00987] train_loss: 0.012116\n",
      "[237/01037] train_loss: 0.012693\n",
      "[237/01087] train_loss: 0.012187\n",
      "[237/01137] train_loss: 0.012482\n",
      "[237/01187] train_loss: 0.011803\n",
      "[238/00011] train_loss: 0.013114\n",
      "[238/00061] train_loss: 0.013057\n",
      "[238/00111] train_loss: 0.012775\n",
      "[238/00161] train_loss: 0.013133\n",
      "[238/00211] train_loss: 0.012691\n",
      "[238/00261] train_loss: 0.013062\n",
      "[238/00311] train_loss: 0.012441\n",
      "[238/00361] train_loss: 0.013090\n",
      "[238/00411] train_loss: 0.012127\n",
      "[238/00461] train_loss: 0.012367\n",
      "[238/00511] train_loss: 0.013211\n",
      "[238/00561] train_loss: 0.012699\n",
      "[238/00611] train_loss: 0.013154\n",
      "[238/00661] train_loss: 0.012300\n",
      "[238/00711] train_loss: 0.012370\n",
      "[238/00761] train_loss: 0.012445\n",
      "[238/00811] train_loss: 0.012247\n",
      "[238/00861] train_loss: 0.012735\n",
      "[238/00911] train_loss: 0.012349\n",
      "[238/00961] train_loss: 0.011729\n",
      "[238/01011] train_loss: 0.012224\n",
      "[238/01061] train_loss: 0.012622\n",
      "[238/01111] train_loss: 0.011716\n",
      "[238/01161] train_loss: 0.012420\n",
      "[238/01211] train_loss: 0.012068\n",
      "[239/00035] train_loss: 0.012524\n",
      "[239/00085] train_loss: 0.013240\n",
      "[239/00135] train_loss: 0.012816\n",
      "[239/00185] train_loss: 0.013310\n",
      "[239/00235] train_loss: 0.012680\n",
      "[239/00285] train_loss: 0.012817\n",
      "[239/00335] train_loss: 0.012308\n",
      "[239/00385] train_loss: 0.011883\n",
      "[239/00435] train_loss: 0.012533\n",
      "[239/00485] train_loss: 0.012487\n",
      "[239/00535] train_loss: 0.012656\n",
      "[239/00585] train_loss: 0.011838\n",
      "[239/00635] train_loss: 0.012406\n",
      "[239/00685] train_loss: 0.012696\n",
      "[239/00735] train_loss: 0.011394\n",
      "[239/00785] train_loss: 0.011901\n",
      "[239/00835] train_loss: 0.011803\n",
      "[239/00885] train_loss: 0.011597\n",
      "[239/00935] train_loss: 0.012301\n",
      "[239/00985] train_loss: 0.011748\n",
      "[239/01035] train_loss: 0.012993\n",
      "[239/01085] train_loss: 0.012808\n",
      "[239/01135] train_loss: 0.012607\n",
      "[239/01185] train_loss: 0.012997\n",
      "[240/00009] train_loss: 0.012494\n",
      "[240/00059] train_loss: 0.012889\n",
      "[240/00109] train_loss: 0.013199\n",
      "[240/00159] train_loss: 0.012230\n",
      "[240/00209] train_loss: 0.012784\n",
      "[240/00259] train_loss: 0.013463\n",
      "[240/00309] train_loss: 0.013075\n",
      "[240/00359] train_loss: 0.011902\n",
      "[240/00409] train_loss: 0.011663\n",
      "[240/00459] train_loss: 0.012216\n",
      "[240/00509] train_loss: 0.012834\n",
      "[240/00559] train_loss: 0.012909\n",
      "[240/00609] train_loss: 0.012536\n",
      "[240/00659] train_loss: 0.011677\n",
      "[240/00709] train_loss: 0.012289\n",
      "[240/00759] train_loss: 0.012117\n",
      "[240/00809] train_loss: 0.012359\n",
      "[240/00859] train_loss: 0.012592\n",
      "[240/00909] train_loss: 0.012328\n",
      "[240/00959] train_loss: 0.012255\n",
      "[240/01009] train_loss: 0.012395\n",
      "[240/01059] train_loss: 0.012773\n",
      "[240/01109] train_loss: 0.011880\n",
      "[240/01159] train_loss: 0.012243\n",
      "[240/01209] train_loss: 0.012135\n",
      "[241/00033] train_loss: 0.013252\n",
      "[241/00083] train_loss: 0.012780\n",
      "[241/00133] train_loss: 0.012810\n",
      "[241/00183] train_loss: 0.013247\n",
      "[241/00233] train_loss: 0.013209\n",
      "[241/00283] train_loss: 0.012402\n",
      "[241/00333] train_loss: 0.013129\n",
      "[241/00383] train_loss: 0.012273\n",
      "[241/00433] train_loss: 0.012836\n",
      "[241/00483] train_loss: 0.012186\n",
      "[241/00533] train_loss: 0.012112\n",
      "[241/00583] train_loss: 0.012141\n",
      "[241/00633] train_loss: 0.012761\n",
      "[241/00683] train_loss: 0.012596\n",
      "[241/00733] train_loss: 0.012700\n",
      "[241/00783] train_loss: 0.012417\n",
      "[241/00833] train_loss: 0.012529\n",
      "[241/00883] train_loss: 0.012005\n",
      "[241/00933] train_loss: 0.012453\n",
      "[241/00983] train_loss: 0.011956\n",
      "[241/01033] train_loss: 0.012333\n",
      "[241/01083] train_loss: 0.011687\n",
      "[241/01133] train_loss: 0.012059\n",
      "[241/01183] train_loss: 0.011873\n",
      "[242/00007] train_loss: 0.013314\n",
      "[242/00057] train_loss: 0.013090\n",
      "[242/00107] train_loss: 0.012401\n",
      "[242/00157] train_loss: 0.013320\n",
      "[242/00207] train_loss: 0.012220\n",
      "[242/00257] train_loss: 0.012315\n",
      "[242/00307] train_loss: 0.012708\n",
      "[242/00357] train_loss: 0.012049\n",
      "[242/00407] train_loss: 0.012587\n",
      "[242/00457] train_loss: 0.012262\n",
      "[242/00507] train_loss: 0.012842\n",
      "[242/00557] train_loss: 0.012699\n",
      "[242/00607] train_loss: 0.012333\n",
      "[242/00657] train_loss: 0.012439\n",
      "[242/00707] train_loss: 0.012854\n",
      "[242/00757] train_loss: 0.012038\n",
      "[242/00807] train_loss: 0.013237\n",
      "[242/00857] train_loss: 0.012323\n",
      "[242/00907] train_loss: 0.012633\n",
      "[242/00957] train_loss: 0.011908\n",
      "[242/01007] train_loss: 0.011469\n",
      "[242/01057] train_loss: 0.013016\n",
      "[242/01107] train_loss: 0.012468\n",
      "[242/01157] train_loss: 0.012481\n",
      "[242/01207] train_loss: 0.012536\n",
      "[243/00031] train_loss: 0.012472\n",
      "[243/00081] train_loss: 0.013438\n",
      "[243/00131] train_loss: 0.012859\n",
      "[243/00181] train_loss: 0.012438\n",
      "[243/00231] train_loss: 0.012368\n",
      "[243/00281] train_loss: 0.012596\n",
      "[243/00331] train_loss: 0.012093\n",
      "[243/00381] train_loss: 0.012107\n",
      "[243/00431] train_loss: 0.012213\n",
      "[243/00481] train_loss: 0.012431\n",
      "[243/00531] train_loss: 0.012735\n",
      "[243/00581] train_loss: 0.012937\n",
      "[243/00631] train_loss: 0.012059\n",
      "[243/00681] train_loss: 0.012433\n",
      "[243/00731] train_loss: 0.012278\n",
      "[243/00781] train_loss: 0.011929\n",
      "[243/00831] train_loss: 0.011591\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[243/00881] train_loss: 0.012020\n",
      "[243/00931] train_loss: 0.012023\n",
      "[243/00981] train_loss: 0.012329\n",
      "[243/01031] train_loss: 0.012655\n",
      "[243/01081] train_loss: 0.012242\n",
      "[243/01131] train_loss: 0.013166\n",
      "[243/01181] train_loss: 0.012204\n",
      "[244/00005] train_loss: 0.012306\n",
      "[244/00055] train_loss: 0.012930\n",
      "[244/00105] train_loss: 0.013316\n",
      "[244/00155] train_loss: 0.012507\n",
      "[244/00205] train_loss: 0.012283\n",
      "[244/00255] train_loss: 0.012628\n",
      "[244/00305] train_loss: 0.012715\n",
      "[244/00355] train_loss: 0.012677\n",
      "[244/00405] train_loss: 0.012399\n",
      "[244/00455] train_loss: 0.012997\n",
      "[244/00505] train_loss: 0.012130\n",
      "[244/00555] train_loss: 0.012972\n",
      "[244/00605] train_loss: 0.012518\n",
      "[244/00655] train_loss: 0.012873\n",
      "[244/00705] train_loss: 0.012387\n",
      "[244/00755] train_loss: 0.011557\n",
      "[244/00805] train_loss: 0.013577\n",
      "[244/00855] train_loss: 0.011944\n",
      "[244/00905] train_loss: 0.012027\n",
      "[244/00955] train_loss: 0.011875\n",
      "[244/01005] train_loss: 0.011651\n",
      "[244/01055] train_loss: 0.012448\n",
      "[244/01105] train_loss: 0.011751\n",
      "[244/01155] train_loss: 0.012347\n",
      "[244/01205] train_loss: 0.011450\n",
      "[245/00029] train_loss: 0.013053\n",
      "[245/00079] train_loss: 0.012523\n",
      "[245/00129] train_loss: 0.012715\n",
      "[245/00179] train_loss: 0.013265\n",
      "[245/00229] train_loss: 0.013157\n",
      "[245/00279] train_loss: 0.013480\n",
      "[245/00329] train_loss: 0.012540\n",
      "[245/00379] train_loss: 0.012366\n",
      "[245/00429] train_loss: 0.012296\n",
      "[245/00479] train_loss: 0.011846\n",
      "[245/00529] train_loss: 0.011658\n",
      "[245/00579] train_loss: 0.012681\n",
      "[245/00629] train_loss: 0.012272\n",
      "[245/00679] train_loss: 0.012418\n",
      "[245/00729] train_loss: 0.013150\n",
      "[245/00779] train_loss: 0.012199\n",
      "[245/00829] train_loss: 0.012681\n",
      "[245/00879] train_loss: 0.012529\n",
      "[245/00929] train_loss: 0.012194\n",
      "[245/00979] train_loss: 0.012574\n",
      "[245/01029] train_loss: 0.012174\n",
      "[245/01079] train_loss: 0.011995\n",
      "[245/01129] train_loss: 0.012692\n",
      "[245/01179] train_loss: 0.012465\n",
      "[246/00003] train_loss: 0.012625\n",
      "[246/00053] train_loss: 0.013299\n",
      "[246/00103] train_loss: 0.013050\n",
      "[246/00153] train_loss: 0.012754\n",
      "[246/00203] train_loss: 0.012034\n",
      "[246/00253] train_loss: 0.012881\n",
      "[246/00303] train_loss: 0.012651\n",
      "[246/00353] train_loss: 0.012417\n",
      "[246/00403] train_loss: 0.012533\n",
      "[246/00453] train_loss: 0.012194\n",
      "[246/00503] train_loss: 0.012602\n",
      "[246/00553] train_loss: 0.012840\n",
      "[246/00603] train_loss: 0.012468\n",
      "[246/00653] train_loss: 0.011630\n",
      "[246/00703] train_loss: 0.011831\n",
      "[246/00753] train_loss: 0.012263\n",
      "[246/00803] train_loss: 0.012195\n",
      "[246/00853] train_loss: 0.012750\n",
      "[246/00903] train_loss: 0.011859\n",
      "[246/00953] train_loss: 0.013118\n",
      "[246/01003] train_loss: 0.012369\n",
      "[246/01053] train_loss: 0.012023\n",
      "[246/01103] train_loss: 0.012888\n",
      "[246/01153] train_loss: 0.012148\n",
      "[246/01203] train_loss: 0.012177\n",
      "[247/00027] train_loss: 0.012316\n",
      "[247/00077] train_loss: 0.012506\n",
      "[247/00127] train_loss: 0.013059\n",
      "[247/00177] train_loss: 0.012458\n",
      "[247/00227] train_loss: 0.012754\n",
      "[247/00277] train_loss: 0.013196\n",
      "[247/00327] train_loss: 0.013136\n",
      "[247/00377] train_loss: 0.012508\n",
      "[247/00427] train_loss: 0.012684\n",
      "[247/00477] train_loss: 0.012186\n",
      "[247/00527] train_loss: 0.012830\n",
      "[247/00577] train_loss: 0.012034\n",
      "[247/00627] train_loss: 0.012783\n",
      "[247/00677] train_loss: 0.012834\n",
      "[247/00727] train_loss: 0.012299\n",
      "[247/00777] train_loss: 0.012358\n",
      "[247/00827] train_loss: 0.012623\n",
      "[247/00877] train_loss: 0.012336\n",
      "[247/00927] train_loss: 0.012236\n",
      "[247/00977] train_loss: 0.012662\n",
      "[247/01027] train_loss: 0.013038\n",
      "[247/01077] train_loss: 0.011716\n",
      "[247/01127] train_loss: 0.011281\n",
      "[247/01177] train_loss: 0.011946\n",
      "[248/00001] train_loss: 0.011783\n",
      "[248/00051] train_loss: 0.013379\n",
      "[248/00101] train_loss: 0.013419\n",
      "[248/00151] train_loss: 0.012408\n",
      "[248/00201] train_loss: 0.012306\n",
      "[248/00251] train_loss: 0.012570\n",
      "[248/00301] train_loss: 0.013244\n",
      "[248/00351] train_loss: 0.012312\n",
      "[248/00401] train_loss: 0.012124\n",
      "[248/00451] train_loss: 0.012551\n",
      "[248/00501] train_loss: 0.011648\n",
      "[248/00551] train_loss: 0.012906\n",
      "[248/00601] train_loss: 0.012447\n",
      "[248/00651] train_loss: 0.012949\n",
      "[248/00701] train_loss: 0.011863\n",
      "[248/00751] train_loss: 0.012433\n",
      "[248/00801] train_loss: 0.012581\n",
      "[248/00851] train_loss: 0.012398\n",
      "[248/00901] train_loss: 0.012811\n",
      "[248/00951] train_loss: 0.012141\n",
      "[248/01001] train_loss: 0.012437\n",
      "[248/01051] train_loss: 0.012200\n",
      "[248/01101] train_loss: 0.011780\n",
      "[248/01151] train_loss: 0.012838\n",
      "[248/01201] train_loss: 0.012099\n",
      "[249/00025] train_loss: 0.012532\n",
      "[249/00075] train_loss: 0.013456\n",
      "[249/00125] train_loss: 0.012618\n",
      "[249/00175] train_loss: 0.012624\n",
      "[249/00225] train_loss: 0.012553\n",
      "[249/00275] train_loss: 0.011856\n",
      "[249/00325] train_loss: 0.013399\n",
      "[249/00375] train_loss: 0.012629\n",
      "[249/00425] train_loss: 0.012452\n",
      "[249/00475] train_loss: 0.013135\n",
      "[249/00525] train_loss: 0.012649\n",
      "[249/00575] train_loss: 0.011811\n",
      "[249/00625] train_loss: 0.012127\n",
      "[249/00675] train_loss: 0.011809\n",
      "[249/00725] train_loss: 0.011814\n",
      "[249/00775] train_loss: 0.011888\n",
      "[249/00825] train_loss: 0.011680\n",
      "[249/00875] train_loss: 0.011798\n",
      "[249/00925] train_loss: 0.011975\n",
      "[249/00975] train_loss: 0.012026\n",
      "[249/01025] train_loss: 0.012007\n",
      "[249/01075] train_loss: 0.012108\n",
      "[249/01125] train_loss: 0.012008\n",
      "[249/01175] train_loss: 0.012622\n",
      "[249/01225] train_loss: 0.012053\n",
      "[250/00049] train_loss: 0.012766\n",
      "[250/00099] train_loss: 0.012777\n",
      "[250/00149] train_loss: 0.012155\n",
      "[250/00199] train_loss: 0.013060\n",
      "[250/00249] train_loss: 0.012156\n",
      "[250/00299] train_loss: 0.012323\n",
      "[250/00349] train_loss: 0.012223\n",
      "[250/00399] train_loss: 0.012410\n",
      "[250/00449] train_loss: 0.013197\n",
      "[250/00499] train_loss: 0.012171\n",
      "[250/00549] train_loss: 0.012117\n",
      "[250/00599] train_loss: 0.012446\n",
      "[250/00649] train_loss: 0.012335\n",
      "[250/00699] train_loss: 0.012873\n",
      "[250/00749] train_loss: 0.012246\n",
      "[250/00799] train_loss: 0.012251\n",
      "[250/00849] train_loss: 0.013324\n",
      "[250/00899] train_loss: 0.012135\n",
      "[250/00949] train_loss: 0.011889\n",
      "[250/00999] train_loss: 0.012974\n",
      "[250/01049] train_loss: 0.011870\n",
      "[250/01099] train_loss: 0.012318\n",
      "[250/01149] train_loss: 0.012063\n",
      "[250/01199] train_loss: 0.012089\n",
      "[251/00023] train_loss: 0.012371\n",
      "[251/00073] train_loss: 0.012365\n",
      "[251/00123] train_loss: 0.012953\n",
      "[251/00173] train_loss: 0.012428\n",
      "[251/00223] train_loss: 0.012508\n",
      "[251/00273] train_loss: 0.011981\n",
      "[251/00323] train_loss: 0.011677\n",
      "[251/00373] train_loss: 0.012398\n",
      "[251/00423] train_loss: 0.013223\n",
      "[251/00473] train_loss: 0.012033\n",
      "[251/00523] train_loss: 0.013098\n",
      "[251/00573] train_loss: 0.012073\n",
      "[251/00623] train_loss: 0.013024\n",
      "[251/00673] train_loss: 0.012176\n",
      "[251/00723] train_loss: 0.012419\n",
      "[251/00773] train_loss: 0.012094\n",
      "[251/00823] train_loss: 0.012351\n",
      "[251/00873] train_loss: 0.011826\n",
      "[251/00923] train_loss: 0.011688\n",
      "[251/00973] train_loss: 0.011612\n",
      "[251/01023] train_loss: 0.012757\n",
      "[251/01073] train_loss: 0.012277\n",
      "[251/01123] train_loss: 0.012287\n",
      "[251/01173] train_loss: 0.012697\n",
      "[251/01223] train_loss: 0.011638\n",
      "[252/00047] train_loss: 0.012404\n",
      "[252/00097] train_loss: 0.012030\n",
      "[252/00147] train_loss: 0.012535\n",
      "[252/00197] train_loss: 0.012122\n",
      "[252/00247] train_loss: 0.012869\n",
      "[252/00297] train_loss: 0.012214\n",
      "[252/00347] train_loss: 0.012223\n",
      "[252/00397] train_loss: 0.012408\n",
      "[252/00447] train_loss: 0.011942\n",
      "[252/00497] train_loss: 0.012577\n",
      "[252/00547] train_loss: 0.011995\n",
      "[252/00597] train_loss: 0.012144\n",
      "[252/00647] train_loss: 0.012683\n",
      "[252/00697] train_loss: 0.012485\n",
      "[252/00747] train_loss: 0.012748\n",
      "[252/00797] train_loss: 0.012374\n",
      "[252/00847] train_loss: 0.012802\n",
      "[252/00897] train_loss: 0.012622\n",
      "[252/00947] train_loss: 0.012581\n",
      "[252/00997] train_loss: 0.012065\n",
      "[252/01047] train_loss: 0.011805\n",
      "[252/01097] train_loss: 0.012210\n",
      "[252/01147] train_loss: 0.012930\n",
      "[252/01197] train_loss: 0.012550\n",
      "[253/00021] train_loss: 0.012676\n",
      "[253/00071] train_loss: 0.012156\n",
      "[253/00121] train_loss: 0.013058\n",
      "[253/00171] train_loss: 0.013366\n",
      "[253/00221] train_loss: 0.012901\n",
      "[253/00271] train_loss: 0.012001\n",
      "[253/00321] train_loss: 0.012535\n",
      "[253/00371] train_loss: 0.012258\n",
      "[253/00421] train_loss: 0.012413\n",
      "[253/00471] train_loss: 0.012777\n",
      "[253/00521] train_loss: 0.011181\n",
      "[253/00571] train_loss: 0.012264\n",
      "[253/00621] train_loss: 0.011915\n",
      "[253/00671] train_loss: 0.012040\n",
      "[253/00721] train_loss: 0.012766\n",
      "[253/00771] train_loss: 0.012154\n",
      "[253/00821] train_loss: 0.012493\n",
      "[253/00871] train_loss: 0.012113\n",
      "[253/00921] train_loss: 0.011849\n",
      "[253/00971] train_loss: 0.011267\n",
      "[253/01021] train_loss: 0.011831\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[253/01071] train_loss: 0.012739\n",
      "[253/01121] train_loss: 0.012823\n",
      "[253/01171] train_loss: 0.011909\n",
      "[253/01221] train_loss: 0.011743\n",
      "[254/00045] train_loss: 0.013322\n",
      "[254/00095] train_loss: 0.013360\n",
      "[254/00145] train_loss: 0.012465\n",
      "[254/00195] train_loss: 0.012855\n",
      "[254/00245] train_loss: 0.012131\n",
      "[254/00295] train_loss: 0.012423\n",
      "[254/00345] train_loss: 0.012492\n",
      "[254/00395] train_loss: 0.013403\n",
      "[254/00445] train_loss: 0.011990\n",
      "[254/00495] train_loss: 0.011589\n",
      "[254/00545] train_loss: 0.012931\n",
      "[254/00595] train_loss: 0.012056\n",
      "[254/00645] train_loss: 0.011594\n",
      "[254/00695] train_loss: 0.012250\n",
      "[254/00745] train_loss: 0.011643\n",
      "[254/00795] train_loss: 0.012488\n",
      "[254/00845] train_loss: 0.012302\n",
      "[254/00895] train_loss: 0.012993\n",
      "[254/00945] train_loss: 0.012481\n",
      "[254/00995] train_loss: 0.012102\n",
      "[254/01045] train_loss: 0.012271\n",
      "[254/01095] train_loss: 0.011816\n",
      "[254/01145] train_loss: 0.011781\n",
      "[254/01195] train_loss: 0.012187\n",
      "[255/00019] train_loss: 0.013096\n",
      "[255/00069] train_loss: 0.012183\n",
      "[255/00119] train_loss: 0.012402\n",
      "[255/00169] train_loss: 0.012902\n",
      "[255/00219] train_loss: 0.012413\n",
      "[255/00269] train_loss: 0.012670\n",
      "[255/00319] train_loss: 0.012023\n",
      "[255/00369] train_loss: 0.012364\n",
      "[255/00419] train_loss: 0.011936\n",
      "[255/00469] train_loss: 0.011589\n",
      "[255/00519] train_loss: 0.011857\n",
      "[255/00569] train_loss: 0.012180\n",
      "[255/00619] train_loss: 0.012173\n",
      "[255/00669] train_loss: 0.011700\n",
      "[255/00719] train_loss: 0.012596\n",
      "[255/00769] train_loss: 0.012360\n",
      "[255/00819] train_loss: 0.012397\n",
      "[255/00869] train_loss: 0.012722\n",
      "[255/00919] train_loss: 0.011995\n",
      "[255/00969] train_loss: 0.012174\n",
      "[255/01019] train_loss: 0.012064\n",
      "[255/01069] train_loss: 0.012191\n",
      "[255/01119] train_loss: 0.012266\n",
      "[255/01169] train_loss: 0.012414\n",
      "[255/01219] train_loss: 0.012327\n",
      "[256/00043] train_loss: 0.012970\n",
      "[256/00093] train_loss: 0.013318\n",
      "[256/00143] train_loss: 0.013045\n",
      "[256/00193] train_loss: 0.012209\n",
      "[256/00243] train_loss: 0.012606\n",
      "[256/00293] train_loss: 0.012817\n",
      "[256/00343] train_loss: 0.011873\n",
      "[256/00393] train_loss: 0.013144\n",
      "[256/00443] train_loss: 0.012545\n",
      "[256/00493] train_loss: 0.011729\n",
      "[256/00543] train_loss: 0.011719\n",
      "[256/00593] train_loss: 0.012916\n",
      "[256/00643] train_loss: 0.012093\n",
      "[256/00693] train_loss: 0.013209\n",
      "[256/00743] train_loss: 0.012509\n",
      "[256/00793] train_loss: 0.011989\n",
      "[256/00843] train_loss: 0.011970\n",
      "[256/00893] train_loss: 0.012350\n",
      "[256/00943] train_loss: 0.011962\n",
      "[256/00993] train_loss: 0.011940\n",
      "[256/01043] train_loss: 0.012406\n",
      "[256/01093] train_loss: 0.011830\n",
      "[256/01143] train_loss: 0.011766\n",
      "[256/01193] train_loss: 0.012448\n",
      "[257/00017] train_loss: 0.012341\n",
      "[257/00067] train_loss: 0.013068\n",
      "[257/00117] train_loss: 0.012718\n",
      "[257/00167] train_loss: 0.012076\n",
      "[257/00217] train_loss: 0.012986\n",
      "[257/00267] train_loss: 0.012591\n",
      "[257/00317] train_loss: 0.012229\n",
      "[257/00367] train_loss: 0.012282\n",
      "[257/00417] train_loss: 0.012936\n",
      "[257/00467] train_loss: 0.012392\n",
      "[257/00517] train_loss: 0.011619\n",
      "[257/00567] train_loss: 0.012250\n",
      "[257/00617] train_loss: 0.011360\n",
      "[257/00667] train_loss: 0.012394\n",
      "[257/00717] train_loss: 0.012177\n",
      "[257/00767] train_loss: 0.011292\n",
      "[257/00817] train_loss: 0.011628\n",
      "[257/00867] train_loss: 0.012310\n",
      "[257/00917] train_loss: 0.012664\n",
      "[257/00967] train_loss: 0.012295\n",
      "[257/01017] train_loss: 0.012008\n",
      "[257/01067] train_loss: 0.012039\n",
      "[257/01117] train_loss: 0.012227\n",
      "[257/01167] train_loss: 0.012520\n",
      "[257/01217] train_loss: 0.012489\n",
      "[258/00041] train_loss: 0.013269\n",
      "[258/00091] train_loss: 0.013030\n",
      "[258/00141] train_loss: 0.012361\n",
      "[258/00191] train_loss: 0.013072\n",
      "[258/00241] train_loss: 0.012708\n",
      "[258/00291] train_loss: 0.011575\n",
      "[258/00341] train_loss: 0.012579\n",
      "[258/00391] train_loss: 0.012585\n",
      "[258/00441] train_loss: 0.012178\n",
      "[258/00491] train_loss: 0.012138\n",
      "[258/00541] train_loss: 0.011968\n",
      "[258/00591] train_loss: 0.011900\n",
      "[258/00641] train_loss: 0.012025\n",
      "[258/00691] train_loss: 0.011616\n",
      "[258/00741] train_loss: 0.012033\n",
      "[258/00791] train_loss: 0.012790\n",
      "[258/00841] train_loss: 0.012360\n",
      "[258/00891] train_loss: 0.011535\n",
      "[258/00941] train_loss: 0.012060\n",
      "[258/00991] train_loss: 0.011703\n",
      "[258/01041] train_loss: 0.013275\n",
      "[258/01091] train_loss: 0.011735\n",
      "[258/01141] train_loss: 0.012277\n",
      "[258/01191] train_loss: 0.011717\n",
      "[259/00015] train_loss: 0.012821\n",
      "[259/00065] train_loss: 0.013233\n",
      "[259/00115] train_loss: 0.013426\n",
      "[259/00165] train_loss: 0.012107\n",
      "[259/00215] train_loss: 0.013605\n",
      "[259/00265] train_loss: 0.012203\n",
      "[259/00315] train_loss: 0.011639\n",
      "[259/00365] train_loss: 0.012646\n",
      "[259/00415] train_loss: 0.011584\n",
      "[259/00465] train_loss: 0.012177\n",
      "[259/00515] train_loss: 0.013362\n",
      "[259/00565] train_loss: 0.012057\n",
      "[259/00615] train_loss: 0.011749\n",
      "[259/00665] train_loss: 0.012258\n",
      "[259/00715] train_loss: 0.011995\n",
      "[259/00765] train_loss: 0.012378\n",
      "[259/00815] train_loss: 0.012393\n",
      "[259/00865] train_loss: 0.012383\n",
      "[259/00915] train_loss: 0.011994\n",
      "[259/00965] train_loss: 0.012003\n",
      "[259/01015] train_loss: 0.012472\n",
      "[259/01065] train_loss: 0.011452\n",
      "[259/01115] train_loss: 0.011651\n",
      "[259/01165] train_loss: 0.011577\n",
      "[259/01215] train_loss: 0.011486\n",
      "[260/00039] train_loss: 0.012479\n",
      "[260/00089] train_loss: 0.013098\n",
      "[260/00139] train_loss: 0.013119\n",
      "[260/00189] train_loss: 0.012667\n",
      "[260/00239] train_loss: 0.011815\n",
      "[260/00289] train_loss: 0.012869\n",
      "[260/00339] train_loss: 0.012330\n",
      "[260/00389] train_loss: 0.011811\n",
      "[260/00439] train_loss: 0.011985\n",
      "[260/00489] train_loss: 0.012144\n",
      "[260/00539] train_loss: 0.011884\n",
      "[260/00589] train_loss: 0.012246\n",
      "[260/00639] train_loss: 0.011867\n",
      "[260/00689] train_loss: 0.012246\n",
      "[260/00739] train_loss: 0.012008\n",
      "[260/00789] train_loss: 0.011957\n",
      "[260/00839] train_loss: 0.012281\n",
      "[260/00889] train_loss: 0.011946\n",
      "[260/00939] train_loss: 0.012963\n",
      "[260/00989] train_loss: 0.012216\n",
      "[260/01039] train_loss: 0.012244\n",
      "[260/01089] train_loss: 0.012748\n",
      "[260/01139] train_loss: 0.012444\n",
      "[260/01189] train_loss: 0.011620\n",
      "[261/00013] train_loss: 0.012764\n",
      "[261/00063] train_loss: 0.013254\n",
      "[261/00113] train_loss: 0.012451\n",
      "[261/00163] train_loss: 0.012699\n",
      "[261/00213] train_loss: 0.012311\n",
      "[261/00263] train_loss: 0.012409\n",
      "[261/00313] train_loss: 0.013004\n",
      "[261/00363] train_loss: 0.012673\n",
      "[261/00413] train_loss: 0.012377\n",
      "[261/00463] train_loss: 0.012355\n",
      "[261/00513] train_loss: 0.011796\n",
      "[261/00563] train_loss: 0.011870\n",
      "[261/00613] train_loss: 0.012315\n",
      "[261/00663] train_loss: 0.012665\n",
      "[261/00713] train_loss: 0.011821\n",
      "[261/00763] train_loss: 0.011758\n",
      "[261/00813] train_loss: 0.011671\n",
      "[261/00863] train_loss: 0.012555\n",
      "[261/00913] train_loss: 0.012073\n",
      "[261/00963] train_loss: 0.012313\n",
      "[261/01013] train_loss: 0.011458\n",
      "[261/01063] train_loss: 0.011675\n",
      "[261/01113] train_loss: 0.012491\n",
      "[261/01163] train_loss: 0.011508\n",
      "[261/01213] train_loss: 0.011892\n",
      "[262/00037] train_loss: 0.012808\n",
      "[262/00087] train_loss: 0.012475\n",
      "[262/00137] train_loss: 0.012291\n",
      "[262/00187] train_loss: 0.013087\n",
      "[262/00237] train_loss: 0.013207\n",
      "[262/00287] train_loss: 0.012424\n",
      "[262/00337] train_loss: 0.012230\n",
      "[262/00387] train_loss: 0.011768\n",
      "[262/00437] train_loss: 0.012434\n",
      "[262/00487] train_loss: 0.012116\n",
      "[262/00537] train_loss: 0.012857\n",
      "[262/00587] train_loss: 0.012323\n",
      "[262/00637] train_loss: 0.011683\n",
      "[262/00687] train_loss: 0.011883\n",
      "[262/00737] train_loss: 0.011994\n",
      "[262/00787] train_loss: 0.011189\n",
      "[262/00837] train_loss: 0.012198\n",
      "[262/00887] train_loss: 0.012710\n",
      "[262/00937] train_loss: 0.012233\n",
      "[262/00987] train_loss: 0.012263\n",
      "[262/01037] train_loss: 0.012264\n",
      "[262/01087] train_loss: 0.012389\n",
      "[262/01137] train_loss: 0.012264\n",
      "[262/01187] train_loss: 0.012292\n",
      "[263/00011] train_loss: 0.012929\n",
      "[263/00061] train_loss: 0.012500\n",
      "[263/00111] train_loss: 0.012472\n",
      "[263/00161] train_loss: 0.012876\n",
      "[263/00211] train_loss: 0.012000\n",
      "[263/00261] train_loss: 0.011920\n",
      "[263/00311] train_loss: 0.011874\n",
      "[263/00361] train_loss: 0.012443\n",
      "[263/00411] train_loss: 0.012611\n",
      "[263/00461] train_loss: 0.013072\n",
      "[263/00511] train_loss: 0.012373\n",
      "[263/00561] train_loss: 0.012175\n",
      "[263/00611] train_loss: 0.012652\n",
      "[263/00661] train_loss: 0.012341\n",
      "[263/00711] train_loss: 0.011951\n",
      "[263/00761] train_loss: 0.011544\n",
      "[263/00811] train_loss: 0.011808\n",
      "[263/00861] train_loss: 0.012382\n",
      "[263/00911] train_loss: 0.011802\n",
      "[263/00961] train_loss: 0.013341\n",
      "[263/01011] train_loss: 0.012553\n",
      "[263/01061] train_loss: 0.011617\n",
      "[263/01111] train_loss: 0.012269\n",
      "[263/01161] train_loss: 0.012473\n",
      "[263/01211] train_loss: 0.011947\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[264/00035] train_loss: 0.012898\n",
      "[264/00085] train_loss: 0.012526\n",
      "[264/00135] train_loss: 0.012466\n",
      "[264/00185] train_loss: 0.012905\n",
      "[264/00235] train_loss: 0.013216\n",
      "[264/00285] train_loss: 0.013136\n",
      "[264/00335] train_loss: 0.012581\n",
      "[264/00385] train_loss: 0.012680\n",
      "[264/00435] train_loss: 0.012320\n",
      "[264/00485] train_loss: 0.011643\n",
      "[264/00535] train_loss: 0.012108\n",
      "[264/00585] train_loss: 0.012397\n",
      "[264/00635] train_loss: 0.012203\n",
      "[264/00685] train_loss: 0.011991\n",
      "[264/00735] train_loss: 0.011640\n",
      "[264/00785] train_loss: 0.011667\n",
      "[264/00835] train_loss: 0.010857\n",
      "[264/00885] train_loss: 0.012183\n",
      "[264/00935] train_loss: 0.012627\n",
      "[264/00985] train_loss: 0.011616\n",
      "[264/01035] train_loss: 0.012343\n",
      "[264/01085] train_loss: 0.012057\n",
      "[264/01135] train_loss: 0.011971\n",
      "[264/01185] train_loss: 0.011693\n",
      "[265/00009] train_loss: 0.012631\n",
      "[265/00059] train_loss: 0.013112\n",
      "[265/00109] train_loss: 0.013335\n",
      "[265/00159] train_loss: 0.012003\n",
      "[265/00209] train_loss: 0.013196\n",
      "[265/00259] train_loss: 0.012447\n",
      "[265/00309] train_loss: 0.012279\n",
      "[265/00359] train_loss: 0.012302\n",
      "[265/00409] train_loss: 0.012309\n",
      "[265/00459] train_loss: 0.012145\n",
      "[265/00509] train_loss: 0.012650\n",
      "[265/00559] train_loss: 0.012003\n",
      "[265/00609] train_loss: 0.011594\n",
      "[265/00659] train_loss: 0.012401\n",
      "[265/00709] train_loss: 0.012239\n",
      "[265/00759] train_loss: 0.012389\n",
      "[265/00809] train_loss: 0.011250\n",
      "[265/00859] train_loss: 0.012335\n",
      "[265/00909] train_loss: 0.012032\n",
      "[265/00959] train_loss: 0.013014\n",
      "[265/01009] train_loss: 0.011835\n",
      "[265/01059] train_loss: 0.012527\n",
      "[265/01109] train_loss: 0.011760\n",
      "[265/01159] train_loss: 0.011507\n",
      "[265/01209] train_loss: 0.012469\n",
      "[266/00033] train_loss: 0.012883\n",
      "[266/00083] train_loss: 0.013403\n",
      "[266/00133] train_loss: 0.013411\n",
      "[266/00183] train_loss: 0.013196\n",
      "[266/00233] train_loss: 0.012444\n",
      "[266/00283] train_loss: 0.012567\n",
      "[266/00333] train_loss: 0.011791\n",
      "[266/00383] train_loss: 0.012565\n",
      "[266/00433] train_loss: 0.012669\n",
      "[266/00483] train_loss: 0.013283\n",
      "[266/00533] train_loss: 0.012335\n",
      "[266/00583] train_loss: 0.011891\n",
      "[266/00633] train_loss: 0.010966\n",
      "[266/00683] train_loss: 0.012467\n",
      "[266/00733] train_loss: 0.011833\n",
      "[266/00783] train_loss: 0.011975\n",
      "[266/00833] train_loss: 0.011972\n",
      "[266/00883] train_loss: 0.011780\n",
      "[266/00933] train_loss: 0.012021\n",
      "[266/00983] train_loss: 0.011895\n",
      "[266/01033] train_loss: 0.011926\n",
      "[266/01083] train_loss: 0.011525\n",
      "[266/01133] train_loss: 0.012693\n",
      "[266/01183] train_loss: 0.012162\n",
      "[267/00007] train_loss: 0.011757\n",
      "[267/00057] train_loss: 0.013019\n",
      "[267/00107] train_loss: 0.012508\n",
      "[267/00157] train_loss: 0.011913\n",
      "[267/00207] train_loss: 0.012626\n",
      "[267/00257] train_loss: 0.012522\n",
      "[267/00307] train_loss: 0.012527\n",
      "[267/00357] train_loss: 0.012410\n",
      "[267/00407] train_loss: 0.011940\n",
      "[267/00457] train_loss: 0.011902\n",
      "[267/00507] train_loss: 0.012152\n",
      "[267/00557] train_loss: 0.012606\n",
      "[267/00607] train_loss: 0.011793\n",
      "[267/00657] train_loss: 0.012561\n",
      "[267/00707] train_loss: 0.012348\n",
      "[267/00757] train_loss: 0.012191\n",
      "[267/00807] train_loss: 0.012440\n",
      "[267/00857] train_loss: 0.011487\n",
      "[267/00907] train_loss: 0.012061\n",
      "[267/00957] train_loss: 0.012200\n",
      "[267/01007] train_loss: 0.011136\n",
      "[267/01057] train_loss: 0.011716\n",
      "[267/01107] train_loss: 0.011647\n",
      "[267/01157] train_loss: 0.012212\n",
      "[267/01207] train_loss: 0.012065\n",
      "[268/00031] train_loss: 0.012578\n",
      "[268/00081] train_loss: 0.013135\n",
      "[268/00131] train_loss: 0.013318\n",
      "[268/00181] train_loss: 0.012101\n",
      "[268/00231] train_loss: 0.012613\n",
      "[268/00281] train_loss: 0.012153\n",
      "[268/00331] train_loss: 0.012116\n",
      "[268/00381] train_loss: 0.011761\n",
      "[268/00431] train_loss: 0.012126\n",
      "[268/00481] train_loss: 0.012830\n",
      "[268/00531] train_loss: 0.012140\n",
      "[268/00581] train_loss: 0.012184\n",
      "[268/00631] train_loss: 0.012079\n",
      "[268/00681] train_loss: 0.012279\n",
      "[268/00731] train_loss: 0.013030\n",
      "[268/00781] train_loss: 0.012077\n",
      "[268/00831] train_loss: 0.011744\n",
      "[268/00881] train_loss: 0.012075\n",
      "[268/00931] train_loss: 0.011973\n",
      "[268/00981] train_loss: 0.012298\n",
      "[268/01031] train_loss: 0.011761\n",
      "[268/01081] train_loss: 0.012497\n",
      "[268/01131] train_loss: 0.011959\n",
      "[268/01181] train_loss: 0.012084\n",
      "[269/00005] train_loss: 0.012259\n",
      "[269/00055] train_loss: 0.012601\n",
      "[269/00105] train_loss: 0.012259\n",
      "[269/00155] train_loss: 0.012575\n",
      "[269/00205] train_loss: 0.012854\n",
      "[269/00255] train_loss: 0.012032\n",
      "[269/00305] train_loss: 0.011711\n",
      "[269/00355] train_loss: 0.011776\n",
      "[269/00405] train_loss: 0.011990\n",
      "[269/00455] train_loss: 0.013107\n",
      "[269/00505] train_loss: 0.012153\n",
      "[269/00555] train_loss: 0.012441\n",
      "[269/00605] train_loss: 0.012647\n",
      "[269/00655] train_loss: 0.012156\n",
      "[269/00705] train_loss: 0.011734\n",
      "[269/00755] train_loss: 0.012034\n",
      "[269/00805] train_loss: 0.011515\n",
      "[269/00855] train_loss: 0.012233\n",
      "[269/00905] train_loss: 0.012497\n",
      "[269/00955] train_loss: 0.011811\n",
      "[269/01005] train_loss: 0.012616\n",
      "[269/01055] train_loss: 0.011472\n",
      "[269/01105] train_loss: 0.012659\n",
      "[269/01155] train_loss: 0.011882\n",
      "[269/01205] train_loss: 0.012346\n",
      "[270/00029] train_loss: 0.012363\n",
      "[270/00079] train_loss: 0.012616\n",
      "[270/00129] train_loss: 0.012902\n",
      "[270/00179] train_loss: 0.012433\n",
      "[270/00229] train_loss: 0.012753\n",
      "[270/00279] train_loss: 0.012378\n",
      "[270/00329] train_loss: 0.013340\n",
      "[270/00379] train_loss: 0.012310\n",
      "[270/00429] train_loss: 0.012660\n",
      "[270/00479] train_loss: 0.011806\n",
      "[270/00529] train_loss: 0.012040\n",
      "[270/00579] train_loss: 0.012455\n",
      "[270/00629] train_loss: 0.012133\n",
      "[270/00679] train_loss: 0.011997\n",
      "[270/00729] train_loss: 0.011785\n",
      "[270/00779] train_loss: 0.012135\n",
      "[270/00829] train_loss: 0.011892\n",
      "[270/00879] train_loss: 0.011541\n",
      "[270/00929] train_loss: 0.012334\n",
      "[270/00979] train_loss: 0.012099\n",
      "[270/01029] train_loss: 0.012668\n",
      "[270/01079] train_loss: 0.011591\n",
      "[270/01129] train_loss: 0.012234\n",
      "[270/01179] train_loss: 0.011902\n",
      "[271/00003] train_loss: 0.012182\n",
      "[271/00053] train_loss: 0.012264\n",
      "[271/00103] train_loss: 0.012120\n",
      "[271/00153] train_loss: 0.012316\n",
      "[271/00203] train_loss: 0.012424\n",
      "[271/00253] train_loss: 0.011830\n",
      "[271/00303] train_loss: 0.012530\n",
      "[271/00353] train_loss: 0.012501\n",
      "[271/00403] train_loss: 0.012509\n",
      "[271/00453] train_loss: 0.012160\n",
      "[271/00503] train_loss: 0.011650\n",
      "[271/00553] train_loss: 0.011733\n",
      "[271/00603] train_loss: 0.012327\n",
      "[271/00653] train_loss: 0.012559\n",
      "[271/00703] train_loss: 0.012503\n",
      "[271/00753] train_loss: 0.012214\n",
      "[271/00803] train_loss: 0.011177\n",
      "[271/00853] train_loss: 0.011306\n",
      "[271/00903] train_loss: 0.011921\n",
      "[271/00953] train_loss: 0.011959\n",
      "[271/01003] train_loss: 0.012238\n",
      "[271/01053] train_loss: 0.012303\n",
      "[271/01103] train_loss: 0.012175\n",
      "[271/01153] train_loss: 0.011071\n",
      "[271/01203] train_loss: 0.012141\n",
      "[272/00027] train_loss: 0.012078\n",
      "[272/00077] train_loss: 0.012836\n",
      "[272/00127] train_loss: 0.012504\n",
      "[272/00177] train_loss: 0.012889\n",
      "[272/00227] train_loss: 0.012475\n",
      "[272/00277] train_loss: 0.011666\n",
      "[272/00327] train_loss: 0.012250\n",
      "[272/00377] train_loss: 0.012730\n",
      "[272/00427] train_loss: 0.011474\n",
      "[272/00477] train_loss: 0.012523\n",
      "[272/00527] train_loss: 0.012868\n",
      "[272/00577] train_loss: 0.011775\n",
      "[272/00627] train_loss: 0.011786\n",
      "[272/00677] train_loss: 0.011568\n",
      "[272/00727] train_loss: 0.012719\n",
      "[272/00777] train_loss: 0.012445\n",
      "[272/00827] train_loss: 0.011828\n",
      "[272/00877] train_loss: 0.012518\n",
      "[272/00927] train_loss: 0.012290\n",
      "[272/00977] train_loss: 0.011986\n",
      "[272/01027] train_loss: 0.011941\n",
      "[272/01077] train_loss: 0.012075\n",
      "[272/01127] train_loss: 0.011732\n",
      "[272/01177] train_loss: 0.012074\n",
      "[273/00001] train_loss: 0.012039\n",
      "[273/00051] train_loss: 0.013453\n",
      "[273/00101] train_loss: 0.012449\n",
      "[273/00151] train_loss: 0.012525\n",
      "[273/00201] train_loss: 0.012813\n",
      "[273/00251] train_loss: 0.012482\n",
      "[273/00301] train_loss: 0.012496\n",
      "[273/00351] train_loss: 0.013059\n",
      "[273/00401] train_loss: 0.011992\n",
      "[273/00451] train_loss: 0.012285\n",
      "[273/00501] train_loss: 0.011784\n",
      "[273/00551] train_loss: 0.012885\n",
      "[273/00601] train_loss: 0.011509\n",
      "[273/00651] train_loss: 0.012651\n",
      "[273/00701] train_loss: 0.011783\n",
      "[273/00751] train_loss: 0.012460\n",
      "[273/00801] train_loss: 0.012272\n",
      "[273/00851] train_loss: 0.012168\n",
      "[273/00901] train_loss: 0.011523\n",
      "[273/00951] train_loss: 0.011992\n",
      "[273/01001] train_loss: 0.011649\n",
      "[273/01051] train_loss: 0.011909\n",
      "[273/01101] train_loss: 0.012218\n",
      "[273/01151] train_loss: 0.011943\n",
      "[273/01201] train_loss: 0.012024\n",
      "[274/00025] train_loss: 0.012446\n",
      "[274/00075] train_loss: 0.013370\n",
      "[274/00125] train_loss: 0.012241\n",
      "[274/00175] train_loss: 0.011702\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[274/00225] train_loss: 0.012559\n",
      "[274/00275] train_loss: 0.012264\n",
      "[274/00325] train_loss: 0.011627\n",
      "[274/00375] train_loss: 0.013084\n",
      "[274/00425] train_loss: 0.013390\n",
      "[274/00475] train_loss: 0.012534\n",
      "[274/00525] train_loss: 0.012965\n",
      "[274/00575] train_loss: 0.012452\n",
      "[274/00625] train_loss: 0.011731\n",
      "[274/00675] train_loss: 0.011784\n",
      "[274/00725] train_loss: 0.012102\n",
      "[274/00775] train_loss: 0.011863\n",
      "[274/00825] train_loss: 0.011988\n",
      "[274/00875] train_loss: 0.011537\n",
      "[274/00925] train_loss: 0.011729\n",
      "[274/00975] train_loss: 0.012119\n",
      "[274/01025] train_loss: 0.012481\n",
      "[274/01075] train_loss: 0.011911\n",
      "[274/01125] train_loss: 0.012615\n",
      "[274/01175] train_loss: 0.012154\n",
      "[274/01225] train_loss: 0.011701\n",
      "[275/00049] train_loss: 0.012486\n",
      "[275/00099] train_loss: 0.012361\n",
      "[275/00149] train_loss: 0.011975\n",
      "[275/00199] train_loss: 0.012310\n",
      "[275/00249] train_loss: 0.011709\n",
      "[275/00299] train_loss: 0.012622\n",
      "[275/00349] train_loss: 0.012377\n",
      "[275/00399] train_loss: 0.012014\n",
      "[275/00449] train_loss: 0.012753\n",
      "[275/00499] train_loss: 0.012676\n",
      "[275/00549] train_loss: 0.011689\n",
      "[275/00599] train_loss: 0.012360\n",
      "[275/00649] train_loss: 0.011800\n",
      "[275/00699] train_loss: 0.012205\n",
      "[275/00749] train_loss: 0.012186\n",
      "[275/00799] train_loss: 0.011846\n",
      "[275/00849] train_loss: 0.012310\n",
      "[275/00899] train_loss: 0.011792\n",
      "[275/00949] train_loss: 0.012856\n",
      "[275/00999] train_loss: 0.012419\n",
      "[275/01049] train_loss: 0.012170\n",
      "[275/01099] train_loss: 0.011703\n",
      "[275/01149] train_loss: 0.011562\n",
      "[275/01199] train_loss: 0.011594\n",
      "[276/00023] train_loss: 0.011919\n",
      "[276/00073] train_loss: 0.012863\n",
      "[276/00123] train_loss: 0.013236\n",
      "[276/00173] train_loss: 0.012637\n",
      "[276/00223] train_loss: 0.012434\n",
      "[276/00273] train_loss: 0.011729\n",
      "[276/00323] train_loss: 0.012273\n",
      "[276/00373] train_loss: 0.012519\n",
      "[276/00423] train_loss: 0.011999\n",
      "[276/00473] train_loss: 0.012939\n",
      "[276/00523] train_loss: 0.011974\n",
      "[276/00573] train_loss: 0.011906\n",
      "[276/00623] train_loss: 0.012377\n",
      "[276/00673] train_loss: 0.011577\n",
      "[276/00723] train_loss: 0.011445\n",
      "[276/00773] train_loss: 0.011971\n",
      "[276/00823] train_loss: 0.013080\n",
      "[276/00873] train_loss: 0.012561\n",
      "[276/00923] train_loss: 0.013284\n",
      "[276/00973] train_loss: 0.011569\n",
      "[276/01023] train_loss: 0.011688\n",
      "[276/01073] train_loss: 0.012496\n",
      "[276/01123] train_loss: 0.011406\n",
      "[276/01173] train_loss: 0.011387\n",
      "[276/01223] train_loss: 0.012086\n",
      "[277/00047] train_loss: 0.012499\n",
      "[277/00097] train_loss: 0.013441\n",
      "[277/00147] train_loss: 0.012327\n",
      "[277/00197] train_loss: 0.012563\n",
      "[277/00247] train_loss: 0.012228\n",
      "[277/00297] train_loss: 0.012610\n",
      "[277/00347] train_loss: 0.012037\n",
      "[277/00397] train_loss: 0.012409\n",
      "[277/00447] train_loss: 0.012163\n",
      "[277/00497] train_loss: 0.012354\n",
      "[277/00547] train_loss: 0.012018\n",
      "[277/00597] train_loss: 0.011319\n",
      "[277/00647] train_loss: 0.011522\n",
      "[277/00697] train_loss: 0.011883\n",
      "[277/00747] train_loss: 0.012152\n",
      "[277/00797] train_loss: 0.011956\n",
      "[277/00847] train_loss: 0.012329\n",
      "[277/00897] train_loss: 0.012605\n",
      "[277/00947] train_loss: 0.012429\n",
      "[277/00997] train_loss: 0.012390\n",
      "[277/01047] train_loss: 0.012103\n",
      "[277/01097] train_loss: 0.011931\n",
      "[277/01147] train_loss: 0.011365\n",
      "[277/01197] train_loss: 0.011290\n",
      "[278/00021] train_loss: 0.012067\n",
      "[278/00071] train_loss: 0.012180\n",
      "[278/00121] train_loss: 0.011837\n",
      "[278/00171] train_loss: 0.013224\n",
      "[278/00221] train_loss: 0.013149\n",
      "[278/00271] train_loss: 0.012411\n",
      "[278/00321] train_loss: 0.011922\n",
      "[278/00371] train_loss: 0.012109\n",
      "[278/00421] train_loss: 0.012237\n",
      "[278/00471] train_loss: 0.012607\n",
      "[278/00521] train_loss: 0.011531\n",
      "[278/00571] train_loss: 0.012869\n",
      "[278/00621] train_loss: 0.012038\n",
      "[278/00671] train_loss: 0.012074\n",
      "[278/00721] train_loss: 0.011875\n",
      "[278/00771] train_loss: 0.011587\n",
      "[278/00821] train_loss: 0.011855\n",
      "[278/00871] train_loss: 0.011819\n",
      "[278/00921] train_loss: 0.012017\n",
      "[278/00971] train_loss: 0.012323\n",
      "[278/01021] train_loss: 0.011807\n",
      "[278/01071] train_loss: 0.011961\n",
      "[278/01121] train_loss: 0.012191\n",
      "[278/01171] train_loss: 0.012086\n",
      "[278/01221] train_loss: 0.012488\n",
      "[279/00045] train_loss: 0.012502\n",
      "[279/00095] train_loss: 0.012436\n",
      "[279/00145] train_loss: 0.012445\n",
      "[279/00195] train_loss: 0.012235\n",
      "[279/00245] train_loss: 0.012377\n",
      "[279/00295] train_loss: 0.012327\n",
      "[279/00345] train_loss: 0.013053\n",
      "[279/00395] train_loss: 0.012275\n",
      "[279/00445] train_loss: 0.012072\n",
      "[279/00495] train_loss: 0.012027\n",
      "[279/00545] train_loss: 0.011972\n",
      "[279/00595] train_loss: 0.011845\n",
      "[279/00645] train_loss: 0.012139\n",
      "[279/00695] train_loss: 0.011281\n",
      "[279/00745] train_loss: 0.011531\n",
      "[279/00795] train_loss: 0.011638\n",
      "[279/00845] train_loss: 0.011788\n",
      "[279/00895] train_loss: 0.011942\n",
      "[279/00945] train_loss: 0.012467\n",
      "[279/00995] train_loss: 0.012562\n",
      "[279/01045] train_loss: 0.011553\n",
      "[279/01095] train_loss: 0.011725\n",
      "[279/01145] train_loss: 0.011961\n",
      "[279/01195] train_loss: 0.011424\n",
      "[280/00019] train_loss: 0.011854\n",
      "[280/00069] train_loss: 0.012777\n",
      "[280/00119] train_loss: 0.012297\n",
      "[280/00169] train_loss: 0.013240\n",
      "[280/00219] train_loss: 0.012585\n",
      "[280/00269] train_loss: 0.012597\n",
      "[280/00319] train_loss: 0.011980\n",
      "[280/00369] train_loss: 0.011725\n",
      "[280/00419] train_loss: 0.011820\n",
      "[280/00469] train_loss: 0.011890\n",
      "[280/00519] train_loss: 0.011325\n",
      "[280/00569] train_loss: 0.012442\n",
      "[280/00619] train_loss: 0.011326\n",
      "[280/00669] train_loss: 0.011724\n",
      "[280/00719] train_loss: 0.011999\n",
      "[280/00769] train_loss: 0.011439\n",
      "[280/00819] train_loss: 0.011784\n",
      "[280/00869] train_loss: 0.012300\n",
      "[280/00919] train_loss: 0.011726\n",
      "[280/00969] train_loss: 0.012335\n",
      "[280/01019] train_loss: 0.011875\n",
      "[280/01069] train_loss: 0.011907\n",
      "[280/01119] train_loss: 0.011981\n",
      "[280/01169] train_loss: 0.012487\n",
      "[280/01219] train_loss: 0.011739\n",
      "[281/00043] train_loss: 0.012576\n",
      "[281/00093] train_loss: 0.012618\n",
      "[281/00143] train_loss: 0.011965\n",
      "[281/00193] train_loss: 0.013076\n",
      "[281/00243] train_loss: 0.012696\n",
      "[281/00293] train_loss: 0.011607\n",
      "[281/00343] train_loss: 0.013273\n",
      "[281/00393] train_loss: 0.012241\n",
      "[281/00443] train_loss: 0.012111\n",
      "[281/00493] train_loss: 0.012060\n",
      "[281/00543] train_loss: 0.012149\n",
      "[281/00593] train_loss: 0.012072\n",
      "[281/00643] train_loss: 0.011274\n",
      "[281/00693] train_loss: 0.011709\n",
      "[281/00743] train_loss: 0.012111\n",
      "[281/00793] train_loss: 0.011817\n",
      "[281/00843] train_loss: 0.011616\n",
      "[281/00893] train_loss: 0.012196\n",
      "[281/00943] train_loss: 0.011359\n",
      "[281/00993] train_loss: 0.011910\n",
      "[281/01043] train_loss: 0.012903\n",
      "[281/01093] train_loss: 0.012526\n",
      "[281/01143] train_loss: 0.011704\n",
      "[281/01193] train_loss: 0.011156\n",
      "[282/00017] train_loss: 0.011824\n",
      "[282/00067] train_loss: 0.012967\n",
      "[282/00117] train_loss: 0.012505\n",
      "[282/00167] train_loss: 0.013316\n",
      "[282/00217] train_loss: 0.012281\n",
      "[282/00267] train_loss: 0.012351\n",
      "[282/00317] train_loss: 0.011990\n",
      "[282/00367] train_loss: 0.011206\n",
      "[282/00417] train_loss: 0.011693\n",
      "[282/00467] train_loss: 0.013369\n",
      "[282/00517] train_loss: 0.012665\n",
      "[282/00567] train_loss: 0.011851\n",
      "[282/00617] train_loss: 0.011471\n",
      "[282/00667] train_loss: 0.011867\n",
      "[282/00717] train_loss: 0.011395\n",
      "[282/00767] train_loss: 0.011974\n",
      "[282/00817] train_loss: 0.011748\n",
      "[282/00867] train_loss: 0.012158\n",
      "[282/00917] train_loss: 0.011959\n",
      "[282/00967] train_loss: 0.012283\n",
      "[282/01017] train_loss: 0.011449\n",
      "[282/01067] train_loss: 0.011793\n",
      "[282/01117] train_loss: 0.012380\n",
      "[282/01167] train_loss: 0.011680\n",
      "[282/01217] train_loss: 0.011565\n",
      "[283/00041] train_loss: 0.012922\n",
      "[283/00091] train_loss: 0.013390\n",
      "[283/00141] train_loss: 0.012938\n",
      "[283/00191] train_loss: 0.012668\n",
      "[283/00241] train_loss: 0.012837\n",
      "[283/00291] train_loss: 0.012425\n",
      "[283/00341] train_loss: 0.011875\n",
      "[283/00391] train_loss: 0.012106\n",
      "[283/00441] train_loss: 0.012766\n",
      "[283/00491] train_loss: 0.011326\n",
      "[283/00541] train_loss: 0.012316\n",
      "[283/00591] train_loss: 0.011756\n",
      "[283/00641] train_loss: 0.012282\n",
      "[283/00691] train_loss: 0.012048\n",
      "[283/00741] train_loss: 0.011637\n",
      "[283/00791] train_loss: 0.011713\n",
      "[283/00841] train_loss: 0.011140\n",
      "[283/00891] train_loss: 0.011998\n",
      "[283/00941] train_loss: 0.011580\n",
      "[283/00991] train_loss: 0.011701\n",
      "[283/01041] train_loss: 0.011467\n",
      "[283/01091] train_loss: 0.011861\n",
      "[283/01141] train_loss: 0.011703\n",
      "[283/01191] train_loss: 0.011875\n",
      "[284/00015] train_loss: 0.012416\n",
      "[284/00065] train_loss: 0.012661\n",
      "[284/00115] train_loss: 0.012775\n",
      "[284/00165] train_loss: 0.012836\n",
      "[284/00215] train_loss: 0.012049\n",
      "[284/00265] train_loss: 0.012030\n",
      "[284/00315] train_loss: 0.012476\n",
      "[284/00365] train_loss: 0.011735\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[284/00415] train_loss: 0.012036\n",
      "[284/00465] train_loss: 0.012579\n",
      "[284/00515] train_loss: 0.011530\n",
      "[284/00565] train_loss: 0.012256\n",
      "[284/00615] train_loss: 0.012533\n",
      "[284/00665] train_loss: 0.011912\n",
      "[284/00715] train_loss: 0.011316\n",
      "[284/00765] train_loss: 0.011981\n",
      "[284/00815] train_loss: 0.011418\n",
      "[284/00865] train_loss: 0.012253\n",
      "[284/00915] train_loss: 0.011128\n",
      "[284/00965] train_loss: 0.011749\n",
      "[284/01015] train_loss: 0.011883\n",
      "[284/01065] train_loss: 0.011919\n",
      "[284/01115] train_loss: 0.013121\n",
      "[284/01165] train_loss: 0.012597\n",
      "[284/01215] train_loss: 0.012378\n",
      "[285/00039] train_loss: 0.012817\n",
      "[285/00089] train_loss: 0.012162\n",
      "[285/00139] train_loss: 0.013355\n",
      "[285/00189] train_loss: 0.012101\n",
      "[285/00239] train_loss: 0.011684\n",
      "[285/00289] train_loss: 0.011861\n",
      "[285/00339] train_loss: 0.012042\n",
      "[285/00389] train_loss: 0.012345\n",
      "[285/00439] train_loss: 0.011165\n",
      "[285/00489] train_loss: 0.012316\n",
      "[285/00539] train_loss: 0.012328\n",
      "[285/00589] train_loss: 0.011961\n",
      "[285/00639] train_loss: 0.011916\n",
      "[285/00689] train_loss: 0.011431\n",
      "[285/00739] train_loss: 0.011894\n",
      "[285/00789] train_loss: 0.011162\n",
      "[285/00839] train_loss: 0.011508\n",
      "[285/00889] train_loss: 0.012242\n",
      "[285/00939] train_loss: 0.012194\n",
      "[285/00989] train_loss: 0.012086\n",
      "[285/01039] train_loss: 0.012083\n",
      "[285/01089] train_loss: 0.011800\n",
      "[285/01139] train_loss: 0.011537\n",
      "[285/01189] train_loss: 0.012231\n",
      "[286/00013] train_loss: 0.012942\n",
      "[286/00063] train_loss: 0.013553\n",
      "[286/00113] train_loss: 0.013387\n",
      "[286/00163] train_loss: 0.012799\n",
      "[286/00213] train_loss: 0.013001\n",
      "[286/00263] train_loss: 0.012072\n",
      "[286/00313] train_loss: 0.012213\n",
      "[286/00363] train_loss: 0.011615\n",
      "[286/00413] train_loss: 0.012082\n",
      "[286/00463] train_loss: 0.011952\n",
      "[286/00513] train_loss: 0.012834\n",
      "[286/00563] train_loss: 0.012692\n",
      "[286/00613] train_loss: 0.012120\n",
      "[286/00663] train_loss: 0.011854\n",
      "[286/00713] train_loss: 0.011839\n",
      "[286/00763] train_loss: 0.011182\n",
      "[286/00813] train_loss: 0.011547\n",
      "[286/00863] train_loss: 0.012013\n",
      "[286/00913] train_loss: 0.011877\n",
      "[286/00963] train_loss: 0.011786\n",
      "[286/01013] train_loss: 0.011451\n",
      "[286/01063] train_loss: 0.011821\n",
      "[286/01113] train_loss: 0.011346\n",
      "[286/01163] train_loss: 0.011915\n",
      "[286/01213] train_loss: 0.011354\n",
      "[287/00037] train_loss: 0.012014\n",
      "[287/00087] train_loss: 0.013085\n",
      "[287/00137] train_loss: 0.012655\n",
      "[287/00187] train_loss: 0.012316\n",
      "[287/00237] train_loss: 0.011785\n",
      "[287/00287] train_loss: 0.011591\n",
      "[287/00337] train_loss: 0.011978\n",
      "[287/00387] train_loss: 0.011745\n",
      "[287/00437] train_loss: 0.012086\n",
      "[287/00487] train_loss: 0.011885\n",
      "[287/00537] train_loss: 0.012223\n",
      "[287/00587] train_loss: 0.012710\n",
      "[287/00637] train_loss: 0.011727\n",
      "[287/00687] train_loss: 0.011786\n",
      "[287/00737] train_loss: 0.012356\n",
      "[287/00787] train_loss: 0.011949\n",
      "[287/00837] train_loss: 0.012427\n",
      "[287/00887] train_loss: 0.011790\n",
      "[287/00937] train_loss: 0.011921\n",
      "[287/00987] train_loss: 0.011702\n",
      "[287/01037] train_loss: 0.011237\n",
      "[287/01087] train_loss: 0.011774\n",
      "[287/01137] train_loss: 0.012252\n",
      "[287/01187] train_loss: 0.011678\n",
      "[288/00011] train_loss: 0.012173\n",
      "[288/00061] train_loss: 0.012315\n",
      "[288/00111] train_loss: 0.012234\n",
      "[288/00161] train_loss: 0.012585\n",
      "[288/00211] train_loss: 0.012192\n",
      "[288/00261] train_loss: 0.012652\n",
      "[288/00311] train_loss: 0.012405\n",
      "[288/00361] train_loss: 0.011970\n",
      "[288/00411] train_loss: 0.011538\n",
      "[288/00461] train_loss: 0.012786\n",
      "[288/00511] train_loss: 0.012099\n",
      "[288/00561] train_loss: 0.012312\n",
      "[288/00611] train_loss: 0.012481\n",
      "[288/00661] train_loss: 0.011386\n",
      "[288/00711] train_loss: 0.011708\n",
      "[288/00761] train_loss: 0.012472\n",
      "[288/00811] train_loss: 0.011326\n",
      "[288/00861] train_loss: 0.012123\n",
      "[288/00911] train_loss: 0.012428\n",
      "[288/00961] train_loss: 0.011390\n",
      "[288/01011] train_loss: 0.012003\n",
      "[288/01061] train_loss: 0.012050\n",
      "[288/01111] train_loss: 0.012429\n",
      "[288/01161] train_loss: 0.011645\n",
      "[288/01211] train_loss: 0.011901\n",
      "[289/00035] train_loss: 0.013283\n",
      "[289/00085] train_loss: 0.012287\n",
      "[289/00135] train_loss: 0.013412\n",
      "[289/00185] train_loss: 0.012133\n",
      "[289/00235] train_loss: 0.012281\n",
      "[289/00285] train_loss: 0.011731\n",
      "[289/00335] train_loss: 0.011832\n",
      "[289/00385] train_loss: 0.011865\n",
      "[289/00435] train_loss: 0.012430\n",
      "[289/00485] train_loss: 0.012103\n",
      "[289/00535] train_loss: 0.012805\n",
      "[289/00585] train_loss: 0.012864\n",
      "[289/00635] train_loss: 0.012158\n",
      "[289/00685] train_loss: 0.012374\n",
      "[289/00735] train_loss: 0.012158\n",
      "[289/00785] train_loss: 0.011561\n",
      "[289/00835] train_loss: 0.012226\n",
      "[289/00885] train_loss: 0.012519\n",
      "[289/00935] train_loss: 0.011925\n",
      "[289/00985] train_loss: 0.012001\n",
      "[289/01035] train_loss: 0.011908\n",
      "[289/01085] train_loss: 0.011338\n",
      "[289/01135] train_loss: 0.012023\n",
      "[289/01185] train_loss: 0.011454\n",
      "[290/00009] train_loss: 0.011879\n",
      "[290/00059] train_loss: 0.012206\n",
      "[290/00109] train_loss: 0.012878\n",
      "[290/00159] train_loss: 0.012287\n",
      "[290/00209] train_loss: 0.012378\n",
      "[290/00259] train_loss: 0.012824\n",
      "[290/00309] train_loss: 0.011378\n",
      "[290/00359] train_loss: 0.011912\n",
      "[290/00409] train_loss: 0.012068\n",
      "[290/00459] train_loss: 0.012573\n",
      "[290/00509] train_loss: 0.012349\n",
      "[290/00559] train_loss: 0.011757\n",
      "[290/00609] train_loss: 0.012108\n",
      "[290/00659] train_loss: 0.012177\n",
      "[290/00709] train_loss: 0.011833\n",
      "[290/00759] train_loss: 0.011643\n",
      "[290/00809] train_loss: 0.012312\n",
      "[290/00859] train_loss: 0.011773\n",
      "[290/00909] train_loss: 0.011937\n",
      "[290/00959] train_loss: 0.011237\n",
      "[290/01009] train_loss: 0.012540\n",
      "[290/01059] train_loss: 0.012831\n",
      "[290/01109] train_loss: 0.011292\n",
      "[290/01159] train_loss: 0.011915\n",
      "[290/01209] train_loss: 0.011699\n",
      "[291/00033] train_loss: 0.011701\n",
      "[291/00083] train_loss: 0.012636\n",
      "[291/00133] train_loss: 0.012477\n",
      "[291/00183] train_loss: 0.012161\n",
      "[291/00233] train_loss: 0.012249\n",
      "[291/00283] train_loss: 0.012613\n",
      "[291/00333] train_loss: 0.012080\n",
      "[291/00383] train_loss: 0.012773\n",
      "[291/00433] train_loss: 0.011529\n",
      "[291/00483] train_loss: 0.012258\n",
      "[291/00533] train_loss: 0.012100\n",
      "[291/00583] train_loss: 0.012205\n",
      "[291/00633] train_loss: 0.011758\n",
      "[291/00683] train_loss: 0.011813\n",
      "[291/00733] train_loss: 0.012502\n",
      "[291/00783] train_loss: 0.012146\n",
      "[291/00833] train_loss: 0.011785\n",
      "[291/00883] train_loss: 0.012271\n",
      "[291/00933] train_loss: 0.011970\n",
      "[291/00983] train_loss: 0.011234\n",
      "[291/01033] train_loss: 0.012374\n",
      "[291/01083] train_loss: 0.011483\n",
      "[291/01133] train_loss: 0.011622\n",
      "[291/01183] train_loss: 0.011653\n",
      "[292/00007] train_loss: 0.011283\n",
      "[292/00057] train_loss: 0.012564\n",
      "[292/00107] train_loss: 0.012497\n",
      "[292/00157] train_loss: 0.011674\n",
      "[292/00207] train_loss: 0.012175\n",
      "[292/00257] train_loss: 0.012746\n",
      "[292/00307] train_loss: 0.011849\n",
      "[292/00357] train_loss: 0.012211\n",
      "[292/00407] train_loss: 0.011858\n",
      "[292/00457] train_loss: 0.011982\n",
      "[292/00507] train_loss: 0.011639\n",
      "[292/00557] train_loss: 0.011455\n",
      "[292/00607] train_loss: 0.012829\n",
      "[292/00657] train_loss: 0.011852\n",
      "[292/00707] train_loss: 0.012227\n",
      "[292/00757] train_loss: 0.012269\n",
      "[292/00807] train_loss: 0.011693\n",
      "[292/00857] train_loss: 0.012340\n",
      "[292/00907] train_loss: 0.012139\n",
      "[292/00957] train_loss: 0.012046\n",
      "[292/01007] train_loss: 0.012028\n",
      "[292/01057] train_loss: 0.011472\n",
      "[292/01107] train_loss: 0.012172\n",
      "[292/01157] train_loss: 0.011775\n",
      "[292/01207] train_loss: 0.012422\n",
      "[293/00031] train_loss: 0.012177\n",
      "[293/00081] train_loss: 0.012193\n",
      "[293/00131] train_loss: 0.012279\n",
      "[293/00181] train_loss: 0.012181\n",
      "[293/00231] train_loss: 0.012211\n",
      "[293/00281] train_loss: 0.012602\n",
      "[293/00331] train_loss: 0.012185\n",
      "[293/00381] train_loss: 0.012535\n",
      "[293/00431] train_loss: 0.011770\n",
      "[293/00481] train_loss: 0.011694\n",
      "[293/00531] train_loss: 0.011433\n",
      "[293/00581] train_loss: 0.011841\n",
      "[293/00631] train_loss: 0.011216\n",
      "[293/00681] train_loss: 0.011414\n",
      "[293/00731] train_loss: 0.011903\n",
      "[293/00781] train_loss: 0.011799\n",
      "[293/00831] train_loss: 0.012312\n",
      "[293/00881] train_loss: 0.011477\n",
      "[293/00931] train_loss: 0.012027\n",
      "[293/00981] train_loss: 0.011900\n",
      "[293/01031] train_loss: 0.012106\n",
      "[293/01081] train_loss: 0.012092\n",
      "[293/01131] train_loss: 0.012072\n",
      "[293/01181] train_loss: 0.011717\n",
      "[294/00005] train_loss: 0.011873\n",
      "[294/00055] train_loss: 0.012291\n",
      "[294/00105] train_loss: 0.012138\n",
      "[294/00155] train_loss: 0.012040\n",
      "[294/00205] train_loss: 0.012419\n",
      "[294/00255] train_loss: 0.012073\n",
      "[294/00305] train_loss: 0.012108\n",
      "[294/00355] train_loss: 0.011625\n",
      "[294/00405] train_loss: 0.012317\n",
      "[294/00455] train_loss: 0.012192\n",
      "[294/00505] train_loss: 0.011842\n",
      "[294/00555] train_loss: 0.011866\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[294/00605] train_loss: 0.012220\n",
      "[294/00655] train_loss: 0.012455\n",
      "[294/00705] train_loss: 0.011623\n",
      "[294/00755] train_loss: 0.012313\n",
      "[294/00805] train_loss: 0.012675\n",
      "[294/00855] train_loss: 0.012275\n",
      "[294/00905] train_loss: 0.012550\n",
      "[294/00955] train_loss: 0.011601\n",
      "[294/01005] train_loss: 0.011273\n",
      "[294/01055] train_loss: 0.012416\n",
      "[294/01105] train_loss: 0.012121\n",
      "[294/01155] train_loss: 0.011996\n",
      "[294/01205] train_loss: 0.011682\n",
      "[295/00029] train_loss: 0.012841\n",
      "[295/00079] train_loss: 0.012238\n",
      "[295/00129] train_loss: 0.012749\n",
      "[295/00179] train_loss: 0.012765\n",
      "[295/00229] train_loss: 0.012756\n",
      "[295/00279] train_loss: 0.012599\n",
      "[295/00329] train_loss: 0.011832\n",
      "[295/00379] train_loss: 0.011906\n",
      "[295/00429] train_loss: 0.012120\n",
      "[295/00479] train_loss: 0.011352\n",
      "[295/00529] train_loss: 0.012145\n",
      "[295/00579] train_loss: 0.011169\n",
      "[295/00629] train_loss: 0.011577\n",
      "[295/00679] train_loss: 0.011960\n",
      "[295/00729] train_loss: 0.012464\n",
      "[295/00779] train_loss: 0.011834\n",
      "[295/00829] train_loss: 0.012425\n",
      "[295/00879] train_loss: 0.011974\n",
      "[295/00929] train_loss: 0.012532\n",
      "[295/00979] train_loss: 0.011476\n",
      "[295/01029] train_loss: 0.011754\n",
      "[295/01079] train_loss: 0.011585\n",
      "[295/01129] train_loss: 0.011786\n",
      "[295/01179] train_loss: 0.011714\n",
      "[296/00003] train_loss: 0.011719\n",
      "[296/00053] train_loss: 0.012533\n",
      "[296/00103] train_loss: 0.012107\n",
      "[296/00153] train_loss: 0.012550\n",
      "[296/00203] train_loss: 0.012188\n",
      "[296/00253] train_loss: 0.011705\n",
      "[296/00303] train_loss: 0.012348\n",
      "[296/00353] train_loss: 0.011196\n",
      "[296/00403] train_loss: 0.011460\n",
      "[296/00453] train_loss: 0.011967\n",
      "[296/00503] train_loss: 0.012120\n",
      "[296/00553] train_loss: 0.011963\n",
      "[296/00603] train_loss: 0.012029\n",
      "[296/00653] train_loss: 0.011619\n",
      "[296/00703] train_loss: 0.011473\n",
      "[296/00753] train_loss: 0.012155\n",
      "[296/00803] train_loss: 0.011949\n",
      "[296/00853] train_loss: 0.011412\n",
      "[296/00903] train_loss: 0.011525\n",
      "[296/00953] train_loss: 0.011593\n",
      "[296/01003] train_loss: 0.011754\n",
      "[296/01053] train_loss: 0.012348\n",
      "[296/01103] train_loss: 0.012187\n",
      "[296/01153] train_loss: 0.011898\n",
      "[296/01203] train_loss: 0.012317\n",
      "[297/00027] train_loss: 0.012999\n",
      "[297/00077] train_loss: 0.012268\n",
      "[297/00127] train_loss: 0.012974\n",
      "[297/00177] train_loss: 0.012585\n",
      "[297/00227] train_loss: 0.012392\n",
      "[297/00277] train_loss: 0.011520\n",
      "[297/00327] train_loss: 0.012629\n",
      "[297/00377] train_loss: 0.012169\n",
      "[297/00427] train_loss: 0.011197\n",
      "[297/00477] train_loss: 0.011178\n",
      "[297/00527] train_loss: 0.011964\n",
      "[297/00577] train_loss: 0.012108\n",
      "[297/00627] train_loss: 0.011430\n",
      "[297/00677] train_loss: 0.011901\n",
      "[297/00727] train_loss: 0.012260\n",
      "[297/00777] train_loss: 0.011989\n",
      "[297/00827] train_loss: 0.012397\n",
      "[297/00877] train_loss: 0.012629\n",
      "[297/00927] train_loss: 0.011956\n",
      "[297/00977] train_loss: 0.011841\n",
      "[297/01027] train_loss: 0.012620\n",
      "[297/01077] train_loss: 0.012234\n",
      "[297/01127] train_loss: 0.011948\n",
      "[297/01177] train_loss: 0.011802\n",
      "[298/00001] train_loss: 0.011027\n",
      "[298/00051] train_loss: 0.012660\n",
      "[298/00101] train_loss: 0.012341\n",
      "[298/00151] train_loss: 0.012387\n",
      "[298/00201] train_loss: 0.012175\n",
      "[298/00251] train_loss: 0.012065\n",
      "[298/00301] train_loss: 0.011408\n",
      "[298/00351] train_loss: 0.012606\n",
      "[298/00401] train_loss: 0.011539\n",
      "[298/00451] train_loss: 0.011990\n",
      "[298/00501] train_loss: 0.012550\n",
      "[298/00551] train_loss: 0.012344\n",
      "[298/00601] train_loss: 0.011960\n",
      "[298/00651] train_loss: 0.011295\n",
      "[298/00701] train_loss: 0.012107\n",
      "[298/00751] train_loss: 0.013023\n",
      "[298/00801] train_loss: 0.012394\n",
      "[298/00851] train_loss: 0.011865\n",
      "[298/00901] train_loss: 0.011690\n",
      "[298/00951] train_loss: 0.011872\n",
      "[298/01001] train_loss: 0.011919\n",
      "[298/01051] train_loss: 0.011304\n",
      "[298/01101] train_loss: 0.011388\n",
      "[298/01151] train_loss: 0.011901\n",
      "[298/01201] train_loss: 0.012483\n",
      "[299/00025] train_loss: 0.011760\n",
      "[299/00075] train_loss: 0.013006\n",
      "[299/00125] train_loss: 0.012053\n",
      "[299/00175] train_loss: 0.012196\n",
      "[299/00225] train_loss: 0.011874\n",
      "[299/00275] train_loss: 0.011696\n",
      "[299/00325] train_loss: 0.011654\n",
      "[299/00375] train_loss: 0.011748\n",
      "[299/00425] train_loss: 0.011778\n",
      "[299/00475] train_loss: 0.011611\n",
      "[299/00525] train_loss: 0.011583\n",
      "[299/00575] train_loss: 0.011911\n",
      "[299/00625] train_loss: 0.012065\n",
      "[299/00675] train_loss: 0.011774\n",
      "[299/00725] train_loss: 0.011311\n",
      "[299/00775] train_loss: 0.012092\n",
      "[299/00825] train_loss: 0.011827\n",
      "[299/00875] train_loss: 0.011889\n",
      "[299/00925] train_loss: 0.012594\n",
      "[299/00975] train_loss: 0.013097\n",
      "[299/01025] train_loss: 0.011116\n",
      "[299/01075] train_loss: 0.011146\n",
      "[299/01125] train_loss: 0.011952\n",
      "[299/01175] train_loss: 0.012794\n",
      "[299/01225] train_loss: 0.012128\n",
      "[300/00049] train_loss: 0.011894\n",
      "[300/00099] train_loss: 0.012243\n",
      "[300/00149] train_loss: 0.012507\n",
      "[300/00199] train_loss: 0.012244\n",
      "[300/00249] train_loss: 0.012015\n",
      "[300/00299] train_loss: 0.011084\n",
      "[300/00349] train_loss: 0.011390\n",
      "[300/00399] train_loss: 0.012458\n",
      "[300/00449] train_loss: 0.011534\n",
      "[300/00499] train_loss: 0.011706\n",
      "[300/00549] train_loss: 0.011515\n",
      "[300/00599] train_loss: 0.012337\n",
      "[300/00649] train_loss: 0.012233\n",
      "[300/00699] train_loss: 0.012062\n",
      "[300/00749] train_loss: 0.012158\n",
      "[300/00799] train_loss: 0.012092\n",
      "[300/00849] train_loss: 0.012537\n",
      "[300/00899] train_loss: 0.012084\n",
      "[300/00949] train_loss: 0.012171\n",
      "[300/00999] train_loss: 0.011813\n",
      "[300/01049] train_loss: 0.011895\n",
      "[300/01099] train_loss: 0.011314\n",
      "[300/01149] train_loss: 0.012332\n",
      "[300/01199] train_loss: 0.011683\n",
      "[301/00023] train_loss: 0.012048\n",
      "[301/00073] train_loss: 0.012590\n",
      "[301/00123] train_loss: 0.011777\n",
      "[301/00173] train_loss: 0.012299\n",
      "[301/00223] train_loss: 0.012215\n",
      "[301/00273] train_loss: 0.011922\n",
      "[301/00323] train_loss: 0.011680\n",
      "[301/00373] train_loss: 0.012314\n",
      "[301/00423] train_loss: 0.011744\n",
      "[301/00473] train_loss: 0.011193\n",
      "[301/00523] train_loss: 0.012854\n",
      "[301/00573] train_loss: 0.011948\n",
      "[301/00623] train_loss: 0.011665\n",
      "[301/00673] train_loss: 0.012544\n",
      "[301/00723] train_loss: 0.011887\n",
      "[301/00773] train_loss: 0.011800\n",
      "[301/00823] train_loss: 0.012347\n",
      "[301/00873] train_loss: 0.011323\n",
      "[301/00923] train_loss: 0.011726\n",
      "[301/00973] train_loss: 0.011990\n",
      "[301/01023] train_loss: 0.011538\n",
      "[301/01073] train_loss: 0.012533\n",
      "[301/01123] train_loss: 0.011190\n",
      "[301/01173] train_loss: 0.011742\n",
      "[301/01223] train_loss: 0.011382\n",
      "[302/00047] train_loss: 0.012235\n",
      "[302/00097] train_loss: 0.013104\n",
      "[302/00147] train_loss: 0.012233\n",
      "[302/00197] train_loss: 0.012485\n",
      "[302/00247] train_loss: 0.012391\n",
      "[302/00297] train_loss: 0.011573\n",
      "[302/00347] train_loss: 0.012433\n",
      "[302/00397] train_loss: 0.011732\n",
      "[302/00447] train_loss: 0.012307\n",
      "[302/00497] train_loss: 0.012238\n",
      "[302/00547] train_loss: 0.012415\n",
      "[302/00597] train_loss: 0.011890\n",
      "[302/00647] train_loss: 0.012650\n",
      "[302/00697] train_loss: 0.011789\n",
      "[302/00747] train_loss: 0.011786\n",
      "[302/00797] train_loss: 0.011838\n",
      "[302/00847] train_loss: 0.011272\n",
      "[302/00897] train_loss: 0.011995\n",
      "[302/00947] train_loss: 0.011541\n",
      "[302/00997] train_loss: 0.011053\n",
      "[302/01047] train_loss: 0.011812\n",
      "[302/01097] train_loss: 0.011433\n",
      "[302/01147] train_loss: 0.011818\n",
      "[302/01197] train_loss: 0.011915\n",
      "[303/00021] train_loss: 0.012232\n",
      "[303/00071] train_loss: 0.012616\n",
      "[303/00121] train_loss: 0.012236\n",
      "[303/00171] train_loss: 0.012133\n",
      "[303/00221] train_loss: 0.012081\n",
      "[303/00271] train_loss: 0.011660\n",
      "[303/00321] train_loss: 0.012074\n",
      "[303/00371] train_loss: 0.011929\n",
      "[303/00421] train_loss: 0.012240\n",
      "[303/00471] train_loss: 0.011731\n",
      "[303/00521] train_loss: 0.012291\n",
      "[303/00571] train_loss: 0.011521\n",
      "[303/00621] train_loss: 0.011785\n",
      "[303/00671] train_loss: 0.012081\n",
      "[303/00721] train_loss: 0.012065\n",
      "[303/00771] train_loss: 0.012043\n",
      "[303/00821] train_loss: 0.011732\n",
      "[303/00871] train_loss: 0.011529\n",
      "[303/00921] train_loss: 0.011399\n",
      "[303/00971] train_loss: 0.012202\n",
      "[303/01021] train_loss: 0.011851\n",
      "[303/01071] train_loss: 0.011720\n",
      "[303/01121] train_loss: 0.011754\n",
      "[303/01171] train_loss: 0.011406\n",
      "[303/01221] train_loss: 0.012025\n",
      "[304/00045] train_loss: 0.012526\n",
      "[304/00095] train_loss: 0.012394\n",
      "[304/00145] train_loss: 0.012296\n",
      "[304/00195] train_loss: 0.012155\n",
      "[304/00245] train_loss: 0.011639\n",
      "[304/00295] train_loss: 0.012264\n",
      "[304/00345] train_loss: 0.012951\n",
      "[304/00395] train_loss: 0.012558\n",
      "[304/00445] train_loss: 0.011639\n",
      "[304/00495] train_loss: 0.012211\n",
      "[304/00545] train_loss: 0.011656\n",
      "[304/00595] train_loss: 0.012389\n",
      "[304/00645] train_loss: 0.011917\n",
      "[304/00695] train_loss: 0.012241\n",
      "[304/00745] train_loss: 0.012595\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[304/00795] train_loss: 0.011372\n",
      "[304/00845] train_loss: 0.012050\n",
      "[304/00895] train_loss: 0.011847\n",
      "[304/00945] train_loss: 0.011930\n",
      "[304/00995] train_loss: 0.011504\n",
      "[304/01045] train_loss: 0.012571\n",
      "[304/01095] train_loss: 0.011494\n",
      "[304/01145] train_loss: 0.011721\n",
      "[304/01195] train_loss: 0.011617\n",
      "[305/00019] train_loss: 0.011730\n",
      "[305/00069] train_loss: 0.012286\n",
      "[305/00119] train_loss: 0.012729\n",
      "[305/00169] train_loss: 0.012100\n",
      "[305/00219] train_loss: 0.011909\n",
      "[305/00269] train_loss: 0.011983\n",
      "[305/00319] train_loss: 0.012432\n",
      "[305/00369] train_loss: 0.012252\n",
      "[305/00419] train_loss: 0.012150\n",
      "[305/00469] train_loss: 0.012188\n",
      "[305/00519] train_loss: 0.011927\n",
      "[305/00569] train_loss: 0.012479\n",
      "[305/00619] train_loss: 0.012060\n",
      "[305/00669] train_loss: 0.012027\n",
      "[305/00719] train_loss: 0.011739\n",
      "[305/00769] train_loss: 0.011697\n",
      "[305/00819] train_loss: 0.011903\n",
      "[305/00869] train_loss: 0.011359\n",
      "[305/00919] train_loss: 0.011618\n",
      "[305/00969] train_loss: 0.012134\n",
      "[305/01019] train_loss: 0.011243\n",
      "[305/01069] train_loss: 0.011542\n",
      "[305/01119] train_loss: 0.011766\n",
      "[305/01169] train_loss: 0.011213\n",
      "[305/01219] train_loss: 0.011668\n",
      "[306/00043] train_loss: 0.012114\n",
      "[306/00093] train_loss: 0.012060\n",
      "[306/00143] train_loss: 0.011900\n",
      "[306/00193] train_loss: 0.011693\n",
      "[306/00243] train_loss: 0.012681\n",
      "[306/00293] train_loss: 0.012223\n",
      "[306/00343] train_loss: 0.012665\n",
      "[306/00393] train_loss: 0.011761\n",
      "[306/00443] train_loss: 0.012119\n",
      "[306/00493] train_loss: 0.012023\n",
      "[306/00543] train_loss: 0.011726\n",
      "[306/00593] train_loss: 0.012435\n",
      "[306/00643] train_loss: 0.012501\n",
      "[306/00693] train_loss: 0.011910\n",
      "[306/00743] train_loss: 0.012522\n",
      "[306/00793] train_loss: 0.011987\n",
      "[306/00843] train_loss: 0.011814\n",
      "[306/00893] train_loss: 0.011691\n",
      "[306/00943] train_loss: 0.011919\n",
      "[306/00993] train_loss: 0.012350\n",
      "[306/01043] train_loss: 0.011698\n",
      "[306/01093] train_loss: 0.011487\n",
      "[306/01143] train_loss: 0.012003\n",
      "[306/01193] train_loss: 0.011934\n",
      "[307/00017] train_loss: 0.011774\n",
      "[307/00067] train_loss: 0.012553\n",
      "[307/00117] train_loss: 0.012810\n",
      "[307/00167] train_loss: 0.012184\n",
      "[307/00217] train_loss: 0.012040\n",
      "[307/00267] train_loss: 0.012410\n",
      "[307/00317] train_loss: 0.012010\n",
      "[307/00367] train_loss: 0.011738\n",
      "[307/00417] train_loss: 0.011458\n",
      "[307/00467] train_loss: 0.012188\n",
      "[307/00517] train_loss: 0.012101\n",
      "[307/00567] train_loss: 0.011794\n",
      "[307/00617] train_loss: 0.011891\n",
      "[307/00667] train_loss: 0.012090\n",
      "[307/00717] train_loss: 0.011895\n",
      "[307/00767] train_loss: 0.011755\n",
      "[307/00817] train_loss: 0.012156\n",
      "[307/00867] train_loss: 0.011761\n",
      "[307/00917] train_loss: 0.011583\n",
      "[307/00967] train_loss: 0.011844\n",
      "[307/01017] train_loss: 0.011723\n",
      "[307/01067] train_loss: 0.011623\n",
      "[307/01117] train_loss: 0.011202\n",
      "[307/01167] train_loss: 0.011468\n",
      "[307/01217] train_loss: 0.011086\n",
      "[308/00041] train_loss: 0.012139\n",
      "[308/00091] train_loss: 0.012219\n",
      "[308/00141] train_loss: 0.012335\n",
      "[308/00191] train_loss: 0.012179\n",
      "[308/00241] train_loss: 0.012536\n",
      "[308/00291] train_loss: 0.011819\n",
      "[308/00341] train_loss: 0.012126\n",
      "[308/00391] train_loss: 0.011645\n",
      "[308/00441] train_loss: 0.011759\n",
      "[308/00491] train_loss: 0.012303\n",
      "[308/00541] train_loss: 0.011655\n",
      "[308/00591] train_loss: 0.011591\n",
      "[308/00641] train_loss: 0.012118\n",
      "[308/00691] train_loss: 0.012163\n",
      "[308/00741] train_loss: 0.011857\n",
      "[308/00791] train_loss: 0.012220\n",
      "[308/00841] train_loss: 0.011713\n",
      "[308/00891] train_loss: 0.012015\n",
      "[308/00941] train_loss: 0.012460\n",
      "[308/00991] train_loss: 0.011524\n",
      "[308/01041] train_loss: 0.012016\n",
      "[308/01091] train_loss: 0.011539\n",
      "[308/01141] train_loss: 0.011374\n",
      "[308/01191] train_loss: 0.011460\n",
      "[309/00015] train_loss: 0.012463\n",
      "[309/00065] train_loss: 0.012403\n",
      "[309/00115] train_loss: 0.012630\n",
      "[309/00165] train_loss: 0.011769\n",
      "[309/00215] train_loss: 0.012534\n",
      "[309/00265] train_loss: 0.012376\n",
      "[309/00315] train_loss: 0.011772\n",
      "[309/00365] train_loss: 0.011865\n",
      "[309/00415] train_loss: 0.011324\n",
      "[309/00465] train_loss: 0.011089\n",
      "[309/00515] train_loss: 0.011706\n",
      "[309/00565] train_loss: 0.011831\n",
      "[309/00615] train_loss: 0.012053\n",
      "[309/00665] train_loss: 0.011652\n",
      "[309/00715] train_loss: 0.011371\n",
      "[309/00765] train_loss: 0.011898\n",
      "[309/00815] train_loss: 0.011919\n",
      "[309/00865] train_loss: 0.011329\n",
      "[309/00915] train_loss: 0.011344\n",
      "[309/00965] train_loss: 0.011563\n",
      "[309/01015] train_loss: 0.011553\n",
      "[309/01065] train_loss: 0.011555\n",
      "[309/01115] train_loss: 0.012450\n",
      "[309/01165] train_loss: 0.012238\n",
      "[309/01215] train_loss: 0.011662\n",
      "[310/00039] train_loss: 0.011843\n",
      "[310/00089] train_loss: 0.012609\n",
      "[310/00139] train_loss: 0.011815\n",
      "[310/00189] train_loss: 0.012957\n",
      "[310/00239] train_loss: 0.013109\n",
      "[310/00289] train_loss: 0.012097\n",
      "[310/00339] train_loss: 0.011993\n",
      "[310/00389] train_loss: 0.012241\n",
      "[310/00439] train_loss: 0.012018\n",
      "[310/00489] train_loss: 0.012148\n",
      "[310/00539] train_loss: 0.011486\n",
      "[310/00589] train_loss: 0.012499\n",
      "[310/00639] train_loss: 0.012174\n",
      "[310/00689] train_loss: 0.011447\n",
      "[310/00739] train_loss: 0.011483\n",
      "[310/00789] train_loss: 0.012354\n",
      "[310/00839] train_loss: 0.011059\n",
      "[310/00889] train_loss: 0.011151\n",
      "[310/00939] train_loss: 0.012402\n",
      "[310/00989] train_loss: 0.011032\n",
      "[310/01039] train_loss: 0.011687\n",
      "[310/01089] train_loss: 0.011879\n",
      "[310/01139] train_loss: 0.011827\n",
      "[310/01189] train_loss: 0.011610\n",
      "[311/00013] train_loss: 0.012361\n",
      "[311/00063] train_loss: 0.012822\n",
      "[311/00113] train_loss: 0.012078\n",
      "[311/00163] train_loss: 0.012401\n",
      "[311/00213] train_loss: 0.011903\n",
      "[311/00263] train_loss: 0.012264\n",
      "[311/00313] train_loss: 0.012190\n",
      "[311/00363] train_loss: 0.012073\n",
      "[311/00413] train_loss: 0.011507\n",
      "[311/00463] train_loss: 0.012277\n",
      "[311/00513] train_loss: 0.012586\n",
      "[311/00563] train_loss: 0.011830\n",
      "[311/00613] train_loss: 0.012172\n",
      "[311/00663] train_loss: 0.011646\n",
      "[311/00713] train_loss: 0.011300\n",
      "[311/00763] train_loss: 0.011741\n",
      "[311/00813] train_loss: 0.011507\n",
      "[311/00863] train_loss: 0.011314\n",
      "[311/00913] train_loss: 0.011669\n",
      "[311/00963] train_loss: 0.011528\n",
      "[311/01013] train_loss: 0.012106\n",
      "[311/01063] train_loss: 0.012119\n",
      "[311/01113] train_loss: 0.011306\n",
      "[311/01163] train_loss: 0.011865\n",
      "[311/01213] train_loss: 0.011285\n",
      "[312/00037] train_loss: 0.012527\n",
      "[312/00087] train_loss: 0.012485\n",
      "[312/00137] train_loss: 0.012276\n",
      "[312/00187] train_loss: 0.012052\n",
      "[312/00237] train_loss: 0.011533\n",
      "[312/00287] train_loss: 0.011907\n",
      "[312/00337] train_loss: 0.011801\n",
      "[312/00387] train_loss: 0.011835\n",
      "[312/00437] train_loss: 0.011746\n",
      "[312/00487] train_loss: 0.011933\n",
      "[312/00537] train_loss: 0.011667\n",
      "[312/00587] train_loss: 0.011422\n",
      "[312/00637] train_loss: 0.011834\n",
      "[312/00687] train_loss: 0.011986\n",
      "[312/00737] train_loss: 0.012028\n",
      "[312/00787] train_loss: 0.011776\n",
      "[312/00837] train_loss: 0.011448\n",
      "[312/00887] train_loss: 0.011769\n",
      "[312/00937] train_loss: 0.012271\n",
      "[312/00987] train_loss: 0.011920\n",
      "[312/01037] train_loss: 0.011916\n",
      "[312/01087] train_loss: 0.010974\n",
      "[312/01137] train_loss: 0.010812\n",
      "[312/01187] train_loss: 0.011511\n",
      "[313/00011] train_loss: 0.012129\n",
      "[313/00061] train_loss: 0.012134\n",
      "[313/00111] train_loss: 0.011772\n",
      "[313/00161] train_loss: 0.011625\n",
      "[313/00211] train_loss: 0.011187\n",
      "[313/00261] train_loss: 0.011770\n",
      "[313/00311] train_loss: 0.012239\n",
      "[313/00361] train_loss: 0.012348\n",
      "[313/00411] train_loss: 0.012078\n",
      "[313/00461] train_loss: 0.011495\n",
      "[313/00511] train_loss: 0.012055\n",
      "[313/00561] train_loss: 0.011002\n",
      "[313/00611] train_loss: 0.012439\n",
      "[313/00661] train_loss: 0.011925\n",
      "[313/00711] train_loss: 0.011508\n",
      "[313/00761] train_loss: 0.011923\n",
      "[313/00811] train_loss: 0.011815\n",
      "[313/00861] train_loss: 0.011667\n",
      "[313/00911] train_loss: 0.011933\n",
      "[313/00961] train_loss: 0.011899\n",
      "[313/01011] train_loss: 0.012301\n",
      "[313/01061] train_loss: 0.011848\n",
      "[313/01111] train_loss: 0.012511\n",
      "[313/01161] train_loss: 0.012019\n",
      "[313/01211] train_loss: 0.011791\n",
      "[314/00035] train_loss: 0.012311\n",
      "[314/00085] train_loss: 0.012087\n",
      "[314/00135] train_loss: 0.011740\n",
      "[314/00185] train_loss: 0.011948\n",
      "[314/00235] train_loss: 0.011608\n",
      "[314/00285] train_loss: 0.012550\n",
      "[314/00335] train_loss: 0.011936\n",
      "[314/00385] train_loss: 0.012159\n",
      "[314/00435] train_loss: 0.012520\n",
      "[314/00485] train_loss: 0.011371\n",
      "[314/00535] train_loss: 0.012117\n",
      "[314/00585] train_loss: 0.012596\n",
      "[314/00635] train_loss: 0.012019\n",
      "[314/00685] train_loss: 0.010915\n",
      "[314/00735] train_loss: 0.012151\n",
      "[314/00785] train_loss: 0.011613\n",
      "[314/00835] train_loss: 0.011166\n",
      "[314/00885] train_loss: 0.011466\n",
      "[314/00935] train_loss: 0.012117\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[314/00985] train_loss: 0.011754\n",
      "[314/01035] train_loss: 0.011670\n",
      "[314/01085] train_loss: 0.012019\n",
      "[314/01135] train_loss: 0.011677\n",
      "[314/01185] train_loss: 0.011939\n",
      "[315/00009] train_loss: 0.011998\n",
      "[315/00059] train_loss: 0.012571\n",
      "[315/00109] train_loss: 0.012371\n",
      "[315/00159] train_loss: 0.012164\n",
      "[315/00209] train_loss: 0.011980\n",
      "[315/00259] train_loss: 0.011919\n",
      "[315/00309] train_loss: 0.011627\n",
      "[315/00359] train_loss: 0.011418\n",
      "[315/00409] train_loss: 0.011666\n",
      "[315/00459] train_loss: 0.012351\n",
      "[315/00509] train_loss: 0.011813\n",
      "[315/00559] train_loss: 0.012000\n",
      "[315/00609] train_loss: 0.012513\n",
      "[315/00659] train_loss: 0.011594\n",
      "[315/00709] train_loss: 0.011452\n",
      "[315/00759] train_loss: 0.011928\n",
      "[315/00809] train_loss: 0.011876\n",
      "[315/00859] train_loss: 0.011406\n",
      "[315/00909] train_loss: 0.011737\n",
      "[315/00959] train_loss: 0.011468\n",
      "[315/01009] train_loss: 0.012272\n",
      "[315/01059] train_loss: 0.011650\n",
      "[315/01109] train_loss: 0.012090\n",
      "[315/01159] train_loss: 0.012028\n",
      "[315/01209] train_loss: 0.011576\n",
      "[316/00033] train_loss: 0.011858\n",
      "[316/00083] train_loss: 0.012549\n",
      "[316/00133] train_loss: 0.012429\n",
      "[316/00183] train_loss: 0.011678\n",
      "[316/00233] train_loss: 0.012235\n",
      "[316/00283] train_loss: 0.011589\n",
      "[316/00333] train_loss: 0.012535\n",
      "[316/00383] train_loss: 0.011465\n",
      "[316/00433] train_loss: 0.011354\n",
      "[316/00483] train_loss: 0.011689\n",
      "[316/00533] train_loss: 0.011635\n",
      "[316/00583] train_loss: 0.011783\n",
      "[316/00633] train_loss: 0.012074\n",
      "[316/00683] train_loss: 0.012410\n",
      "[316/00733] train_loss: 0.011957\n",
      "[316/00783] train_loss: 0.011760\n",
      "[316/00833] train_loss: 0.011714\n",
      "[316/00883] train_loss: 0.012421\n",
      "[316/00933] train_loss: 0.011799\n",
      "[316/00983] train_loss: 0.011543\n",
      "[316/01033] train_loss: 0.011770\n",
      "[316/01083] train_loss: 0.011781\n",
      "[316/01133] train_loss: 0.011855\n",
      "[316/01183] train_loss: 0.011325\n",
      "[317/00007] train_loss: 0.011539\n",
      "[317/00057] train_loss: 0.012906\n",
      "[317/00107] train_loss: 0.011885\n",
      "[317/00157] train_loss: 0.012131\n",
      "[317/00207] train_loss: 0.012111\n",
      "[317/00257] train_loss: 0.012161\n",
      "[317/00307] train_loss: 0.011950\n",
      "[317/00357] train_loss: 0.011686\n",
      "[317/00407] train_loss: 0.012054\n",
      "[317/00457] train_loss: 0.011598\n",
      "[317/00507] train_loss: 0.011967\n",
      "[317/00557] train_loss: 0.012017\n",
      "[317/00607] train_loss: 0.012865\n",
      "[317/00657] train_loss: 0.011395\n",
      "[317/00707] train_loss: 0.011220\n",
      "[317/00757] train_loss: 0.012000\n",
      "[317/00807] train_loss: 0.011101\n",
      "[317/00857] train_loss: 0.011556\n",
      "[317/00907] train_loss: 0.011490\n",
      "[317/00957] train_loss: 0.011468\n",
      "[317/01007] train_loss: 0.012354\n",
      "[317/01057] train_loss: 0.011925\n",
      "[317/01107] train_loss: 0.011825\n",
      "[317/01157] train_loss: 0.011280\n",
      "[317/01207] train_loss: 0.011370\n",
      "[318/00031] train_loss: 0.012146\n",
      "[318/00081] train_loss: 0.012582\n",
      "[318/00131] train_loss: 0.012517\n",
      "[318/00181] train_loss: 0.011416\n",
      "[318/00231] train_loss: 0.012585\n",
      "[318/00281] train_loss: 0.012068\n",
      "[318/00331] train_loss: 0.012040\n",
      "[318/00381] train_loss: 0.011543\n",
      "[318/00431] train_loss: 0.012513\n",
      "[318/00481] train_loss: 0.012007\n",
      "[318/00531] train_loss: 0.011580\n",
      "[318/00581] train_loss: 0.011982\n",
      "[318/00631] train_loss: 0.011844\n",
      "[318/00681] train_loss: 0.011196\n",
      "[318/00731] train_loss: 0.010748\n",
      "[318/00781] train_loss: 0.012164\n",
      "[318/00831] train_loss: 0.011564\n",
      "[318/00881] train_loss: 0.011943\n",
      "[318/00931] train_loss: 0.011855\n",
      "[318/00981] train_loss: 0.010979\n",
      "[318/01031] train_loss: 0.011535\n",
      "[318/01081] train_loss: 0.011953\n",
      "[318/01131] train_loss: 0.012139\n",
      "[318/01181] train_loss: 0.011197\n",
      "[319/00005] train_loss: 0.012530\n",
      "[319/00055] train_loss: 0.012215\n",
      "[319/00105] train_loss: 0.011502\n",
      "[319/00155] train_loss: 0.011605\n",
      "[319/00205] train_loss: 0.012203\n",
      "[319/00255] train_loss: 0.012033\n",
      "[319/00305] train_loss: 0.011337\n",
      "[319/00355] train_loss: 0.012398\n",
      "[319/00405] train_loss: 0.012230\n",
      "[319/00455] train_loss: 0.011439\n",
      "[319/00505] train_loss: 0.012134\n",
      "[319/00555] train_loss: 0.012181\n",
      "[319/00605] train_loss: 0.011471\n",
      "[319/00655] train_loss: 0.012017\n",
      "[319/00705] train_loss: 0.011670\n",
      "[319/00755] train_loss: 0.011691\n",
      "[319/00805] train_loss: 0.011182\n",
      "[319/00855] train_loss: 0.012064\n",
      "[319/00905] train_loss: 0.011735\n",
      "[319/00955] train_loss: 0.011880\n",
      "[319/01005] train_loss: 0.011995\n",
      "[319/01055] train_loss: 0.011804\n",
      "[319/01105] train_loss: 0.011898\n",
      "[319/01155] train_loss: 0.011419\n",
      "[319/01205] train_loss: 0.012919\n",
      "[320/00029] train_loss: 0.011584\n",
      "[320/00079] train_loss: 0.011600\n",
      "[320/00129] train_loss: 0.012457\n",
      "[320/00179] train_loss: 0.012045\n",
      "[320/00229] train_loss: 0.012129\n",
      "[320/00279] train_loss: 0.011707\n",
      "[320/00329] train_loss: 0.012618\n",
      "[320/00379] train_loss: 0.012162\n",
      "[320/00429] train_loss: 0.011597\n",
      "[320/00479] train_loss: 0.011721\n",
      "[320/00529] train_loss: 0.011697\n",
      "[320/00579] train_loss: 0.011209\n",
      "[320/00629] train_loss: 0.011474\n",
      "[320/00679] train_loss: 0.011303\n",
      "[320/00729] train_loss: 0.011875\n",
      "[320/00779] train_loss: 0.011300\n",
      "[320/00829] train_loss: 0.010993\n",
      "[320/00879] train_loss: 0.012406\n",
      "[320/00929] train_loss: 0.012548\n",
      "[320/00979] train_loss: 0.012389\n",
      "[320/01029] train_loss: 0.011933\n",
      "[320/01079] train_loss: 0.011601\n",
      "[320/01129] train_loss: 0.011717\n",
      "[320/01179] train_loss: 0.011163\n",
      "[321/00003] train_loss: 0.011140\n",
      "[321/00053] train_loss: 0.012389\n",
      "[321/00103] train_loss: 0.012585\n",
      "[321/00153] train_loss: 0.011652\n",
      "[321/00203] train_loss: 0.012510\n",
      "[321/00253] train_loss: 0.012328\n",
      "[321/00303] train_loss: 0.012047\n",
      "[321/00353] train_loss: 0.012220\n",
      "[321/00403] train_loss: 0.011672\n",
      "[321/00453] train_loss: 0.011764\n",
      "[321/00503] train_loss: 0.011788\n",
      "[321/00553] train_loss: 0.011374\n",
      "[321/00603] train_loss: 0.012503\n",
      "[321/00653] train_loss: 0.012244\n",
      "[321/00703] train_loss: 0.011279\n",
      "[321/00753] train_loss: 0.012237\n",
      "[321/00803] train_loss: 0.011682\n",
      "[321/00853] train_loss: 0.011563\n",
      "[321/00903] train_loss: 0.011673\n",
      "[321/00953] train_loss: 0.011740\n",
      "[321/01003] train_loss: 0.012033\n",
      "[321/01053] train_loss: 0.012519\n",
      "[321/01103] train_loss: 0.011204\n",
      "[321/01153] train_loss: 0.011045\n",
      "[321/01203] train_loss: 0.010927\n",
      "[322/00027] train_loss: 0.012412\n",
      "[322/00077] train_loss: 0.012322\n",
      "[322/00127] train_loss: 0.011689\n",
      "[322/00177] train_loss: 0.012989\n",
      "[322/00227] train_loss: 0.011875\n",
      "[322/00277] train_loss: 0.011616\n",
      "[322/00327] train_loss: 0.011802\n",
      "[322/00377] train_loss: 0.011291\n",
      "[322/00427] train_loss: 0.012009\n",
      "[322/00477] train_loss: 0.011569\n",
      "[322/00527] train_loss: 0.011654\n",
      "[322/00577] train_loss: 0.011749\n",
      "[322/00627] train_loss: 0.011246\n",
      "[322/00677] train_loss: 0.011778\n",
      "[322/00727] train_loss: 0.012432\n",
      "[322/00777] train_loss: 0.011015\n",
      "[322/00827] train_loss: 0.012542\n",
      "[322/00877] train_loss: 0.011694\n",
      "[322/00927] train_loss: 0.011505\n",
      "[322/00977] train_loss: 0.012109\n",
      "[322/01027] train_loss: 0.012105\n",
      "[322/01077] train_loss: 0.012064\n",
      "[322/01127] train_loss: 0.011781\n",
      "[322/01177] train_loss: 0.011646\n",
      "[323/00001] train_loss: 0.012296\n",
      "[323/00051] train_loss: 0.012411\n",
      "[323/00101] train_loss: 0.012218\n",
      "[323/00151] train_loss: 0.012098\n",
      "[323/00201] train_loss: 0.011538\n",
      "[323/00251] train_loss: 0.011657\n",
      "[323/00301] train_loss: 0.011982\n",
      "[323/00351] train_loss: 0.011635\n",
      "[323/00401] train_loss: 0.011248\n",
      "[323/00451] train_loss: 0.011732\n",
      "[323/00501] train_loss: 0.012477\n",
      "[323/00551] train_loss: 0.012078\n",
      "[323/00601] train_loss: 0.012304\n",
      "[323/00651] train_loss: 0.011655\n",
      "[323/00701] train_loss: 0.011600\n",
      "[323/00751] train_loss: 0.011995\n",
      "[323/00801] train_loss: 0.011571\n",
      "[323/00851] train_loss: 0.011008\n",
      "[323/00901] train_loss: 0.011558\n",
      "[323/00951] train_loss: 0.011976\n",
      "[323/01001] train_loss: 0.012014\n",
      "[323/01051] train_loss: 0.012303\n",
      "[323/01101] train_loss: 0.011651\n",
      "[323/01151] train_loss: 0.011796\n",
      "[323/01201] train_loss: 0.012628\n",
      "[324/00025] train_loss: 0.012459\n",
      "[324/00075] train_loss: 0.012710\n",
      "[324/00125] train_loss: 0.012688\n",
      "[324/00175] train_loss: 0.012530\n",
      "[324/00225] train_loss: 0.012511\n",
      "[324/00275] train_loss: 0.011726\n",
      "[324/00325] train_loss: 0.011606\n",
      "[324/00375] train_loss: 0.011174\n",
      "[324/00425] train_loss: 0.011557\n",
      "[324/00475] train_loss: 0.011711\n",
      "[324/00525] train_loss: 0.012122\n",
      "[324/00575] train_loss: 0.011927\n",
      "[324/00625] train_loss: 0.011941\n",
      "[324/00675] train_loss: 0.011576\n",
      "[324/00725] train_loss: 0.012324\n",
      "[324/00775] train_loss: 0.011100\n",
      "[324/00825] train_loss: 0.011676\n",
      "[324/00875] train_loss: 0.011520\n",
      "[324/00925] train_loss: 0.011998\n",
      "[324/00975] train_loss: 0.011597\n",
      "[324/01025] train_loss: 0.011536\n",
      "[324/01075] train_loss: 0.011892\n",
      "[324/01125] train_loss: 0.012166\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[324/01175] train_loss: 0.011400\n",
      "[324/01225] train_loss: 0.011208\n",
      "[325/00049] train_loss: 0.012225\n",
      "[325/00099] train_loss: 0.011616\n",
      "[325/00149] train_loss: 0.012055\n",
      "[325/00199] train_loss: 0.012087\n",
      "[325/00249] train_loss: 0.011221\n",
      "[325/00299] train_loss: 0.012056\n",
      "[325/00349] train_loss: 0.011570\n",
      "[325/00399] train_loss: 0.012577\n",
      "[325/00449] train_loss: 0.012164\n",
      "[325/00499] train_loss: 0.012287\n",
      "[325/00549] train_loss: 0.012118\n",
      "[325/00599] train_loss: 0.011963\n",
      "[325/00649] train_loss: 0.012071\n",
      "[325/00699] train_loss: 0.011334\n",
      "[325/00749] train_loss: 0.011211\n",
      "[325/00799] train_loss: 0.011342\n",
      "[325/00849] train_loss: 0.013023\n",
      "[325/00899] train_loss: 0.011314\n",
      "[325/00949] train_loss: 0.011895\n",
      "[325/00999] train_loss: 0.012427\n",
      "[325/01049] train_loss: 0.011882\n",
      "[325/01099] train_loss: 0.011236\n",
      "[325/01149] train_loss: 0.011385\n",
      "[325/01199] train_loss: 0.011449\n",
      "[326/00023] train_loss: 0.011878\n",
      "[326/00073] train_loss: 0.011436\n",
      "[326/00123] train_loss: 0.011851\n",
      "[326/00173] train_loss: 0.011409\n",
      "[326/00223] train_loss: 0.011934\n",
      "[326/00273] train_loss: 0.012185\n",
      "[326/00323] train_loss: 0.011736\n",
      "[326/00373] train_loss: 0.011689\n",
      "[326/00423] train_loss: 0.012307\n",
      "[326/00473] train_loss: 0.011923\n",
      "[326/00523] train_loss: 0.012471\n",
      "[326/00573] train_loss: 0.011962\n",
      "[326/00623] train_loss: 0.012134\n",
      "[326/00673] train_loss: 0.011772\n",
      "[326/00723] train_loss: 0.010891\n",
      "[326/00773] train_loss: 0.011337\n",
      "[326/00823] train_loss: 0.011524\n",
      "[326/00873] train_loss: 0.012779\n",
      "[326/00923] train_loss: 0.011302\n",
      "[326/00973] train_loss: 0.011695\n",
      "[326/01023] train_loss: 0.011449\n",
      "[326/01073] train_loss: 0.011535\n",
      "[326/01123] train_loss: 0.011610\n",
      "[326/01173] train_loss: 0.011859\n",
      "[326/01223] train_loss: 0.011402\n",
      "[327/00047] train_loss: 0.011930\n",
      "[327/00097] train_loss: 0.012186\n",
      "[327/00147] train_loss: 0.012287\n",
      "[327/00197] train_loss: 0.011866\n",
      "[327/00247] train_loss: 0.011768\n",
      "[327/00297] train_loss: 0.011650\n",
      "[327/00347] train_loss: 0.012335\n",
      "[327/00397] train_loss: 0.011960\n",
      "[327/00447] train_loss: 0.011619\n",
      "[327/00497] train_loss: 0.012223\n",
      "[327/00547] train_loss: 0.012180\n",
      "[327/00597] train_loss: 0.012071\n",
      "[327/00647] train_loss: 0.011246\n",
      "[327/00697] train_loss: 0.011982\n",
      "[327/00747] train_loss: 0.011950\n",
      "[327/00797] train_loss: 0.011864\n",
      "[327/00847] train_loss: 0.011463\n",
      "[327/00897] train_loss: 0.011580\n",
      "[327/00947] train_loss: 0.011620\n",
      "[327/00997] train_loss: 0.011337\n",
      "[327/01047] train_loss: 0.012203\n",
      "[327/01097] train_loss: 0.011214\n",
      "[327/01147] train_loss: 0.011641\n",
      "[327/01197] train_loss: 0.011183\n",
      "[328/00021] train_loss: 0.011591\n",
      "[328/00071] train_loss: 0.012290\n",
      "[328/00121] train_loss: 0.012491\n",
      "[328/00171] train_loss: 0.012204\n",
      "[328/00221] train_loss: 0.012166\n",
      "[328/00271] train_loss: 0.011742\n",
      "[328/00321] train_loss: 0.011778\n",
      "[328/00371] train_loss: 0.011848\n",
      "[328/00421] train_loss: 0.011562\n",
      "[328/00471] train_loss: 0.011994\n",
      "[328/00521] train_loss: 0.012479\n",
      "[328/00571] train_loss: 0.011330\n",
      "[328/00621] train_loss: 0.011632\n",
      "[328/00671] train_loss: 0.010939\n",
      "[328/00721] train_loss: 0.011305\n",
      "[328/00771] train_loss: 0.011766\n",
      "[328/00821] train_loss: 0.012208\n",
      "[328/00871] train_loss: 0.010844\n",
      "[328/00921] train_loss: 0.011614\n",
      "[328/00971] train_loss: 0.011487\n",
      "[328/01021] train_loss: 0.011156\n",
      "[328/01071] train_loss: 0.011705\n",
      "[328/01121] train_loss: 0.011619\n",
      "[328/01171] train_loss: 0.012006\n",
      "[328/01221] train_loss: 0.011731\n",
      "[329/00045] train_loss: 0.012476\n",
      "[329/00095] train_loss: 0.012299\n",
      "[329/00145] train_loss: 0.012041\n",
      "[329/00195] train_loss: 0.011590\n",
      "[329/00245] train_loss: 0.011621\n",
      "[329/00295] train_loss: 0.012011\n",
      "[329/00345] train_loss: 0.011851\n",
      "[329/00395] train_loss: 0.011736\n",
      "[329/00445] train_loss: 0.011753\n",
      "[329/00495] train_loss: 0.011691\n",
      "[329/00545] train_loss: 0.011939\n",
      "[329/00595] train_loss: 0.011378\n",
      "[329/00645] train_loss: 0.012358\n",
      "[329/00695] train_loss: 0.012811\n",
      "[329/00745] train_loss: 0.011555\n",
      "[329/00795] train_loss: 0.011899\n",
      "[329/00845] train_loss: 0.011435\n",
      "[329/00895] train_loss: 0.011485\n",
      "[329/00945] train_loss: 0.011448\n",
      "[329/00995] train_loss: 0.010921\n",
      "[329/01045] train_loss: 0.011300\n",
      "[329/01095] train_loss: 0.011521\n",
      "[329/01145] train_loss: 0.011837\n",
      "[329/01195] train_loss: 0.012149\n",
      "[330/00019] train_loss: 0.011558\n",
      "[330/00069] train_loss: 0.012758\n",
      "[330/00119] train_loss: 0.011692\n",
      "[330/00169] train_loss: 0.012459\n",
      "[330/00219] train_loss: 0.012672\n",
      "[330/00269] train_loss: 0.012618\n",
      "[330/00319] train_loss: 0.012596\n",
      "[330/00369] train_loss: 0.011738\n",
      "[330/00419] train_loss: 0.011674\n",
      "[330/00469] train_loss: 0.011681\n",
      "[330/00519] train_loss: 0.011508\n",
      "[330/00569] train_loss: 0.011463\n",
      "[330/00619] train_loss: 0.011395\n",
      "[330/00669] train_loss: 0.011667\n",
      "[330/00719] train_loss: 0.012155\n",
      "[330/00769] train_loss: 0.011344\n",
      "[330/00819] train_loss: 0.011416\n",
      "[330/00869] train_loss: 0.011766\n",
      "[330/00919] train_loss: 0.011638\n",
      "[330/00969] train_loss: 0.011589\n",
      "[330/01019] train_loss: 0.012141\n",
      "[330/01069] train_loss: 0.011379\n",
      "[330/01119] train_loss: 0.011307\n",
      "[330/01169] train_loss: 0.012018\n",
      "[330/01219] train_loss: 0.010906\n",
      "[331/00043] train_loss: 0.012762\n",
      "[331/00093] train_loss: 0.011745\n",
      "[331/00143] train_loss: 0.011475\n",
      "[331/00193] train_loss: 0.012511\n",
      "[331/00243] train_loss: 0.011736\n",
      "[331/00293] train_loss: 0.012416\n",
      "[331/00343] train_loss: 0.012427\n",
      "[331/00393] train_loss: 0.011337\n",
      "[331/00443] train_loss: 0.011947\n",
      "[331/00493] train_loss: 0.012244\n",
      "[331/00543] train_loss: 0.011840\n",
      "[331/00593] train_loss: 0.011621\n",
      "[331/00643] train_loss: 0.011873\n",
      "[331/00693] train_loss: 0.011673\n",
      "[331/00743] train_loss: 0.011661\n",
      "[331/00793] train_loss: 0.011553\n",
      "[331/00843] train_loss: 0.011523\n",
      "[331/00893] train_loss: 0.011924\n",
      "[331/00943] train_loss: 0.011514\n",
      "[331/00993] train_loss: 0.011506\n",
      "[331/01043] train_loss: 0.011453\n",
      "[331/01093] train_loss: 0.011354\n",
      "[331/01143] train_loss: 0.011433\n",
      "[331/01193] train_loss: 0.011868\n",
      "[332/00017] train_loss: 0.011339\n",
      "[332/00067] train_loss: 0.011990\n",
      "[332/00117] train_loss: 0.012110\n",
      "[332/00167] train_loss: 0.012114\n",
      "[332/00217] train_loss: 0.012120\n",
      "[332/00267] train_loss: 0.012266\n",
      "[332/00317] train_loss: 0.012556\n",
      "[332/00367] train_loss: 0.012827\n",
      "[332/00417] train_loss: 0.011904\n",
      "[332/00467] train_loss: 0.011607\n",
      "[332/00517] train_loss: 0.011757\n",
      "[332/00567] train_loss: 0.011703\n",
      "[332/00617] train_loss: 0.012247\n",
      "[332/00667] train_loss: 0.011399\n",
      "[332/00717] train_loss: 0.012140\n",
      "[332/00767] train_loss: 0.012310\n",
      "[332/00817] train_loss: 0.011367\n",
      "[332/00867] train_loss: 0.011088\n",
      "[332/00917] train_loss: 0.011272\n",
      "[332/00967] train_loss: 0.011811\n",
      "[332/01017] train_loss: 0.011511\n",
      "[332/01067] train_loss: 0.011386\n",
      "[332/01117] train_loss: 0.011503\n",
      "[332/01167] train_loss: 0.011516\n",
      "[332/01217] train_loss: 0.011227\n",
      "[333/00041] train_loss: 0.012171\n",
      "[333/00091] train_loss: 0.012234\n",
      "[333/00141] train_loss: 0.011827\n",
      "[333/00191] train_loss: 0.011926\n",
      "[333/00241] train_loss: 0.012012\n",
      "[333/00291] train_loss: 0.012020\n",
      "[333/00341] train_loss: 0.012100\n",
      "[333/00391] train_loss: 0.011707\n",
      "[333/00441] train_loss: 0.012149\n",
      "[333/00491] train_loss: 0.012101\n",
      "[333/00541] train_loss: 0.011517\n",
      "[333/00591] train_loss: 0.011880\n",
      "[333/00641] train_loss: 0.011274\n",
      "[333/00691] train_loss: 0.011464\n",
      "[333/00741] train_loss: 0.011953\n",
      "[333/00791] train_loss: 0.011114\n",
      "[333/00841] train_loss: 0.011776\n",
      "[333/00891] train_loss: 0.011803\n",
      "[333/00941] train_loss: 0.011511\n",
      "[333/00991] train_loss: 0.011799\n",
      "[333/01041] train_loss: 0.011643\n",
      "[333/01091] train_loss: 0.011162\n",
      "[333/01141] train_loss: 0.011619\n",
      "[333/01191] train_loss: 0.011082\n",
      "[334/00015] train_loss: 0.011980\n",
      "[334/00065] train_loss: 0.012077\n",
      "[334/00115] train_loss: 0.012153\n",
      "[334/00165] train_loss: 0.011900\n",
      "[334/00215] train_loss: 0.012053\n",
      "[334/00265] train_loss: 0.011750\n",
      "[334/00315] train_loss: 0.011310\n",
      "[334/00365] train_loss: 0.012311\n",
      "[334/00415] train_loss: 0.012396\n",
      "[334/00465] train_loss: 0.011119\n",
      "[334/00515] train_loss: 0.012206\n",
      "[334/00565] train_loss: 0.012446\n",
      "[334/00615] train_loss: 0.011698\n",
      "[334/00665] train_loss: 0.011351\n",
      "[334/00715] train_loss: 0.011848\n",
      "[334/00765] train_loss: 0.011399\n",
      "[334/00815] train_loss: 0.012357\n",
      "[334/00865] train_loss: 0.011971\n",
      "[334/00915] train_loss: 0.011989\n",
      "[334/00965] train_loss: 0.011441\n",
      "[334/01015] train_loss: 0.011681\n",
      "[334/01065] train_loss: 0.011785\n",
      "[334/01115] train_loss: 0.012407\n",
      "[334/01165] train_loss: 0.011526\n",
      "[334/01215] train_loss: 0.011032\n",
      "[335/00039] train_loss: 0.011418\n",
      "[335/00089] train_loss: 0.012457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[335/00139] train_loss: 0.012987\n",
      "[335/00189] train_loss: 0.012009\n",
      "[335/00239] train_loss: 0.012108\n",
      "[335/00289] train_loss: 0.011799\n",
      "[335/00339] train_loss: 0.011538\n",
      "[335/00389] train_loss: 0.011458\n",
      "[335/00439] train_loss: 0.011662\n",
      "[335/00489] train_loss: 0.012017\n",
      "[335/00539] train_loss: 0.011528\n",
      "[335/00589] train_loss: 0.011877\n",
      "[335/00639] train_loss: 0.012234\n",
      "[335/00689] train_loss: 0.012103\n",
      "[335/00739] train_loss: 0.012323\n",
      "[335/00789] train_loss: 0.011294\n",
      "[335/00839] train_loss: 0.011993\n",
      "[335/00889] train_loss: 0.011249\n",
      "[335/00939] train_loss: 0.011046\n",
      "[335/00989] train_loss: 0.011200\n",
      "[335/01039] train_loss: 0.010761\n",
      "[335/01089] train_loss: 0.011354\n",
      "[335/01139] train_loss: 0.011325\n",
      "[335/01189] train_loss: 0.011685\n",
      "[336/00013] train_loss: 0.011358\n",
      "[336/00063] train_loss: 0.012527\n",
      "[336/00113] train_loss: 0.012340\n",
      "[336/00163] train_loss: 0.011453\n",
      "[336/00213] train_loss: 0.012022\n",
      "[336/00263] train_loss: 0.012324\n",
      "[336/00313] train_loss: 0.011599\n",
      "[336/00363] train_loss: 0.010829\n",
      "[336/00413] train_loss: 0.011950\n",
      "[336/00463] train_loss: 0.011465\n",
      "[336/00513] train_loss: 0.011820\n",
      "[336/00563] train_loss: 0.012638\n",
      "[336/00613] train_loss: 0.010940\n",
      "[336/00663] train_loss: 0.011830\n",
      "[336/00713] train_loss: 0.011511\n",
      "[336/00763] train_loss: 0.011689\n",
      "[336/00813] train_loss: 0.012054\n",
      "[336/00863] train_loss: 0.011930\n",
      "[336/00913] train_loss: 0.010950\n",
      "[336/00963] train_loss: 0.011422\n",
      "[336/01013] train_loss: 0.012422\n",
      "[336/01063] train_loss: 0.010946\n",
      "[336/01113] train_loss: 0.012207\n",
      "[336/01163] train_loss: 0.011337\n",
      "[336/01213] train_loss: 0.010650\n",
      "[337/00037] train_loss: 0.012099\n",
      "[337/00087] train_loss: 0.011893\n",
      "[337/00137] train_loss: 0.012382\n",
      "[337/00187] train_loss: 0.011928\n",
      "[337/00237] train_loss: 0.011220\n",
      "[337/00287] train_loss: 0.011658\n",
      "[337/00337] train_loss: 0.012815\n",
      "[337/00387] train_loss: 0.010910\n",
      "[337/00437] train_loss: 0.011991\n",
      "[337/00487] train_loss: 0.011608\n",
      "[337/00537] train_loss: 0.012234\n",
      "[337/00587] train_loss: 0.011752\n",
      "[337/00637] train_loss: 0.011781\n",
      "[337/00687] train_loss: 0.012061\n",
      "[337/00737] train_loss: 0.011295\n",
      "[337/00787] train_loss: 0.011991\n",
      "[337/00837] train_loss: 0.011389\n",
      "[337/00887] train_loss: 0.011235\n",
      "[337/00937] train_loss: 0.011905\n",
      "[337/00987] train_loss: 0.011753\n",
      "[337/01037] train_loss: 0.011324\n",
      "[337/01087] train_loss: 0.011877\n",
      "[337/01137] train_loss: 0.011897\n",
      "[337/01187] train_loss: 0.011560\n",
      "[338/00011] train_loss: 0.012812\n",
      "[338/00061] train_loss: 0.011758\n",
      "[338/00111] train_loss: 0.012415\n",
      "[338/00161] train_loss: 0.011718\n",
      "[338/00211] train_loss: 0.012561\n",
      "[338/00261] train_loss: 0.011913\n",
      "[338/00311] train_loss: 0.011513\n",
      "[338/00361] train_loss: 0.012209\n",
      "[338/00411] train_loss: 0.010938\n",
      "[338/00461] train_loss: 0.012427\n",
      "[338/00511] train_loss: 0.011766\n",
      "[338/00561] train_loss: 0.011930\n",
      "[338/00611] train_loss: 0.011800\n",
      "[338/00661] train_loss: 0.012633\n",
      "[338/00711] train_loss: 0.011217\n",
      "[338/00761] train_loss: 0.011604\n",
      "[338/00811] train_loss: 0.011229\n",
      "[338/00861] train_loss: 0.011394\n",
      "[338/00911] train_loss: 0.010965\n",
      "[338/00961] train_loss: 0.011762\n",
      "[338/01011] train_loss: 0.011852\n",
      "[338/01061] train_loss: 0.011112\n",
      "[338/01111] train_loss: 0.012143\n",
      "[338/01161] train_loss: 0.011434\n",
      "[338/01211] train_loss: 0.011386\n",
      "[339/00035] train_loss: 0.012364\n",
      "[339/00085] train_loss: 0.011858\n",
      "[339/00135] train_loss: 0.012219\n",
      "[339/00185] train_loss: 0.011767\n",
      "[339/00235] train_loss: 0.012168\n",
      "[339/00285] train_loss: 0.011647\n",
      "[339/00335] train_loss: 0.011680\n",
      "[339/00385] train_loss: 0.011807\n",
      "[339/00435] train_loss: 0.011667\n",
      "[339/00485] train_loss: 0.011358\n",
      "[339/00535] train_loss: 0.012292\n",
      "[339/00585] train_loss: 0.012052\n",
      "[339/00635] train_loss: 0.011599\n",
      "[339/00685] train_loss: 0.011201\n",
      "[339/00735] train_loss: 0.011611\n",
      "[339/00785] train_loss: 0.011915\n",
      "[339/00835] train_loss: 0.011586\n",
      "[339/00885] train_loss: 0.011595\n",
      "[339/00935] train_loss: 0.011723\n",
      "[339/00985] train_loss: 0.011761\n",
      "[339/01035] train_loss: 0.011477\n",
      "[339/01085] train_loss: 0.010969\n",
      "[339/01135] train_loss: 0.011667\n",
      "[339/01185] train_loss: 0.011561\n",
      "[340/00009] train_loss: 0.011290\n",
      "[340/00059] train_loss: 0.012378\n",
      "[340/00109] train_loss: 0.011941\n",
      "[340/00159] train_loss: 0.012230\n",
      "[340/00209] train_loss: 0.012484\n",
      "[340/00259] train_loss: 0.012514\n",
      "[340/00309] train_loss: 0.011909\n",
      "[340/00359] train_loss: 0.012385\n",
      "[340/00409] train_loss: 0.011256\n",
      "[340/00459] train_loss: 0.011791\n",
      "[340/00509] train_loss: 0.012162\n",
      "[340/00559] train_loss: 0.011082\n",
      "[340/00609] train_loss: 0.011993\n",
      "[340/00659] train_loss: 0.011848\n",
      "[340/00709] train_loss: 0.011541\n",
      "[340/00759] train_loss: 0.011757\n",
      "[340/00809] train_loss: 0.011406\n",
      "[340/00859] train_loss: 0.012158\n",
      "[340/00909] train_loss: 0.011886\n",
      "[340/00959] train_loss: 0.011502\n",
      "[340/01009] train_loss: 0.011507\n",
      "[340/01059] train_loss: 0.011248\n",
      "[340/01109] train_loss: 0.011437\n",
      "[340/01159] train_loss: 0.012213\n",
      "[340/01209] train_loss: 0.011301\n",
      "[341/00033] train_loss: 0.012377\n",
      "[341/00083] train_loss: 0.012904\n",
      "[341/00133] train_loss: 0.011792\n",
      "[341/00183] train_loss: 0.011861\n",
      "[341/00233] train_loss: 0.011886\n",
      "[341/00283] train_loss: 0.011454\n",
      "[341/00333] train_loss: 0.012080\n",
      "[341/00383] train_loss: 0.011505\n",
      "[341/00433] train_loss: 0.011675\n",
      "[341/00483] train_loss: 0.011387\n",
      "[341/00533] train_loss: 0.011814\n",
      "[341/00583] train_loss: 0.012032\n",
      "[341/00633] train_loss: 0.011402\n",
      "[341/00683] train_loss: 0.011573\n",
      "[341/00733] train_loss: 0.011882\n",
      "[341/00783] train_loss: 0.011866\n",
      "[341/00833] train_loss: 0.011226\n",
      "[341/00883] train_loss: 0.011650\n",
      "[341/00933] train_loss: 0.011276\n",
      "[341/00983] train_loss: 0.011506\n",
      "[341/01033] train_loss: 0.011960\n",
      "[341/01083] train_loss: 0.011552\n",
      "[341/01133] train_loss: 0.011383\n",
      "[341/01183] train_loss: 0.011117\n",
      "[342/00007] train_loss: 0.011315\n",
      "[342/00057] train_loss: 0.011779\n",
      "[342/00107] train_loss: 0.011848\n",
      "[342/00157] train_loss: 0.011913\n",
      "[342/00207] train_loss: 0.011959\n",
      "[342/00257] train_loss: 0.011324\n",
      "[342/00307] train_loss: 0.011888\n",
      "[342/00357] train_loss: 0.011420\n",
      "[342/00407] train_loss: 0.011379\n",
      "[342/00457] train_loss: 0.011719\n",
      "[342/00507] train_loss: 0.011973\n",
      "[342/00557] train_loss: 0.011551\n",
      "[342/00607] train_loss: 0.012053\n",
      "[342/00657] train_loss: 0.012433\n",
      "[342/00707] train_loss: 0.011445\n",
      "[342/00757] train_loss: 0.011442\n",
      "[342/00807] train_loss: 0.011325\n",
      "[342/00857] train_loss: 0.011753\n",
      "[342/00907] train_loss: 0.011589\n",
      "[342/00957] train_loss: 0.012263\n",
      "[342/01007] train_loss: 0.012248\n",
      "[342/01057] train_loss: 0.011401\n",
      "[342/01107] train_loss: 0.011598\n",
      "[342/01157] train_loss: 0.011855\n",
      "[342/01207] train_loss: 0.011241\n",
      "[343/00031] train_loss: 0.011700\n",
      "[343/00081] train_loss: 0.012637\n",
      "[343/00131] train_loss: 0.011280\n",
      "[343/00181] train_loss: 0.012082\n",
      "[343/00231] train_loss: 0.012788\n",
      "[343/00281] train_loss: 0.011502\n",
      "[343/00331] train_loss: 0.012042\n",
      "[343/00381] train_loss: 0.012140\n",
      "[343/00431] train_loss: 0.011870\n",
      "[343/00481] train_loss: 0.011761\n",
      "[343/00531] train_loss: 0.011485\n",
      "[343/00581] train_loss: 0.011039\n",
      "[343/00631] train_loss: 0.012178\n",
      "[343/00681] train_loss: 0.011289\n",
      "[343/00731] train_loss: 0.011476\n",
      "[343/00781] train_loss: 0.011292\n",
      "[343/00831] train_loss: 0.012192\n",
      "[343/00881] train_loss: 0.011164\n",
      "[343/00931] train_loss: 0.011337\n",
      "[343/00981] train_loss: 0.012098\n",
      "[343/01031] train_loss: 0.011436\n",
      "[343/01081] train_loss: 0.011592\n",
      "[343/01131] train_loss: 0.010970\n",
      "[343/01181] train_loss: 0.011424\n",
      "[344/00005] train_loss: 0.010938\n",
      "[344/00055] train_loss: 0.012515\n",
      "[344/00105] train_loss: 0.012660\n",
      "[344/00155] train_loss: 0.011152\n",
      "[344/00205] train_loss: 0.011605\n",
      "[344/00255] train_loss: 0.012193\n",
      "[344/00305] train_loss: 0.011904\n",
      "[344/00355] train_loss: 0.011645\n",
      "[344/00405] train_loss: 0.012254\n",
      "[344/00455] train_loss: 0.011523\n",
      "[344/00505] train_loss: 0.012205\n",
      "[344/00555] train_loss: 0.011916\n",
      "[344/00605] train_loss: 0.011107\n",
      "[344/00655] train_loss: 0.011727\n",
      "[344/00705] train_loss: 0.011769\n",
      "[344/00755] train_loss: 0.011676\n",
      "[344/00805] train_loss: 0.011532\n",
      "[344/00855] train_loss: 0.011990\n",
      "[344/00905] train_loss: 0.011265\n",
      "[344/00955] train_loss: 0.012190\n",
      "[344/01005] train_loss: 0.011311\n",
      "[344/01055] train_loss: 0.010789\n",
      "[344/01105] train_loss: 0.011761\n",
      "[344/01155] train_loss: 0.011062\n",
      "[344/01205] train_loss: 0.011321\n",
      "[345/00029] train_loss: 0.012281\n",
      "[345/00079] train_loss: 0.012809\n",
      "[345/00129] train_loss: 0.012476\n",
      "[345/00179] train_loss: 0.011566\n",
      "[345/00229] train_loss: 0.011889\n",
      "[345/00279] train_loss: 0.012024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[345/00329] train_loss: 0.011827\n",
      "[345/00379] train_loss: 0.011438\n",
      "[345/00429] train_loss: 0.011312\n",
      "[345/00479] train_loss: 0.010820\n",
      "[345/00529] train_loss: 0.011797\n",
      "[345/00579] train_loss: 0.012196\n",
      "[345/00629] train_loss: 0.011759\n",
      "[345/00679] train_loss: 0.011701\n",
      "[345/00729] train_loss: 0.011767\n",
      "[345/00779] train_loss: 0.011462\n",
      "[345/00829] train_loss: 0.011685\n",
      "[345/00879] train_loss: 0.011558\n",
      "[345/00929] train_loss: 0.011210\n",
      "[345/00979] train_loss: 0.011712\n",
      "[345/01029] train_loss: 0.011812\n",
      "[345/01079] train_loss: 0.011170\n",
      "[345/01129] train_loss: 0.011844\n",
      "[345/01179] train_loss: 0.011572\n",
      "[346/00003] train_loss: 0.011287\n",
      "[346/00053] train_loss: 0.012654\n",
      "[346/00103] train_loss: 0.012464\n",
      "[346/00153] train_loss: 0.012185\n",
      "[346/00203] train_loss: 0.011487\n",
      "[346/00253] train_loss: 0.011226\n",
      "[346/00303] train_loss: 0.010972\n",
      "[346/00353] train_loss: 0.012267\n",
      "[346/00403] train_loss: 0.012019\n",
      "[346/00453] train_loss: 0.011976\n",
      "[346/00503] train_loss: 0.011167\n",
      "[346/00553] train_loss: 0.011686\n",
      "[346/00603] train_loss: 0.011566\n",
      "[346/00653] train_loss: 0.011874\n",
      "[346/00703] train_loss: 0.011738\n",
      "[346/00753] train_loss: 0.011732\n",
      "[346/00803] train_loss: 0.011865\n",
      "[346/00853] train_loss: 0.011639\n",
      "[346/00903] train_loss: 0.011471\n",
      "[346/00953] train_loss: 0.011875\n",
      "[346/01003] train_loss: 0.011632\n",
      "[346/01053] train_loss: 0.011283\n",
      "[346/01103] train_loss: 0.010807\n",
      "[346/01153] train_loss: 0.011816\n",
      "[346/01203] train_loss: 0.011071\n",
      "[347/00027] train_loss: 0.012525\n",
      "[347/00077] train_loss: 0.012428\n",
      "[347/00127] train_loss: 0.011685\n",
      "[347/00177] train_loss: 0.011274\n",
      "[347/00227] train_loss: 0.011946\n",
      "[347/00277] train_loss: 0.012233\n",
      "[347/00327] train_loss: 0.011655\n",
      "[347/00377] train_loss: 0.011232\n",
      "[347/00427] train_loss: 0.012079\n",
      "[347/00477] train_loss: 0.011315\n",
      "[347/00527] train_loss: 0.012022\n",
      "[347/00577] train_loss: 0.011477\n",
      "[347/00627] train_loss: 0.011248\n",
      "[347/00677] train_loss: 0.011380\n",
      "[347/00727] train_loss: 0.012085\n",
      "[347/00777] train_loss: 0.011776\n",
      "[347/00827] train_loss: 0.010839\n",
      "[347/00877] train_loss: 0.011200\n",
      "[347/00927] train_loss: 0.012566\n",
      "[347/00977] train_loss: 0.011303\n",
      "[347/01027] train_loss: 0.011808\n",
      "[347/01077] train_loss: 0.012180\n",
      "[347/01127] train_loss: 0.011284\n",
      "[347/01177] train_loss: 0.012271\n",
      "[348/00001] train_loss: 0.011616\n",
      "[348/00051] train_loss: 0.011763\n",
      "[348/00101] train_loss: 0.012624\n",
      "[348/00151] train_loss: 0.011758\n",
      "[348/00201] train_loss: 0.011566\n",
      "[348/00251] train_loss: 0.012030\n",
      "[348/00301] train_loss: 0.011557\n",
      "[348/00351] train_loss: 0.011500\n",
      "[348/00401] train_loss: 0.012721\n",
      "[348/00451] train_loss: 0.012021\n",
      "[348/00501] train_loss: 0.011691\n",
      "[348/00551] train_loss: 0.011006\n",
      "[348/00601] train_loss: 0.011208\n",
      "[348/00651] train_loss: 0.011019\n",
      "[348/00701] train_loss: 0.011826\n",
      "[348/00751] train_loss: 0.011431\n",
      "[348/00801] train_loss: 0.011870\n",
      "[348/00851] train_loss: 0.012143\n",
      "[348/00901] train_loss: 0.011638\n",
      "[348/00951] train_loss: 0.012286\n",
      "[348/01001] train_loss: 0.012190\n",
      "[348/01051] train_loss: 0.011727\n",
      "[348/01101] train_loss: 0.010916\n",
      "[348/01151] train_loss: 0.011374\n",
      "[348/01201] train_loss: 0.011088\n",
      "[349/00025] train_loss: 0.011980\n",
      "[349/00075] train_loss: 0.012096\n",
      "[349/00125] train_loss: 0.012348\n",
      "[349/00175] train_loss: 0.011547\n",
      "[349/00225] train_loss: 0.011114\n",
      "[349/00275] train_loss: 0.012359\n",
      "[349/00325] train_loss: 0.011770\n",
      "[349/00375] train_loss: 0.012245\n",
      "[349/00425] train_loss: 0.011436\n",
      "[349/00475] train_loss: 0.012125\n",
      "[349/00525] train_loss: 0.011863\n",
      "[349/00575] train_loss: 0.011843\n",
      "[349/00625] train_loss: 0.011860\n",
      "[349/00675] train_loss: 0.011393\n",
      "[349/00725] train_loss: 0.011765\n",
      "[349/00775] train_loss: 0.011734\n",
      "[349/00825] train_loss: 0.011411\n",
      "[349/00875] train_loss: 0.011483\n",
      "[349/00925] train_loss: 0.011539\n",
      "[349/00975] train_loss: 0.010815\n",
      "[349/01025] train_loss: 0.012138\n",
      "[349/01075] train_loss: 0.011582\n",
      "[349/01125] train_loss: 0.011520\n",
      "[349/01175] train_loss: 0.011817\n",
      "[349/01225] train_loss: 0.012191\n",
      "[350/00049] train_loss: 0.011938\n",
      "[350/00099] train_loss: 0.012354\n",
      "[350/00149] train_loss: 0.013225\n",
      "[350/00199] train_loss: 0.011127\n",
      "[350/00249] train_loss: 0.011863\n",
      "[350/00299] train_loss: 0.012593\n",
      "[350/00349] train_loss: 0.011615\n",
      "[350/00399] train_loss: 0.011234\n",
      "[350/00449] train_loss: 0.011059\n",
      "[350/00499] train_loss: 0.011729\n",
      "[350/00549] train_loss: 0.011081\n",
      "[350/00599] train_loss: 0.010950\n",
      "[350/00649] train_loss: 0.011338\n",
      "[350/00699] train_loss: 0.012038\n",
      "[350/00749] train_loss: 0.011197\n",
      "[350/00799] train_loss: 0.010909\n",
      "[350/00849] train_loss: 0.011203\n",
      "[350/00899] train_loss: 0.011502\n",
      "[350/00949] train_loss: 0.011873\n",
      "[350/00999] train_loss: 0.011899\n",
      "[350/01049] train_loss: 0.012003\n",
      "[350/01099] train_loss: 0.011853\n",
      "[350/01149] train_loss: 0.011776\n",
      "[350/01199] train_loss: 0.010993\n",
      "[351/00023] train_loss: 0.011531\n",
      "[351/00073] train_loss: 0.012210\n",
      "[351/00123] train_loss: 0.011703\n",
      "[351/00173] train_loss: 0.012073\n",
      "[351/00223] train_loss: 0.012280\n",
      "[351/00273] train_loss: 0.012389\n",
      "[351/00323] train_loss: 0.011945\n",
      "[351/00373] train_loss: 0.010925\n",
      "[351/00423] train_loss: 0.011501\n",
      "[351/00473] train_loss: 0.011280\n",
      "[351/00523] train_loss: 0.012152\n",
      "[351/00573] train_loss: 0.011427\n",
      "[351/00623] train_loss: 0.011452\n",
      "[351/00673] train_loss: 0.011371\n",
      "[351/00723] train_loss: 0.011691\n",
      "[351/00773] train_loss: 0.011058\n",
      "[351/00823] train_loss: 0.011061\n",
      "[351/00873] train_loss: 0.011609\n",
      "[351/00923] train_loss: 0.011433\n",
      "[351/00973] train_loss: 0.011629\n",
      "[351/01023] train_loss: 0.011880\n",
      "[351/01073] train_loss: 0.011721\n",
      "[351/01123] train_loss: 0.011709\n",
      "[351/01173] train_loss: 0.011453\n",
      "[351/01223] train_loss: 0.011620\n",
      "[352/00047] train_loss: 0.012760\n",
      "[352/00097] train_loss: 0.012195\n",
      "[352/00147] train_loss: 0.012131\n",
      "[352/00197] train_loss: 0.012533\n",
      "[352/00247] train_loss: 0.011336\n",
      "[352/00297] train_loss: 0.011513\n",
      "[352/00347] train_loss: 0.012065\n",
      "[352/00397] train_loss: 0.010977\n",
      "[352/00447] train_loss: 0.011751\n",
      "[352/00497] train_loss: 0.011740\n",
      "[352/00547] train_loss: 0.011486\n",
      "[352/00597] train_loss: 0.011605\n",
      "[352/00647] train_loss: 0.011418\n",
      "[352/00697] train_loss: 0.011955\n",
      "[352/00747] train_loss: 0.011538\n",
      "[352/00797] train_loss: 0.012108\n",
      "[352/00847] train_loss: 0.011546\n",
      "[352/00897] train_loss: 0.011466\n",
      "[352/00947] train_loss: 0.011178\n",
      "[352/00997] train_loss: 0.011934\n",
      "[352/01047] train_loss: 0.011198\n",
      "[352/01097] train_loss: 0.011578\n",
      "[352/01147] train_loss: 0.010858\n",
      "[352/01197] train_loss: 0.011342\n",
      "[353/00021] train_loss: 0.011663\n",
      "[353/00071] train_loss: 0.012076\n",
      "[353/00121] train_loss: 0.011984\n",
      "[353/00171] train_loss: 0.011622\n",
      "[353/00221] train_loss: 0.011440\n",
      "[353/00271] train_loss: 0.011840\n",
      "[353/00321] train_loss: 0.011814\n",
      "[353/00371] train_loss: 0.011345\n",
      "[353/00421] train_loss: 0.011594\n",
      "[353/00471] train_loss: 0.011293\n",
      "[353/00521] train_loss: 0.012148\n",
      "[353/00571] train_loss: 0.011732\n",
      "[353/00621] train_loss: 0.010887\n",
      "[353/00671] train_loss: 0.012022\n",
      "[353/00721] train_loss: 0.011508\n",
      "[353/00771] train_loss: 0.011684\n",
      "[353/00821] train_loss: 0.011598\n",
      "[353/00871] train_loss: 0.011613\n",
      "[353/00921] train_loss: 0.012072\n",
      "[353/00971] train_loss: 0.011562\n",
      "[353/01021] train_loss: 0.011555\n",
      "[353/01071] train_loss: 0.011357\n",
      "[353/01121] train_loss: 0.010420\n",
      "[353/01171] train_loss: 0.012168\n",
      "[353/01221] train_loss: 0.011330\n",
      "[354/00045] train_loss: 0.012273\n",
      "[354/00095] train_loss: 0.011968\n",
      "[354/00145] train_loss: 0.011441\n",
      "[354/00195] train_loss: 0.012277\n",
      "[354/00245] train_loss: 0.011609\n",
      "[354/00295] train_loss: 0.011522\n",
      "[354/00345] train_loss: 0.012329\n",
      "[354/00395] train_loss: 0.011575\n",
      "[354/00445] train_loss: 0.012111\n",
      "[354/00495] train_loss: 0.011463\n",
      "[354/00545] train_loss: 0.011421\n",
      "[354/00595] train_loss: 0.012141\n",
      "[354/00645] train_loss: 0.011703\n",
      "[354/00695] train_loss: 0.011101\n",
      "[354/00745] train_loss: 0.011476\n",
      "[354/00795] train_loss: 0.011055\n",
      "[354/00845] train_loss: 0.011466\n",
      "[354/00895] train_loss: 0.011710\n",
      "[354/00945] train_loss: 0.011652\n",
      "[354/00995] train_loss: 0.011649\n",
      "[354/01045] train_loss: 0.011293\n",
      "[354/01095] train_loss: 0.011697\n",
      "[354/01145] train_loss: 0.011957\n",
      "[354/01195] train_loss: 0.011207\n",
      "[355/00019] train_loss: 0.011847\n",
      "[355/00069] train_loss: 0.012424\n",
      "[355/00119] train_loss: 0.012622\n",
      "[355/00169] train_loss: 0.011975\n",
      "[355/00219] train_loss: 0.011566\n",
      "[355/00269] train_loss: 0.011958\n",
      "[355/00319] train_loss: 0.011437\n",
      "[355/00369] train_loss: 0.011163\n",
      "[355/00419] train_loss: 0.011707\n",
      "[355/00469] train_loss: 0.010481\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[355/00519] train_loss: 0.011736\n",
      "[355/00569] train_loss: 0.011546\n",
      "[355/00619] train_loss: 0.011715\n",
      "[355/00669] train_loss: 0.011344\n",
      "[355/00719] train_loss: 0.012105\n",
      "[355/00769] train_loss: 0.011941\n",
      "[355/00819] train_loss: 0.011283\n",
      "[355/00869] train_loss: 0.011338\n",
      "[355/00919] train_loss: 0.011635\n",
      "[355/00969] train_loss: 0.011009\n",
      "[355/01019] train_loss: 0.010563\n",
      "[355/01069] train_loss: 0.011748\n",
      "[355/01119] train_loss: 0.011736\n",
      "[355/01169] train_loss: 0.012329\n",
      "[355/01219] train_loss: 0.011602\n",
      "[356/00043] train_loss: 0.012230\n",
      "[356/00093] train_loss: 0.012463\n",
      "[356/00143] train_loss: 0.012341\n",
      "[356/00193] train_loss: 0.010695\n",
      "[356/00243] train_loss: 0.011767\n",
      "[356/00293] train_loss: 0.011411\n",
      "[356/00343] train_loss: 0.012445\n",
      "[356/00393] train_loss: 0.012185\n",
      "[356/00443] train_loss: 0.012199\n",
      "[356/00493] train_loss: 0.012032\n",
      "[356/00543] train_loss: 0.011478\n",
      "[356/00593] train_loss: 0.010797\n",
      "[356/00643] train_loss: 0.011401\n",
      "[356/00693] train_loss: 0.011247\n",
      "[356/00743] train_loss: 0.011616\n",
      "[356/00793] train_loss: 0.011956\n",
      "[356/00843] train_loss: 0.011582\n",
      "[356/00893] train_loss: 0.011582\n",
      "[356/00943] train_loss: 0.012147\n",
      "[356/00993] train_loss: 0.011325\n",
      "[356/01043] train_loss: 0.011653\n",
      "[356/01093] train_loss: 0.011347\n",
      "[356/01143] train_loss: 0.011790\n",
      "[356/01193] train_loss: 0.011444\n",
      "[357/00017] train_loss: 0.011446\n",
      "[357/00067] train_loss: 0.012007\n",
      "[357/00117] train_loss: 0.012387\n",
      "[357/00167] train_loss: 0.012613\n",
      "[357/00217] train_loss: 0.012024\n",
      "[357/00267] train_loss: 0.011520\n",
      "[357/00317] train_loss: 0.010816\n",
      "[357/00367] train_loss: 0.011083\n",
      "[357/00417] train_loss: 0.011687\n",
      "[357/00467] train_loss: 0.012313\n",
      "[357/00517] train_loss: 0.011573\n",
      "[357/00567] train_loss: 0.011634\n",
      "[357/00617] train_loss: 0.011485\n",
      "[357/00667] train_loss: 0.011394\n",
      "[357/00717] train_loss: 0.011876\n",
      "[357/00767] train_loss: 0.011537\n",
      "[357/00817] train_loss: 0.011678\n",
      "[357/00867] train_loss: 0.010980\n",
      "[357/00917] train_loss: 0.011349\n",
      "[357/00967] train_loss: 0.011393\n",
      "[357/01017] train_loss: 0.011415\n",
      "[357/01067] train_loss: 0.011666\n",
      "[357/01117] train_loss: 0.012163\n",
      "[357/01167] train_loss: 0.011619\n",
      "[357/01217] train_loss: 0.011198\n",
      "[358/00041] train_loss: 0.011892\n",
      "[358/00091] train_loss: 0.012458\n",
      "[358/00141] train_loss: 0.011673\n",
      "[358/00191] train_loss: 0.011073\n",
      "[358/00241] train_loss: 0.011988\n",
      "[358/00291] train_loss: 0.011872\n",
      "[358/00341] train_loss: 0.011925\n",
      "[358/00391] train_loss: 0.011786\n",
      "[358/00441] train_loss: 0.011760\n",
      "[358/00491] train_loss: 0.011726\n",
      "[358/00541] train_loss: 0.011743\n",
      "[358/00591] train_loss: 0.011338\n",
      "[358/00641] train_loss: 0.011605\n",
      "[358/00691] train_loss: 0.010792\n",
      "[358/00741] train_loss: 0.011732\n",
      "[358/00791] train_loss: 0.011552\n",
      "[358/00841] train_loss: 0.011546\n",
      "[358/00891] train_loss: 0.011637\n",
      "[358/00941] train_loss: 0.011429\n",
      "[358/00991] train_loss: 0.011216\n",
      "[358/01041] train_loss: 0.011886\n",
      "[358/01091] train_loss: 0.010978\n",
      "[358/01141] train_loss: 0.011247\n",
      "[358/01191] train_loss: 0.011725\n",
      "[359/00015] train_loss: 0.012824\n",
      "[359/00065] train_loss: 0.011574\n",
      "[359/00115] train_loss: 0.011862\n",
      "[359/00165] train_loss: 0.011977\n",
      "[359/00215] train_loss: 0.011579\n",
      "[359/00265] train_loss: 0.011209\n",
      "[359/00315] train_loss: 0.011529\n",
      "[359/00365] train_loss: 0.011850\n",
      "[359/00415] train_loss: 0.011724\n",
      "[359/00465] train_loss: 0.011547\n",
      "[359/00515] train_loss: 0.012287\n",
      "[359/00565] train_loss: 0.011440\n",
      "[359/00615] train_loss: 0.011467\n",
      "[359/00665] train_loss: 0.011523\n",
      "[359/00715] train_loss: 0.011782\n",
      "[359/00765] train_loss: 0.011520\n",
      "[359/00815] train_loss: 0.011462\n",
      "[359/00865] train_loss: 0.011799\n",
      "[359/00915] train_loss: 0.011761\n",
      "[359/00965] train_loss: 0.011602\n",
      "[359/01015] train_loss: 0.010938\n",
      "[359/01065] train_loss: 0.011173\n",
      "[359/01115] train_loss: 0.011572\n",
      "[359/01165] train_loss: 0.011571\n",
      "[359/01215] train_loss: 0.011159\n",
      "[360/00039] train_loss: 0.011304\n",
      "[360/00089] train_loss: 0.012155\n",
      "[360/00139] train_loss: 0.011773\n",
      "[360/00189] train_loss: 0.012198\n",
      "[360/00239] train_loss: 0.011522\n",
      "[360/00289] train_loss: 0.011429\n",
      "[360/00339] train_loss: 0.011549\n",
      "[360/00389] train_loss: 0.011856\n",
      "[360/00439] train_loss: 0.012244\n",
      "[360/00489] train_loss: 0.010932\n",
      "[360/00539] train_loss: 0.012043\n",
      "[360/00589] train_loss: 0.011779\n",
      "[360/00639] train_loss: 0.011180\n",
      "[360/00689] train_loss: 0.012175\n",
      "[360/00739] train_loss: 0.011381\n",
      "[360/00789] train_loss: 0.011470\n",
      "[360/00839] train_loss: 0.011331\n",
      "[360/00889] train_loss: 0.011219\n",
      "[360/00939] train_loss: 0.011073\n",
      "[360/00989] train_loss: 0.011836\n",
      "[360/01039] train_loss: 0.011408\n",
      "[360/01089] train_loss: 0.011336\n",
      "[360/01139] train_loss: 0.011183\n",
      "[360/01189] train_loss: 0.011482\n",
      "[361/00013] train_loss: 0.011492\n",
      "[361/00063] train_loss: 0.012686\n",
      "[361/00113] train_loss: 0.012353\n",
      "[361/00163] train_loss: 0.012017\n",
      "[361/00213] train_loss: 0.011010\n",
      "[361/00263] train_loss: 0.011523\n",
      "[361/00313] train_loss: 0.010936\n",
      "[361/00363] train_loss: 0.011934\n",
      "[361/00413] train_loss: 0.011723\n",
      "[361/00463] train_loss: 0.011486\n",
      "[361/00513] train_loss: 0.011773\n",
      "[361/00563] train_loss: 0.011640\n",
      "[361/00613] train_loss: 0.011924\n",
      "[361/00663] train_loss: 0.011272\n",
      "[361/00713] train_loss: 0.011767\n",
      "[361/00763] train_loss: 0.011094\n",
      "[361/00813] train_loss: 0.012611\n",
      "[361/00863] train_loss: 0.011190\n",
      "[361/00913] train_loss: 0.011277\n",
      "[361/00963] train_loss: 0.011528\n",
      "[361/01013] train_loss: 0.011363\n",
      "[361/01063] train_loss: 0.011370\n",
      "[361/01113] train_loss: 0.011789\n",
      "[361/01163] train_loss: 0.010800\n",
      "[361/01213] train_loss: 0.011710\n",
      "[362/00037] train_loss: 0.012126\n",
      "[362/00087] train_loss: 0.011488\n",
      "[362/00137] train_loss: 0.011263\n",
      "[362/00187] train_loss: 0.011525\n",
      "[362/00237] train_loss: 0.011507\n",
      "[362/00287] train_loss: 0.011552\n",
      "[362/00337] train_loss: 0.011646\n",
      "[362/00387] train_loss: 0.011292\n",
      "[362/00437] train_loss: 0.011725\n",
      "[362/00487] train_loss: 0.011511\n",
      "[362/00537] train_loss: 0.012289\n",
      "[362/00587] train_loss: 0.011536\n",
      "[362/00637] train_loss: 0.010888\n",
      "[362/00687] train_loss: 0.011651\n",
      "[362/00737] train_loss: 0.011706\n",
      "[362/00787] train_loss: 0.011687\n",
      "[362/00837] train_loss: 0.011946\n",
      "[362/00887] train_loss: 0.011628\n",
      "[362/00937] train_loss: 0.011939\n",
      "[362/00987] train_loss: 0.011982\n",
      "[362/01037] train_loss: 0.010709\n",
      "[362/01087] train_loss: 0.011045\n",
      "[362/01137] train_loss: 0.011391\n",
      "[362/01187] train_loss: 0.011558\n",
      "[363/00011] train_loss: 0.011638\n",
      "[363/00061] train_loss: 0.012333\n",
      "[363/00111] train_loss: 0.012048\n",
      "[363/00161] train_loss: 0.011981\n",
      "[363/00211] train_loss: 0.011814\n",
      "[363/00261] train_loss: 0.011632\n",
      "[363/00311] train_loss: 0.012024\n",
      "[363/00361] train_loss: 0.011159\n",
      "[363/00411] train_loss: 0.011560\n",
      "[363/00461] train_loss: 0.012034\n",
      "[363/00511] train_loss: 0.011073\n",
      "[363/00561] train_loss: 0.011389\n",
      "[363/00611] train_loss: 0.011467\n",
      "[363/00661] train_loss: 0.011273\n",
      "[363/00711] train_loss: 0.011128\n",
      "[363/00761] train_loss: 0.012708\n",
      "[363/00811] train_loss: 0.012199\n",
      "[363/00861] train_loss: 0.011752\n",
      "[363/00911] train_loss: 0.011581\n",
      "[363/00961] train_loss: 0.010950\n",
      "[363/01011] train_loss: 0.011105\n",
      "[363/01061] train_loss: 0.011610\n",
      "[363/01111] train_loss: 0.011362\n",
      "[363/01161] train_loss: 0.011155\n",
      "[363/01211] train_loss: 0.011018\n",
      "[364/00035] train_loss: 0.012938\n",
      "[364/00085] train_loss: 0.012142\n",
      "[364/00135] train_loss: 0.011856\n",
      "[364/00185] train_loss: 0.011693\n",
      "[364/00235] train_loss: 0.011182\n",
      "[364/00285] train_loss: 0.011684\n",
      "[364/00335] train_loss: 0.011484\n",
      "[364/00385] train_loss: 0.011802\n",
      "[364/00435] train_loss: 0.012444\n",
      "[364/00485] train_loss: 0.011763\n",
      "[364/00535] train_loss: 0.011179\n",
      "[364/00585] train_loss: 0.011468\n",
      "[364/00635] train_loss: 0.011715\n",
      "[364/00685] train_loss: 0.011140\n",
      "[364/00735] train_loss: 0.011815\n",
      "[364/00785] train_loss: 0.011929\n",
      "[364/00835] train_loss: 0.012108\n",
      "[364/00885] train_loss: 0.011378\n",
      "[364/00935] train_loss: 0.012317\n",
      "[364/00985] train_loss: 0.011695\n",
      "[364/01035] train_loss: 0.011132\n",
      "[364/01085] train_loss: 0.011353\n",
      "[364/01135] train_loss: 0.011900\n",
      "[364/01185] train_loss: 0.011417\n",
      "[365/00009] train_loss: 0.011807\n",
      "[365/00059] train_loss: 0.012119\n",
      "[365/00109] train_loss: 0.011909\n",
      "[365/00159] train_loss: 0.011824\n",
      "[365/00209] train_loss: 0.011534\n",
      "[365/00259] train_loss: 0.010976\n",
      "[365/00309] train_loss: 0.011371\n",
      "[365/00359] train_loss: 0.011862\n",
      "[365/00409] train_loss: 0.011388\n",
      "[365/00459] train_loss: 0.012284\n",
      "[365/00509] train_loss: 0.011712\n",
      "[365/00559] train_loss: 0.011596\n",
      "[365/00609] train_loss: 0.011148\n",
      "[365/00659] train_loss: 0.011842\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[365/00709] train_loss: 0.011627\n",
      "[365/00759] train_loss: 0.011988\n",
      "[365/00809] train_loss: 0.011788\n",
      "[365/00859] train_loss: 0.011454\n",
      "[365/00909] train_loss: 0.011431\n",
      "[365/00959] train_loss: 0.011339\n",
      "[365/01009] train_loss: 0.011737\n",
      "[365/01059] train_loss: 0.011225\n",
      "[365/01109] train_loss: 0.011684\n",
      "[365/01159] train_loss: 0.011608\n",
      "[365/01209] train_loss: 0.011410\n",
      "[366/00033] train_loss: 0.011881\n",
      "[366/00083] train_loss: 0.012298\n",
      "[366/00133] train_loss: 0.012010\n",
      "[366/00183] train_loss: 0.011900\n",
      "[366/00233] train_loss: 0.011354\n",
      "[366/00283] train_loss: 0.011481\n",
      "[366/00333] train_loss: 0.011803\n",
      "[366/00383] train_loss: 0.012219\n",
      "[366/00433] train_loss: 0.011356\n",
      "[366/00483] train_loss: 0.011435\n",
      "[366/00533] train_loss: 0.011097\n",
      "[366/00583] train_loss: 0.011486\n",
      "[366/00633] train_loss: 0.011148\n",
      "[366/00683] train_loss: 0.010842\n",
      "[366/00733] train_loss: 0.011897\n",
      "[366/00783] train_loss: 0.010758\n",
      "[366/00833] train_loss: 0.012553\n",
      "[366/00883] train_loss: 0.010594\n",
      "[366/00933] train_loss: 0.010959\n",
      "[366/00983] train_loss: 0.011826\n",
      "[366/01033] train_loss: 0.011107\n",
      "[366/01083] train_loss: 0.011026\n",
      "[366/01133] train_loss: 0.011149\n",
      "[366/01183] train_loss: 0.011460\n",
      "[367/00007] train_loss: 0.011850\n",
      "[367/00057] train_loss: 0.012684\n",
      "[367/00107] train_loss: 0.012506\n",
      "[367/00157] train_loss: 0.011863\n",
      "[367/00207] train_loss: 0.011357\n",
      "[367/00257] train_loss: 0.011466\n",
      "[367/00307] train_loss: 0.011550\n",
      "[367/00357] train_loss: 0.011357\n",
      "[367/00407] train_loss: 0.011537\n",
      "[367/00457] train_loss: 0.012202\n",
      "[367/00507] train_loss: 0.011920\n",
      "[367/00557] train_loss: 0.011286\n",
      "[367/00607] train_loss: 0.011899\n",
      "[367/00657] train_loss: 0.011308\n",
      "[367/00707] train_loss: 0.011590\n",
      "[367/00757] train_loss: 0.011071\n",
      "[367/00807] train_loss: 0.011119\n",
      "[367/00857] train_loss: 0.012082\n",
      "[367/00907] train_loss: 0.011346\n",
      "[367/00957] train_loss: 0.011465\n",
      "[367/01007] train_loss: 0.011139\n",
      "[367/01057] train_loss: 0.011923\n",
      "[367/01107] train_loss: 0.011050\n",
      "[367/01157] train_loss: 0.011509\n",
      "[367/01207] train_loss: 0.011973\n",
      "[368/00031] train_loss: 0.011937\n",
      "[368/00081] train_loss: 0.012278\n",
      "[368/00131] train_loss: 0.012136\n",
      "[368/00181] train_loss: 0.011861\n",
      "[368/00231] train_loss: 0.011547\n",
      "[368/00281] train_loss: 0.012053\n",
      "[368/00331] train_loss: 0.011966\n",
      "[368/00381] train_loss: 0.011429\n",
      "[368/00431] train_loss: 0.011562\n",
      "[368/00481] train_loss: 0.011198\n",
      "[368/00531] train_loss: 0.011405\n",
      "[368/00581] train_loss: 0.011089\n",
      "[368/00631] train_loss: 0.011498\n",
      "[368/00681] train_loss: 0.011251\n",
      "[368/00731] train_loss: 0.011168\n",
      "[368/00781] train_loss: 0.011939\n",
      "[368/00831] train_loss: 0.011801\n",
      "[368/00881] train_loss: 0.011763\n",
      "[368/00931] train_loss: 0.011261\n",
      "[368/00981] train_loss: 0.011108\n",
      "[368/01031] train_loss: 0.011695\n",
      "[368/01081] train_loss: 0.011624\n",
      "[368/01131] train_loss: 0.011636\n",
      "[368/01181] train_loss: 0.011681\n",
      "[369/00005] train_loss: 0.011152\n",
      "[369/00055] train_loss: 0.012313\n",
      "[369/00105] train_loss: 0.012086\n",
      "[369/00155] train_loss: 0.011919\n",
      "[369/00205] train_loss: 0.011591\n",
      "[369/00255] train_loss: 0.012153\n",
      "[369/00305] train_loss: 0.011665\n",
      "[369/00355] train_loss: 0.011650\n",
      "[369/00405] train_loss: 0.011849\n",
      "[369/00455] train_loss: 0.011898\n",
      "[369/00505] train_loss: 0.011429\n",
      "[369/00555] train_loss: 0.011476\n",
      "[369/00605] train_loss: 0.011409\n",
      "[369/00655] train_loss: 0.012008\n",
      "[369/00705] train_loss: 0.011983\n",
      "[369/00755] train_loss: 0.011425\n",
      "[369/00805] train_loss: 0.010942\n",
      "[369/00855] train_loss: 0.011446\n",
      "[369/00905] train_loss: 0.011864\n",
      "[369/00955] train_loss: 0.011467\n",
      "[369/01005] train_loss: 0.011457\n",
      "[369/01055] train_loss: 0.011090\n",
      "[369/01105] train_loss: 0.011366\n",
      "[369/01155] train_loss: 0.011017\n",
      "[369/01205] train_loss: 0.011079\n",
      "[370/00029] train_loss: 0.011879\n",
      "[370/00079] train_loss: 0.011749\n",
      "[370/00129] train_loss: 0.011788\n",
      "[370/00179] train_loss: 0.011316\n",
      "[370/00229] train_loss: 0.012036\n",
      "[370/00279] train_loss: 0.010891\n",
      "[370/00329] train_loss: 0.011469\n",
      "[370/00379] train_loss: 0.011724\n",
      "[370/00429] train_loss: 0.011653\n",
      "[370/00479] train_loss: 0.011266\n",
      "[370/00529] train_loss: 0.011459\n",
      "[370/00579] train_loss: 0.011180\n",
      "[370/00629] train_loss: 0.011313\n",
      "[370/00679] train_loss: 0.011348\n",
      "[370/00729] train_loss: 0.011390\n",
      "[370/00779] train_loss: 0.011865\n",
      "[370/00829] train_loss: 0.011887\n",
      "[370/00879] train_loss: 0.011442\n",
      "[370/00929] train_loss: 0.011486\n",
      "[370/00979] train_loss: 0.011067\n",
      "[370/01029] train_loss: 0.012010\n",
      "[370/01079] train_loss: 0.011108\n",
      "[370/01129] train_loss: 0.011389\n",
      "[370/01179] train_loss: 0.011468\n",
      "[371/00003] train_loss: 0.011351\n",
      "[371/00053] train_loss: 0.011657\n",
      "[371/00103] train_loss: 0.011855\n",
      "[371/00153] train_loss: 0.011828\n",
      "[371/00203] train_loss: 0.011910\n",
      "[371/00253] train_loss: 0.011288\n",
      "[371/00303] train_loss: 0.011730\n",
      "[371/00353] train_loss: 0.012040\n",
      "[371/00403] train_loss: 0.011798\n",
      "[371/00453] train_loss: 0.011913\n",
      "[371/00503] train_loss: 0.011536\n",
      "[371/00553] train_loss: 0.011454\n",
      "[371/00603] train_loss: 0.011759\n",
      "[371/00653] train_loss: 0.011770\n",
      "[371/00703] train_loss: 0.012006\n",
      "[371/00753] train_loss: 0.011316\n",
      "[371/00803] train_loss: 0.011027\n",
      "[371/00853] train_loss: 0.010852\n",
      "[371/00903] train_loss: 0.011113\n",
      "[371/00953] train_loss: 0.011564\n",
      "[371/01003] train_loss: 0.012030\n",
      "[371/01053] train_loss: 0.010828\n",
      "[371/01103] train_loss: 0.011534\n",
      "[371/01153] train_loss: 0.011141\n",
      "[371/01203] train_loss: 0.011362\n",
      "[372/00027] train_loss: 0.011491\n",
      "[372/00077] train_loss: 0.011362\n",
      "[372/00127] train_loss: 0.012497\n",
      "[372/00177] train_loss: 0.011937\n",
      "[372/00227] train_loss: 0.011272\n",
      "[372/00277] train_loss: 0.011561\n",
      "[372/00327] train_loss: 0.012186\n",
      "[372/00377] train_loss: 0.011904\n",
      "[372/00427] train_loss: 0.011312\n",
      "[372/00477] train_loss: 0.011305\n",
      "[372/00527] train_loss: 0.011906\n",
      "[372/00577] train_loss: 0.011702\n",
      "[372/00627] train_loss: 0.010915\n",
      "[372/00677] train_loss: 0.011922\n",
      "[372/00727] train_loss: 0.011995\n",
      "[372/00777] train_loss: 0.011431\n",
      "[372/00827] train_loss: 0.010786\n",
      "[372/00877] train_loss: 0.011408\n",
      "[372/00927] train_loss: 0.011949\n",
      "[372/00977] train_loss: 0.011218\n",
      "[372/01027] train_loss: 0.012239\n",
      "[372/01077] train_loss: 0.011560\n",
      "[372/01127] train_loss: 0.011815\n",
      "[372/01177] train_loss: 0.011169\n",
      "[373/00001] train_loss: 0.011448\n",
      "[373/00051] train_loss: 0.012082\n",
      "[373/00101] train_loss: 0.011833\n",
      "[373/00151] train_loss: 0.011670\n",
      "[373/00201] train_loss: 0.011594\n",
      "[373/00251] train_loss: 0.011519\n",
      "[373/00301] train_loss: 0.012351\n",
      "[373/00351] train_loss: 0.011581\n",
      "[373/00401] train_loss: 0.011349\n",
      "[373/00451] train_loss: 0.011441\n",
      "[373/00501] train_loss: 0.011877\n",
      "[373/00551] train_loss: 0.011705\n",
      "[373/00601] train_loss: 0.011271\n",
      "[373/00651] train_loss: 0.011509\n",
      "[373/00701] train_loss: 0.011132\n",
      "[373/00751] train_loss: 0.011425\n",
      "[373/00801] train_loss: 0.011283\n",
      "[373/00851] train_loss: 0.011283\n",
      "[373/00901] train_loss: 0.010896\n",
      "[373/00951] train_loss: 0.011255\n",
      "[373/01001] train_loss: 0.011706\n",
      "[373/01051] train_loss: 0.011084\n",
      "[373/01101] train_loss: 0.011204\n",
      "[373/01151] train_loss: 0.011658\n",
      "[373/01201] train_loss: 0.012014\n",
      "[374/00025] train_loss: 0.011362\n",
      "[374/00075] train_loss: 0.011453\n",
      "[374/00125] train_loss: 0.012744\n",
      "[374/00175] train_loss: 0.011765\n",
      "[374/00225] train_loss: 0.011558\n",
      "[374/00275] train_loss: 0.011490\n",
      "[374/00325] train_loss: 0.012413\n",
      "[374/00375] train_loss: 0.012094\n",
      "[374/00425] train_loss: 0.011877\n",
      "[374/00475] train_loss: 0.011829\n",
      "[374/00525] train_loss: 0.010822\n",
      "[374/00575] train_loss: 0.011844\n",
      "[374/00625] train_loss: 0.011712\n",
      "[374/00675] train_loss: 0.011373\n",
      "[374/00725] train_loss: 0.011875\n",
      "[374/00775] train_loss: 0.011594\n",
      "[374/00825] train_loss: 0.011883\n",
      "[374/00875] train_loss: 0.011179\n",
      "[374/00925] train_loss: 0.011349\n",
      "[374/00975] train_loss: 0.011188\n",
      "[374/01025] train_loss: 0.011295\n",
      "[374/01075] train_loss: 0.011660\n",
      "[374/01125] train_loss: 0.011722\n",
      "[374/01175] train_loss: 0.010385\n",
      "[374/01225] train_loss: 0.011140\n",
      "[375/00049] train_loss: 0.011502\n",
      "[375/00099] train_loss: 0.012188\n",
      "[375/00149] train_loss: 0.012455\n",
      "[375/00199] train_loss: 0.012296\n",
      "[375/00249] train_loss: 0.011789\n",
      "[375/00299] train_loss: 0.011784\n",
      "[375/00349] train_loss: 0.011829\n",
      "[375/00399] train_loss: 0.011358\n",
      "[375/00449] train_loss: 0.012299\n",
      "[375/00499] train_loss: 0.010966\n",
      "[375/00549] train_loss: 0.011457\n",
      "[375/00599] train_loss: 0.011608\n",
      "[375/00649] train_loss: 0.011726\n",
      "[375/00699] train_loss: 0.011446\n",
      "[375/00749] train_loss: 0.010985\n",
      "[375/00799] train_loss: 0.011289\n",
      "[375/00849] train_loss: 0.011434\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[375/00899] train_loss: 0.011490\n",
      "[375/00949] train_loss: 0.011801\n",
      "[375/00999] train_loss: 0.011154\n",
      "[375/01049] train_loss: 0.010733\n",
      "[375/01099] train_loss: 0.011069\n",
      "[375/01149] train_loss: 0.011921\n",
      "[375/01199] train_loss: 0.010998\n",
      "[376/00023] train_loss: 0.011636\n",
      "[376/00073] train_loss: 0.012034\n",
      "[376/00123] train_loss: 0.011647\n",
      "[376/00173] train_loss: 0.011552\n",
      "[376/00223] train_loss: 0.011273\n",
      "[376/00273] train_loss: 0.012126\n",
      "[376/00323] train_loss: 0.011760\n",
      "[376/00373] train_loss: 0.011898\n",
      "[376/00423] train_loss: 0.011410\n",
      "[376/00473] train_loss: 0.011547\n",
      "[376/00523] train_loss: 0.010787\n",
      "[376/00573] train_loss: 0.011809\n",
      "[376/00623] train_loss: 0.011522\n",
      "[376/00673] train_loss: 0.011550\n",
      "[376/00723] train_loss: 0.011896\n",
      "[376/00773] train_loss: 0.011033\n",
      "[376/00823] train_loss: 0.011657\n",
      "[376/00873] train_loss: 0.011249\n",
      "[376/00923] train_loss: 0.011563\n",
      "[376/00973] train_loss: 0.011814\n",
      "[376/01023] train_loss: 0.011462\n",
      "[376/01073] train_loss: 0.011604\n",
      "[376/01123] train_loss: 0.010993\n",
      "[376/01173] train_loss: 0.011827\n",
      "[376/01223] train_loss: 0.011716\n",
      "[377/00047] train_loss: 0.012240\n",
      "[377/00097] train_loss: 0.011908\n",
      "[377/00147] train_loss: 0.011289\n",
      "[377/00197] train_loss: 0.011710\n",
      "[377/00247] train_loss: 0.011843\n",
      "[377/00297] train_loss: 0.011511\n",
      "[377/00347] train_loss: 0.011737\n",
      "[377/00397] train_loss: 0.011044\n",
      "[377/00447] train_loss: 0.011467\n",
      "[377/00497] train_loss: 0.011953\n",
      "[377/00547] train_loss: 0.011246\n",
      "[377/00597] train_loss: 0.011912\n",
      "[377/00647] train_loss: 0.011132\n",
      "[377/00697] train_loss: 0.011371\n",
      "[377/00747] train_loss: 0.011502\n",
      "[377/00797] train_loss: 0.011922\n",
      "[377/00847] train_loss: 0.011775\n",
      "[377/00897] train_loss: 0.011026\n",
      "[377/00947] train_loss: 0.011443\n",
      "[377/00997] train_loss: 0.011339\n",
      "[377/01047] train_loss: 0.011799\n",
      "[377/01097] train_loss: 0.011236\n",
      "[377/01147] train_loss: 0.011316\n",
      "[377/01197] train_loss: 0.011058\n",
      "[378/00021] train_loss: 0.011982\n",
      "[378/00071] train_loss: 0.011508\n",
      "[378/00121] train_loss: 0.011996\n",
      "[378/00171] train_loss: 0.011524\n",
      "[378/00221] train_loss: 0.012123\n",
      "[378/00271] train_loss: 0.011518\n",
      "[378/00321] train_loss: 0.011236\n",
      "[378/00371] train_loss: 0.011581\n",
      "[378/00421] train_loss: 0.011539\n",
      "[378/00471] train_loss: 0.012022\n",
      "[378/00521] train_loss: 0.011446\n",
      "[378/00571] train_loss: 0.011759\n",
      "[378/00621] train_loss: 0.011066\n",
      "[378/00671] train_loss: 0.011147\n",
      "[378/00721] train_loss: 0.011502\n",
      "[378/00771] train_loss: 0.011899\n",
      "[378/00821] train_loss: 0.011894\n",
      "[378/00871] train_loss: 0.011788\n",
      "[378/00921] train_loss: 0.011630\n",
      "[378/00971] train_loss: 0.011832\n",
      "[378/01021] train_loss: 0.011729\n",
      "[378/01071] train_loss: 0.011292\n",
      "[378/01121] train_loss: 0.011927\n",
      "[378/01171] train_loss: 0.010875\n",
      "[378/01221] train_loss: 0.011291\n",
      "[379/00045] train_loss: 0.012151\n",
      "[379/00095] train_loss: 0.012476\n",
      "[379/00145] train_loss: 0.011679\n",
      "[379/00195] train_loss: 0.011886\n",
      "[379/00245] train_loss: 0.011630\n",
      "[379/00295] train_loss: 0.012062\n",
      "[379/00345] train_loss: 0.011616\n",
      "[379/00395] train_loss: 0.011555\n",
      "[379/00445] train_loss: 0.011570\n",
      "[379/00495] train_loss: 0.011477\n",
      "[379/00545] train_loss: 0.011684\n",
      "[379/00595] train_loss: 0.011456\n",
      "[379/00645] train_loss: 0.011158\n",
      "[379/00695] train_loss: 0.011170\n",
      "[379/00745] train_loss: 0.011270\n",
      "[379/00795] train_loss: 0.011440\n",
      "[379/00845] train_loss: 0.011521\n",
      "[379/00895] train_loss: 0.011463\n",
      "[379/00945] train_loss: 0.011058\n",
      "[379/00995] train_loss: 0.011355\n",
      "[379/01045] train_loss: 0.011094\n",
      "[379/01095] train_loss: 0.011531\n",
      "[379/01145] train_loss: 0.011473\n",
      "[379/01195] train_loss: 0.011386\n",
      "[380/00019] train_loss: 0.011763\n",
      "[380/00069] train_loss: 0.011629\n",
      "[380/00119] train_loss: 0.012197\n",
      "[380/00169] train_loss: 0.011794\n",
      "[380/00219] train_loss: 0.011681\n",
      "[380/00269] train_loss: 0.011966\n",
      "[380/00319] train_loss: 0.011554\n",
      "[380/00369] train_loss: 0.011572\n",
      "[380/00419] train_loss: 0.011221\n",
      "[380/00469] train_loss: 0.011632\n",
      "[380/00519] train_loss: 0.011998\n",
      "[380/00569] train_loss: 0.010979\n",
      "[380/00619] train_loss: 0.011522\n",
      "[380/00669] train_loss: 0.012271\n",
      "[380/00719] train_loss: 0.011783\n",
      "[380/00769] train_loss: 0.011716\n",
      "[380/00819] train_loss: 0.010660\n",
      "[380/00869] train_loss: 0.011517\n",
      "[380/00919] train_loss: 0.011825\n",
      "[380/00969] train_loss: 0.011161\n",
      "[380/01019] train_loss: 0.012148\n",
      "[380/01069] train_loss: 0.011429\n",
      "[380/01119] train_loss: 0.010807\n",
      "[380/01169] train_loss: 0.011302\n",
      "[380/01219] train_loss: 0.011016\n",
      "[381/00043] train_loss: 0.011319\n",
      "[381/00093] train_loss: 0.011825\n",
      "[381/00143] train_loss: 0.011751\n",
      "[381/00193] train_loss: 0.011348\n",
      "[381/00243] train_loss: 0.011610\n",
      "[381/00293] train_loss: 0.011371\n",
      "[381/00343] train_loss: 0.011064\n",
      "[381/00393] train_loss: 0.011548\n",
      "[381/00443] train_loss: 0.011718\n",
      "[381/00493] train_loss: 0.011241\n",
      "[381/00543] train_loss: 0.011897\n",
      "[381/00593] train_loss: 0.011429\n",
      "[381/00643] train_loss: 0.011486\n",
      "[381/00693] train_loss: 0.010806\n",
      "[381/00743] train_loss: 0.011681\n",
      "[381/00793] train_loss: 0.011731\n",
      "[381/00843] train_loss: 0.012116\n",
      "[381/00893] train_loss: 0.010846\n",
      "[381/00943] train_loss: 0.011312\n",
      "[381/00993] train_loss: 0.011464\n",
      "[381/01043] train_loss: 0.012019\n",
      "[381/01093] train_loss: 0.011449\n",
      "[381/01143] train_loss: 0.011283\n",
      "[381/01193] train_loss: 0.011197\n",
      "[382/00017] train_loss: 0.011383\n",
      "[382/00067] train_loss: 0.012211\n",
      "[382/00117] train_loss: 0.012586\n",
      "[382/00167] train_loss: 0.011438\n",
      "[382/00217] train_loss: 0.011188\n",
      "[382/00267] train_loss: 0.012161\n",
      "[382/00317] train_loss: 0.011377\n",
      "[382/00367] train_loss: 0.012276\n",
      "[382/00417] train_loss: 0.011198\n",
      "[382/00467] train_loss: 0.011635\n",
      "[382/00517] train_loss: 0.011461\n",
      "[382/00567] train_loss: 0.011345\n",
      "[382/00617] train_loss: 0.011418\n",
      "[382/00667] train_loss: 0.010870\n",
      "[382/00717] train_loss: 0.011744\n",
      "[382/00767] train_loss: 0.011847\n",
      "[382/00817] train_loss: 0.011640\n",
      "[382/00867] train_loss: 0.011581\n",
      "[382/00917] train_loss: 0.010941\n",
      "[382/00967] train_loss: 0.010706\n",
      "[382/01017] train_loss: 0.011385\n",
      "[382/01067] train_loss: 0.011970\n",
      "[382/01117] train_loss: 0.011714\n",
      "[382/01167] train_loss: 0.011209\n",
      "[382/01217] train_loss: 0.012007\n",
      "[383/00041] train_loss: 0.011610\n",
      "[383/00091] train_loss: 0.012263\n",
      "[383/00141] train_loss: 0.011532\n",
      "[383/00191] train_loss: 0.011171\n",
      "[383/00241] train_loss: 0.012280\n",
      "[383/00291] train_loss: 0.011307\n",
      "[383/00341] train_loss: 0.011119\n",
      "[383/00391] train_loss: 0.012087\n",
      "[383/00441] train_loss: 0.011097\n",
      "[383/00491] train_loss: 0.011753\n",
      "[383/00541] train_loss: 0.011789\n",
      "[383/00591] train_loss: 0.011620\n",
      "[383/00641] train_loss: 0.011260\n",
      "[383/00691] train_loss: 0.011263\n",
      "[383/00741] train_loss: 0.011593\n",
      "[383/00791] train_loss: 0.011661\n",
      "[383/00841] train_loss: 0.010727\n",
      "[383/00891] train_loss: 0.011560\n",
      "[383/00941] train_loss: 0.011851\n",
      "[383/00991] train_loss: 0.011844\n",
      "[383/01041] train_loss: 0.012194\n",
      "[383/01091] train_loss: 0.011284\n",
      "[383/01141] train_loss: 0.011673\n",
      "[383/01191] train_loss: 0.011859\n",
      "[384/00015] train_loss: 0.011476\n",
      "[384/00065] train_loss: 0.011674\n",
      "[384/00115] train_loss: 0.011327\n",
      "[384/00165] train_loss: 0.011821\n",
      "[384/00215] train_loss: 0.011900\n",
      "[384/00265] train_loss: 0.011247\n",
      "[384/00315] train_loss: 0.012011\n",
      "[384/00365] train_loss: 0.011560\n",
      "[384/00415] train_loss: 0.011698\n",
      "[384/00465] train_loss: 0.011041\n",
      "[384/00515] train_loss: 0.012157\n",
      "[384/00565] train_loss: 0.010894\n",
      "[384/00615] train_loss: 0.011384\n",
      "[384/00665] train_loss: 0.011433\n",
      "[384/00715] train_loss: 0.011652\n",
      "[384/00765] train_loss: 0.011586\n",
      "[384/00815] train_loss: 0.011213\n",
      "[384/00865] train_loss: 0.011736\n",
      "[384/00915] train_loss: 0.011918\n",
      "[384/00965] train_loss: 0.011112\n",
      "[384/01015] train_loss: 0.010686\n",
      "[384/01065] train_loss: 0.011052\n",
      "[384/01115] train_loss: 0.011579\n",
      "[384/01165] train_loss: 0.011405\n",
      "[384/01215] train_loss: 0.011857\n",
      "[385/00039] train_loss: 0.012124\n",
      "[385/00089] train_loss: 0.013068\n",
      "[385/00139] train_loss: 0.012243\n",
      "[385/00189] train_loss: 0.011501\n",
      "[385/00239] train_loss: 0.012129\n",
      "[385/00289] train_loss: 0.011733\n",
      "[385/00339] train_loss: 0.011612\n",
      "[385/00389] train_loss: 0.010989\n",
      "[385/00439] train_loss: 0.011419\n",
      "[385/00489] train_loss: 0.011590\n",
      "[385/00539] train_loss: 0.011439\n",
      "[385/00589] train_loss: 0.011784\n",
      "[385/00639] train_loss: 0.011647\n",
      "[385/00689] train_loss: 0.011244\n",
      "[385/00739] train_loss: 0.011465\n",
      "[385/00789] train_loss: 0.011107\n",
      "[385/00839] train_loss: 0.011556\n",
      "[385/00889] train_loss: 0.011443\n",
      "[385/00939] train_loss: 0.011982\n",
      "[385/00989] train_loss: 0.011211\n",
      "[385/01039] train_loss: 0.011007\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[385/01089] train_loss: 0.011148\n",
      "[385/01139] train_loss: 0.011627\n",
      "[385/01189] train_loss: 0.011388\n",
      "[386/00013] train_loss: 0.011465\n",
      "[386/00063] train_loss: 0.011909\n",
      "[386/00113] train_loss: 0.011765\n",
      "[386/00163] train_loss: 0.012315\n",
      "[386/00213] train_loss: 0.011868\n",
      "[386/00263] train_loss: 0.011283\n",
      "[386/00313] train_loss: 0.011662\n",
      "[386/00363] train_loss: 0.011617\n",
      "[386/00413] train_loss: 0.011246\n",
      "[386/00463] train_loss: 0.011234\n",
      "[386/00513] train_loss: 0.011603\n",
      "[386/00563] train_loss: 0.011645\n",
      "[386/00613] train_loss: 0.011851\n",
      "[386/00663] train_loss: 0.012267\n",
      "[386/00713] train_loss: 0.011459\n",
      "[386/00763] train_loss: 0.011146\n",
      "[386/00813] train_loss: 0.010838\n",
      "[386/00863] train_loss: 0.011438\n",
      "[386/00913] train_loss: 0.011854\n",
      "[386/00963] train_loss: 0.011543\n",
      "[386/01013] train_loss: 0.011411\n",
      "[386/01063] train_loss: 0.011217\n",
      "[386/01113] train_loss: 0.011376\n",
      "[386/01163] train_loss: 0.011384\n",
      "[386/01213] train_loss: 0.011088\n",
      "[387/00037] train_loss: 0.012304\n",
      "[387/00087] train_loss: 0.011909\n",
      "[387/00137] train_loss: 0.011990\n",
      "[387/00187] train_loss: 0.011688\n",
      "[387/00237] train_loss: 0.011909\n",
      "[387/00287] train_loss: 0.011187\n",
      "[387/00337] train_loss: 0.010826\n",
      "[387/00387] train_loss: 0.012148\n",
      "[387/00437] train_loss: 0.012004\n",
      "[387/00487] train_loss: 0.011853\n",
      "[387/00537] train_loss: 0.010957\n",
      "[387/00587] train_loss: 0.011754\n",
      "[387/00637] train_loss: 0.011978\n",
      "[387/00687] train_loss: 0.011628\n",
      "[387/00737] train_loss: 0.011744\n",
      "[387/00787] train_loss: 0.011545\n",
      "[387/00837] train_loss: 0.011162\n",
      "[387/00887] train_loss: 0.011455\n",
      "[387/00937] train_loss: 0.011555\n",
      "[387/00987] train_loss: 0.011325\n",
      "[387/01037] train_loss: 0.011040\n",
      "[387/01087] train_loss: 0.010729\n",
      "[387/01137] train_loss: 0.011112\n",
      "[387/01187] train_loss: 0.011421\n",
      "[388/00011] train_loss: 0.011310\n",
      "[388/00061] train_loss: 0.012161\n",
      "[388/00111] train_loss: 0.012279\n",
      "[388/00161] train_loss: 0.011950\n",
      "[388/00211] train_loss: 0.011643\n",
      "[388/00261] train_loss: 0.011427\n",
      "[388/00311] train_loss: 0.011377\n",
      "[388/00361] train_loss: 0.011129\n",
      "[388/00411] train_loss: 0.011029\n",
      "[388/00461] train_loss: 0.012148\n",
      "[388/00511] train_loss: 0.011395\n",
      "[388/00561] train_loss: 0.010900\n",
      "[388/00611] train_loss: 0.011288\n",
      "[388/00661] train_loss: 0.011516\n",
      "[388/00711] train_loss: 0.012277\n",
      "[388/00761] train_loss: 0.011719\n",
      "[388/00811] train_loss: 0.011700\n",
      "[388/00861] train_loss: 0.010994\n",
      "[388/00911] train_loss: 0.011085\n",
      "[388/00961] train_loss: 0.010788\n",
      "[388/01011] train_loss: 0.011447\n",
      "[388/01061] train_loss: 0.011078\n",
      "[388/01111] train_loss: 0.011257\n",
      "[388/01161] train_loss: 0.011960\n",
      "[388/01211] train_loss: 0.011553\n",
      "[389/00035] train_loss: 0.012088\n",
      "[389/00085] train_loss: 0.012091\n",
      "[389/00135] train_loss: 0.011345\n",
      "[389/00185] train_loss: 0.011570\n",
      "[389/00235] train_loss: 0.011536\n",
      "[389/00285] train_loss: 0.011982\n",
      "[389/00335] train_loss: 0.012390\n",
      "[389/00385] train_loss: 0.012028\n",
      "[389/00435] train_loss: 0.011120\n",
      "[389/00485] train_loss: 0.011286\n",
      "[389/00535] train_loss: 0.011444\n",
      "[389/00585] train_loss: 0.011888\n",
      "[389/00635] train_loss: 0.011375\n",
      "[389/00685] train_loss: 0.011179\n",
      "[389/00735] train_loss: 0.011742\n",
      "[389/00785] train_loss: 0.011630\n",
      "[389/00835] train_loss: 0.011134\n",
      "[389/00885] train_loss: 0.010939\n",
      "[389/00935] train_loss: 0.011602\n",
      "[389/00985] train_loss: 0.011574\n",
      "[389/01035] train_loss: 0.011401\n",
      "[389/01085] train_loss: 0.011684\n",
      "[389/01135] train_loss: 0.011262\n",
      "[389/01185] train_loss: 0.010870\n",
      "[390/00009] train_loss: 0.011383\n",
      "[390/00059] train_loss: 0.012551\n",
      "[390/00109] train_loss: 0.011633\n",
      "[390/00159] train_loss: 0.011912\n",
      "[390/00209] train_loss: 0.011717\n",
      "[390/00259] train_loss: 0.011032\n",
      "[390/00309] train_loss: 0.011195\n",
      "[390/00359] train_loss: 0.011675\n",
      "[390/00409] train_loss: 0.012097\n",
      "[390/00459] train_loss: 0.011565\n",
      "[390/00509] train_loss: 0.010981\n",
      "[390/00559] train_loss: 0.011647\n",
      "[390/00609] train_loss: 0.010920\n",
      "[390/00659] train_loss: 0.011192\n",
      "[390/00709] train_loss: 0.011814\n",
      "[390/00759] train_loss: 0.011014\n",
      "[390/00809] train_loss: 0.010566\n",
      "[390/00859] train_loss: 0.011679\n",
      "[390/00909] train_loss: 0.010878\n",
      "[390/00959] train_loss: 0.011678\n",
      "[390/01009] train_loss: 0.011346\n",
      "[390/01059] train_loss: 0.011476\n",
      "[390/01109] train_loss: 0.011511\n",
      "[390/01159] train_loss: 0.011637\n",
      "[390/01209] train_loss: 0.011511\n",
      "[391/00033] train_loss: 0.012187\n",
      "[391/00083] train_loss: 0.012571\n",
      "[391/00133] train_loss: 0.011611\n",
      "[391/00183] train_loss: 0.012047\n",
      "[391/00233] train_loss: 0.011652\n",
      "[391/00283] train_loss: 0.011351\n",
      "[391/00333] train_loss: 0.011553\n",
      "[391/00383] train_loss: 0.012250\n",
      "[391/00433] train_loss: 0.012603\n",
      "[391/00483] train_loss: 0.011580\n",
      "[391/00533] train_loss: 0.011372\n",
      "[391/00583] train_loss: 0.010954\n",
      "[391/00633] train_loss: 0.011549\n",
      "[391/00683] train_loss: 0.011461\n",
      "[391/00733] train_loss: 0.011281\n",
      "[391/00783] train_loss: 0.011652\n",
      "[391/00833] train_loss: 0.011207\n",
      "[391/00883] train_loss: 0.010848\n",
      "[391/00933] train_loss: 0.011275\n",
      "[391/00983] train_loss: 0.011401\n",
      "[391/01033] train_loss: 0.010790\n",
      "[391/01083] train_loss: 0.011200\n",
      "[391/01133] train_loss: 0.010812\n",
      "[391/01183] train_loss: 0.011141\n",
      "[392/00007] train_loss: 0.010987\n",
      "[392/00057] train_loss: 0.011605\n",
      "[392/00107] train_loss: 0.010999\n",
      "[392/00157] train_loss: 0.011427\n",
      "[392/00207] train_loss: 0.011648\n",
      "[392/00257] train_loss: 0.011662\n",
      "[392/00307] train_loss: 0.012242\n",
      "[392/00357] train_loss: 0.011345\n",
      "[392/00407] train_loss: 0.011257\n",
      "[392/00457] train_loss: 0.011954\n",
      "[392/00507] train_loss: 0.011512\n",
      "[392/00557] train_loss: 0.012020\n",
      "[392/00607] train_loss: 0.011171\n",
      "[392/00657] train_loss: 0.011679\n",
      "[392/00707] train_loss: 0.011533\n",
      "[392/00757] train_loss: 0.011068\n",
      "[392/00807] train_loss: 0.011489\n",
      "[392/00857] train_loss: 0.010902\n",
      "[392/00907] train_loss: 0.010922\n",
      "[392/00957] train_loss: 0.011632\n",
      "[392/01007] train_loss: 0.011826\n",
      "[392/01057] train_loss: 0.010658\n",
      "[392/01107] train_loss: 0.011286\n",
      "[392/01157] train_loss: 0.010831\n",
      "[392/01207] train_loss: 0.011249\n",
      "[393/00031] train_loss: 0.011051\n",
      "[393/00081] train_loss: 0.011633\n",
      "[393/00131] train_loss: 0.012181\n",
      "[393/00181] train_loss: 0.011771\n",
      "[393/00231] train_loss: 0.011876\n",
      "[393/00281] train_loss: 0.011447\n",
      "[393/00331] train_loss: 0.011002\n",
      "[393/00381] train_loss: 0.011037\n",
      "[393/00431] train_loss: 0.011973\n",
      "[393/00481] train_loss: 0.011924\n",
      "[393/00531] train_loss: 0.011760\n",
      "[393/00581] train_loss: 0.011200\n",
      "[393/00631] train_loss: 0.011482\n",
      "[393/00681] train_loss: 0.011472\n",
      "[393/00731] train_loss: 0.011438\n",
      "[393/00781] train_loss: 0.011522\n",
      "[393/00831] train_loss: 0.011316\n",
      "[393/00881] train_loss: 0.011468\n",
      "[393/00931] train_loss: 0.011693\n",
      "[393/00981] train_loss: 0.011406\n",
      "[393/01031] train_loss: 0.011458\n",
      "[393/01081] train_loss: 0.010681\n",
      "[393/01131] train_loss: 0.011294\n",
      "[393/01181] train_loss: 0.010842\n",
      "[394/00005] train_loss: 0.011516\n",
      "[394/00055] train_loss: 0.012042\n",
      "[394/00105] train_loss: 0.011661\n",
      "[394/00155] train_loss: 0.011469\n",
      "[394/00205] train_loss: 0.011391\n",
      "[394/00255] train_loss: 0.011204\n",
      "[394/00305] train_loss: 0.011467\n",
      "[394/00355] train_loss: 0.011641\n",
      "[394/00405] train_loss: 0.011238\n",
      "[394/00455] train_loss: 0.012034\n",
      "[394/00505] train_loss: 0.011553\n",
      "[394/00555] train_loss: 0.011151\n",
      "[394/00605] train_loss: 0.011117\n",
      "[394/00655] train_loss: 0.010853\n",
      "[394/00705] train_loss: 0.011876\n",
      "[394/00755] train_loss: 0.010615\n",
      "[394/00805] train_loss: 0.011389\n",
      "[394/00855] train_loss: 0.011609\n",
      "[394/00905] train_loss: 0.011234\n",
      "[394/00955] train_loss: 0.011253\n",
      "[394/01005] train_loss: 0.011581\n",
      "[394/01055] train_loss: 0.011363\n",
      "[394/01105] train_loss: 0.011266\n",
      "[394/01155] train_loss: 0.011380\n",
      "[394/01205] train_loss: 0.011306\n",
      "[395/00029] train_loss: 0.011324\n",
      "[395/00079] train_loss: 0.012204\n",
      "[395/00129] train_loss: 0.011885\n",
      "[395/00179] train_loss: 0.011717\n",
      "[395/00229] train_loss: 0.011958\n",
      "[395/00279] train_loss: 0.011316\n",
      "[395/00329] train_loss: 0.011210\n",
      "[395/00379] train_loss: 0.011777\n",
      "[395/00429] train_loss: 0.011343\n",
      "[395/00479] train_loss: 0.011115\n",
      "[395/00529] train_loss: 0.011316\n",
      "[395/00579] train_loss: 0.011190\n",
      "[395/00629] train_loss: 0.011188\n",
      "[395/00679] train_loss: 0.010965\n",
      "[395/00729] train_loss: 0.011651\n",
      "[395/00779] train_loss: 0.011377\n",
      "[395/00829] train_loss: 0.011839\n",
      "[395/00879] train_loss: 0.011439\n",
      "[395/00929] train_loss: 0.011577\n",
      "[395/00979] train_loss: 0.011784\n",
      "[395/01029] train_loss: 0.011180\n",
      "[395/01079] train_loss: 0.011549\n",
      "[395/01129] train_loss: 0.011771\n",
      "[395/01179] train_loss: 0.010589\n",
      "[396/00003] train_loss: 0.010364\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[396/00053] train_loss: 0.011928\n",
      "[396/00103] train_loss: 0.012053\n",
      "[396/00153] train_loss: 0.011532\n",
      "[396/00203] train_loss: 0.011524\n",
      "[396/00253] train_loss: 0.011847\n",
      "[396/00303] train_loss: 0.011592\n",
      "[396/00353] train_loss: 0.011040\n",
      "[396/00403] train_loss: 0.011479\n",
      "[396/00453] train_loss: 0.011197\n",
      "[396/00503] train_loss: 0.011415\n",
      "[396/00553] train_loss: 0.011492\n",
      "[396/00603] train_loss: 0.010289\n",
      "[396/00653] train_loss: 0.011270\n",
      "[396/00703] train_loss: 0.010865\n",
      "[396/00753] train_loss: 0.010580\n",
      "[396/00803] train_loss: 0.011695\n",
      "[396/00853] train_loss: 0.011551\n",
      "[396/00903] train_loss: 0.011048\n",
      "[396/00953] train_loss: 0.010961\n",
      "[396/01003] train_loss: 0.011901\n",
      "[396/01053] train_loss: 0.011184\n",
      "[396/01103] train_loss: 0.012046\n",
      "[396/01153] train_loss: 0.011360\n",
      "[396/01203] train_loss: 0.010759\n",
      "[397/00027] train_loss: 0.011688\n",
      "[397/00077] train_loss: 0.011504\n",
      "[397/00127] train_loss: 0.011402\n",
      "[397/00177] train_loss: 0.012434\n",
      "[397/00227] train_loss: 0.011557\n",
      "[397/00277] train_loss: 0.011496\n",
      "[397/00327] train_loss: 0.011382\n",
      "[397/00377] train_loss: 0.012255\n",
      "[397/00427] train_loss: 0.010952\n",
      "[397/00477] train_loss: 0.011224\n",
      "[397/00527] train_loss: 0.011194\n",
      "[397/00577] train_loss: 0.012357\n",
      "[397/00627] train_loss: 0.011416\n",
      "[397/00677] train_loss: 0.011075\n",
      "[397/00727] train_loss: 0.011130\n",
      "[397/00777] train_loss: 0.011129\n",
      "[397/00827] train_loss: 0.011762\n",
      "[397/00877] train_loss: 0.010928\n",
      "[397/00927] train_loss: 0.011657\n",
      "[397/00977] train_loss: 0.011534\n",
      "[397/01027] train_loss: 0.012011\n",
      "[397/01077] train_loss: 0.011293\n",
      "[397/01127] train_loss: 0.011803\n",
      "[397/01177] train_loss: 0.011527\n",
      "[398/00001] train_loss: 0.011837\n",
      "[398/00051] train_loss: 0.011641\n",
      "[398/00101] train_loss: 0.011415\n",
      "[398/00151] train_loss: 0.012351\n",
      "[398/00201] train_loss: 0.011663\n",
      "[398/00251] train_loss: 0.012140\n",
      "[398/00301] train_loss: 0.011223\n",
      "[398/00351] train_loss: 0.011674\n",
      "[398/00401] train_loss: 0.011472\n",
      "[398/00451] train_loss: 0.011599\n",
      "[398/00501] train_loss: 0.011143\n",
      "[398/00551] train_loss: 0.011162\n",
      "[398/00601] train_loss: 0.011208\n",
      "[398/00651] train_loss: 0.011704\n",
      "[398/00701] train_loss: 0.010857\n",
      "[398/00751] train_loss: 0.011412\n",
      "[398/00801] train_loss: 0.011417\n",
      "[398/00851] train_loss: 0.012175\n",
      "[398/00901] train_loss: 0.010834\n",
      "[398/00951] train_loss: 0.011242\n",
      "[398/01001] train_loss: 0.011706\n",
      "[398/01051] train_loss: 0.011638\n",
      "[398/01101] train_loss: 0.011185\n",
      "[398/01151] train_loss: 0.011296\n",
      "[398/01201] train_loss: 0.010921\n",
      "[399/00025] train_loss: 0.011917\n",
      "[399/00075] train_loss: 0.011642\n",
      "[399/00125] train_loss: 0.011982\n",
      "[399/00175] train_loss: 0.011746\n",
      "[399/00225] train_loss: 0.011820\n",
      "[399/00275] train_loss: 0.011367\n",
      "[399/00325] train_loss: 0.011631\n",
      "[399/00375] train_loss: 0.011481\n",
      "[399/00425] train_loss: 0.011713\n",
      "[399/00475] train_loss: 0.011076\n",
      "[399/00525] train_loss: 0.011116\n",
      "[399/00575] train_loss: 0.011604\n",
      "[399/00625] train_loss: 0.011175\n",
      "[399/00675] train_loss: 0.011465\n",
      "[399/00725] train_loss: 0.011705\n",
      "[399/00775] train_loss: 0.011025\n",
      "[399/00825] train_loss: 0.011101\n",
      "[399/00875] train_loss: 0.011145\n",
      "[399/00925] train_loss: 0.010804\n",
      "[399/00975] train_loss: 0.011511\n",
      "[399/01025] train_loss: 0.011148\n",
      "[399/01075] train_loss: 0.011149\n",
      "[399/01125] train_loss: 0.011881\n",
      "[399/01175] train_loss: 0.011373\n",
      "[399/01225] train_loss: 0.011405\n",
      "[400/00049] train_loss: 0.011969\n",
      "[400/00099] train_loss: 0.011519\n",
      "[400/00149] train_loss: 0.011153\n",
      "[400/00199] train_loss: 0.011403\n",
      "[400/00249] train_loss: 0.011064\n",
      "[400/00299] train_loss: 0.011702\n",
      "[400/00349] train_loss: 0.010853\n",
      "[400/00399] train_loss: 0.011248\n",
      "[400/00449] train_loss: 0.011803\n",
      "[400/00499] train_loss: 0.011277\n",
      "[400/00549] train_loss: 0.011725\n",
      "[400/00599] train_loss: 0.010581\n",
      "[400/00649] train_loss: 0.011746\n",
      "[400/00699] train_loss: 0.011779\n",
      "[400/00749] train_loss: 0.011135\n",
      "[400/00799] train_loss: 0.011725\n",
      "[400/00849] train_loss: 0.011488\n",
      "[400/00899] train_loss: 0.010928\n",
      "[400/00949] train_loss: 0.011528\n",
      "[400/00999] train_loss: 0.011762\n",
      "[400/01049] train_loss: 0.012076\n",
      "[400/01099] train_loss: 0.011377\n",
      "[400/01149] train_loss: 0.011107\n",
      "[400/01199] train_loss: 0.011484\n",
      "[401/00023] train_loss: 0.011408\n",
      "[401/00073] train_loss: 0.011580\n",
      "[401/00123] train_loss: 0.011441\n",
      "[401/00173] train_loss: 0.011698\n",
      "[401/00223] train_loss: 0.011461\n",
      "[401/00273] train_loss: 0.011675\n",
      "[401/00323] train_loss: 0.011385\n",
      "[401/00373] train_loss: 0.011541\n",
      "[401/00423] train_loss: 0.011886\n",
      "[401/00473] train_loss: 0.011492\n",
      "[401/00523] train_loss: 0.012035\n",
      "[401/00573] train_loss: 0.011368\n",
      "[401/00623] train_loss: 0.011381\n",
      "[401/00673] train_loss: 0.011064\n",
      "[401/00723] train_loss: 0.011266\n",
      "[401/00773] train_loss: 0.011953\n",
      "[401/00823] train_loss: 0.011636\n",
      "[401/00873] train_loss: 0.011041\n",
      "[401/00923] train_loss: 0.011268\n",
      "[401/00973] train_loss: 0.011631\n",
      "[401/01023] train_loss: 0.010892\n",
      "[401/01073] train_loss: 0.011770\n",
      "[401/01123] train_loss: 0.011195\n",
      "[401/01173] train_loss: 0.011243\n",
      "[401/01223] train_loss: 0.011307\n",
      "[402/00047] train_loss: 0.011471\n",
      "[402/00097] train_loss: 0.011462\n",
      "[402/00147] train_loss: 0.011979\n",
      "[402/00197] train_loss: 0.011316\n",
      "[402/00247] train_loss: 0.011517\n",
      "[402/00297] train_loss: 0.011191\n",
      "[402/00347] train_loss: 0.011010\n",
      "[402/00397] train_loss: 0.011451\n",
      "[402/00447] train_loss: 0.012022\n",
      "[402/00497] train_loss: 0.011612\n",
      "[402/00547] train_loss: 0.010686\n",
      "[402/00597] train_loss: 0.011325\n",
      "[402/00647] train_loss: 0.011730\n",
      "[402/00697] train_loss: 0.011563\n",
      "[402/00747] train_loss: 0.011542\n",
      "[402/00797] train_loss: 0.011659\n",
      "[402/00847] train_loss: 0.011098\n",
      "[402/00897] train_loss: 0.011825\n",
      "[402/00947] train_loss: 0.011889\n",
      "[402/00997] train_loss: 0.011085\n",
      "[402/01047] train_loss: 0.010948\n",
      "[402/01097] train_loss: 0.011119\n",
      "[402/01147] train_loss: 0.010770\n",
      "[402/01197] train_loss: 0.011186\n",
      "[403/00021] train_loss: 0.011241\n",
      "[403/00071] train_loss: 0.011905\n",
      "[403/00121] train_loss: 0.011840\n",
      "[403/00171] train_loss: 0.011376\n",
      "[403/00221] train_loss: 0.012512\n",
      "[403/00271] train_loss: 0.011651\n",
      "[403/00321] train_loss: 0.012186\n",
      "[403/00371] train_loss: 0.011091\n",
      "[403/00421] train_loss: 0.011944\n",
      "[403/00471] train_loss: 0.011396\n",
      "[403/00521] train_loss: 0.012633\n",
      "[403/00571] train_loss: 0.011329\n",
      "[403/00621] train_loss: 0.011197\n",
      "[403/00671] train_loss: 0.010466\n",
      "[403/00721] train_loss: 0.011568\n",
      "[403/00771] train_loss: 0.011118\n",
      "[403/00821] train_loss: 0.011344\n",
      "[403/00871] train_loss: 0.011524\n",
      "[403/00921] train_loss: 0.011128\n",
      "[403/00971] train_loss: 0.011314\n",
      "[403/01021] train_loss: 0.010775\n",
      "[403/01071] train_loss: 0.011810\n",
      "[403/01121] train_loss: 0.011721\n",
      "[403/01171] train_loss: 0.010790\n",
      "[403/01221] train_loss: 0.011210\n",
      "[404/00045] train_loss: 0.012063\n",
      "[404/00095] train_loss: 0.011495\n",
      "[404/00145] train_loss: 0.010849\n",
      "[404/00195] train_loss: 0.012199\n",
      "[404/00245] train_loss: 0.012205\n",
      "[404/00295] train_loss: 0.011316\n",
      "[404/00345] train_loss: 0.011107\n",
      "[404/00395] train_loss: 0.011376\n",
      "[404/00445] train_loss: 0.011259\n",
      "[404/00495] train_loss: 0.011798\n",
      "[404/00545] train_loss: 0.010959\n",
      "[404/00595] train_loss: 0.011353\n",
      "[404/00645] train_loss: 0.011332\n",
      "[404/00695] train_loss: 0.010553\n",
      "[404/00745] train_loss: 0.011489\n",
      "[404/00795] train_loss: 0.010514\n",
      "[404/00845] train_loss: 0.011043\n",
      "[404/00895] train_loss: 0.011358\n",
      "[404/00945] train_loss: 0.011364\n",
      "[404/00995] train_loss: 0.011370\n",
      "[404/01045] train_loss: 0.011184\n",
      "[404/01095] train_loss: 0.010930\n",
      "[404/01145] train_loss: 0.011010\n",
      "[404/01195] train_loss: 0.011320\n",
      "[405/00019] train_loss: 0.011603\n",
      "[405/00069] train_loss: 0.011617\n",
      "[405/00119] train_loss: 0.012105\n",
      "[405/00169] train_loss: 0.011915\n",
      "[405/00219] train_loss: 0.011965\n",
      "[405/00269] train_loss: 0.011395\n",
      "[405/00319] train_loss: 0.011623\n",
      "[405/00369] train_loss: 0.011257\n",
      "[405/00419] train_loss: 0.011195\n",
      "[405/00469] train_loss: 0.011719\n",
      "[405/00519] train_loss: 0.011361\n",
      "[405/00569] train_loss: 0.011794\n",
      "[405/00619] train_loss: 0.011115\n",
      "[405/00669] train_loss: 0.010989\n",
      "[405/00719] train_loss: 0.011138\n",
      "[405/00769] train_loss: 0.011351\n",
      "[405/00819] train_loss: 0.011943\n",
      "[405/00869] train_loss: 0.011375\n",
      "[405/00919] train_loss: 0.011387\n",
      "[405/00969] train_loss: 0.011258\n",
      "[405/01019] train_loss: 0.011874\n",
      "[405/01069] train_loss: 0.011515\n",
      "[405/01119] train_loss: 0.011248\n",
      "[405/01169] train_loss: 0.011060\n",
      "[405/01219] train_loss: 0.011358\n",
      "[406/00043] train_loss: 0.011063\n",
      "[406/00093] train_loss: 0.011884\n",
      "[406/00143] train_loss: 0.011313\n",
      "[406/00193] train_loss: 0.011655\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[406/00243] train_loss: 0.011232\n",
      "[406/00293] train_loss: 0.012229\n",
      "[406/00343] train_loss: 0.011322\n",
      "[406/00393] train_loss: 0.011400\n",
      "[406/00443] train_loss: 0.011121\n",
      "[406/00493] train_loss: 0.011129\n",
      "[406/00543] train_loss: 0.011358\n",
      "[406/00593] train_loss: 0.010907\n",
      "[406/00643] train_loss: 0.011719\n",
      "[406/00693] train_loss: 0.011252\n",
      "[406/00743] train_loss: 0.011094\n",
      "[406/00793] train_loss: 0.010943\n",
      "[406/00843] train_loss: 0.011228\n",
      "[406/00893] train_loss: 0.011239\n",
      "[406/00943] train_loss: 0.011584\n",
      "[406/00993] train_loss: 0.010671\n",
      "[406/01043] train_loss: 0.011332\n",
      "[406/01093] train_loss: 0.011427\n",
      "[406/01143] train_loss: 0.011442\n",
      "[406/01193] train_loss: 0.011247\n",
      "[407/00017] train_loss: 0.011651\n",
      "[407/00067] train_loss: 0.011090\n",
      "[407/00117] train_loss: 0.011802\n",
      "[407/00167] train_loss: 0.011561\n",
      "[407/00217] train_loss: 0.012095\n",
      "[407/00267] train_loss: 0.011828\n",
      "[407/00317] train_loss: 0.011812\n",
      "[407/00367] train_loss: 0.011289\n",
      "[407/00417] train_loss: 0.010697\n",
      "[407/00467] train_loss: 0.010756\n",
      "[407/00517] train_loss: 0.011526\n",
      "[407/00567] train_loss: 0.011885\n",
      "[407/00617] train_loss: 0.010951\n",
      "[407/00667] train_loss: 0.011303\n",
      "[407/00717] train_loss: 0.010906\n",
      "[407/00767] train_loss: 0.011809\n",
      "[407/00817] train_loss: 0.011469\n",
      "[407/00867] train_loss: 0.011058\n",
      "[407/00917] train_loss: 0.012063\n",
      "[407/00967] train_loss: 0.011811\n",
      "[407/01017] train_loss: 0.011174\n",
      "[407/01067] train_loss: 0.010931\n",
      "[407/01117] train_loss: 0.010940\n",
      "[407/01167] train_loss: 0.010982\n",
      "[407/01217] train_loss: 0.011391\n",
      "[408/00041] train_loss: 0.011500\n",
      "[408/00091] train_loss: 0.011737\n",
      "[408/00141] train_loss: 0.012049\n",
      "[408/00191] train_loss: 0.010895\n",
      "[408/00241] train_loss: 0.010855\n",
      "[408/00291] train_loss: 0.012251\n",
      "[408/00341] train_loss: 0.011581\n",
      "[408/00391] train_loss: 0.011172\n",
      "[408/00441] train_loss: 0.011118\n",
      "[408/00491] train_loss: 0.011522\n",
      "[408/00541] train_loss: 0.010848\n",
      "[408/00591] train_loss: 0.011007\n",
      "[408/00641] train_loss: 0.011508\n",
      "[408/00691] train_loss: 0.011322\n",
      "[408/00741] train_loss: 0.011008\n",
      "[408/00791] train_loss: 0.011100\n",
      "[408/00841] train_loss: 0.011373\n",
      "[408/00891] train_loss: 0.010970\n",
      "[408/00941] train_loss: 0.010942\n",
      "[408/00991] train_loss: 0.011014\n",
      "[408/01041] train_loss: 0.011748\n",
      "[408/01091] train_loss: 0.010697\n",
      "[408/01141] train_loss: 0.011442\n",
      "[408/01191] train_loss: 0.011736\n",
      "[409/00015] train_loss: 0.011068\n",
      "[409/00065] train_loss: 0.011874\n",
      "[409/00115] train_loss: 0.011972\n",
      "[409/00165] train_loss: 0.012145\n",
      "[409/00215] train_loss: 0.010989\n",
      "[409/00265] train_loss: 0.011550\n",
      "[409/00315] train_loss: 0.011597\n",
      "[409/00365] train_loss: 0.011580\n",
      "[409/00415] train_loss: 0.011726\n",
      "[409/00465] train_loss: 0.011061\n",
      "[409/00515] train_loss: 0.012186\n",
      "[409/00565] train_loss: 0.011240\n",
      "[409/00615] train_loss: 0.011328\n",
      "[409/00665] train_loss: 0.011301\n",
      "[409/00715] train_loss: 0.011301\n",
      "[409/00765] train_loss: 0.010766\n",
      "[409/00815] train_loss: 0.011264\n",
      "[409/00865] train_loss: 0.011220\n",
      "[409/00915] train_loss: 0.010953\n",
      "[409/00965] train_loss: 0.011356\n",
      "[409/01015] train_loss: 0.011416\n",
      "[409/01065] train_loss: 0.011491\n",
      "[409/01115] train_loss: 0.012077\n",
      "[409/01165] train_loss: 0.011419\n",
      "[409/01215] train_loss: 0.011221\n",
      "[410/00039] train_loss: 0.011589\n",
      "[410/00089] train_loss: 0.011738\n",
      "[410/00139] train_loss: 0.011385\n",
      "[410/00189] train_loss: 0.011871\n",
      "[410/00239] train_loss: 0.011664\n",
      "[410/00289] train_loss: 0.011876\n",
      "[410/00339] train_loss: 0.010592\n",
      "[410/00389] train_loss: 0.011060\n",
      "[410/00439] train_loss: 0.011182\n",
      "[410/00489] train_loss: 0.011433\n",
      "[410/00539] train_loss: 0.011017\n",
      "[410/00589] train_loss: 0.011360\n",
      "[410/00639] train_loss: 0.011262\n",
      "[410/00689] train_loss: 0.012199\n",
      "[410/00739] train_loss: 0.011440\n",
      "[410/00789] train_loss: 0.011158\n",
      "[410/00839] train_loss: 0.011285\n",
      "[410/00889] train_loss: 0.011389\n",
      "[410/00939] train_loss: 0.011016\n",
      "[410/00989] train_loss: 0.011678\n",
      "[410/01039] train_loss: 0.010988\n",
      "[410/01089] train_loss: 0.010900\n",
      "[410/01139] train_loss: 0.010822\n",
      "[410/01189] train_loss: 0.010813\n",
      "[411/00013] train_loss: 0.011719\n",
      "[411/00063] train_loss: 0.011572\n",
      "[411/00113] train_loss: 0.011492\n",
      "[411/00163] train_loss: 0.011641\n",
      "[411/00213] train_loss: 0.011769\n",
      "[411/00263] train_loss: 0.011657\n",
      "[411/00313] train_loss: 0.011792\n",
      "[411/00363] train_loss: 0.011063\n",
      "[411/00413] train_loss: 0.011750\n",
      "[411/00463] train_loss: 0.011367\n",
      "[411/00513] train_loss: 0.011231\n",
      "[411/00563] train_loss: 0.011137\n",
      "[411/00613] train_loss: 0.011472\n",
      "[411/00663] train_loss: 0.011117\n",
      "[411/00713] train_loss: 0.011108\n",
      "[411/00763] train_loss: 0.011190\n",
      "[411/00813] train_loss: 0.011396\n",
      "[411/00863] train_loss: 0.011890\n",
      "[411/00913] train_loss: 0.011720\n",
      "[411/00963] train_loss: 0.011407\n",
      "[411/01013] train_loss: 0.011243\n",
      "[411/01063] train_loss: 0.010522\n",
      "[411/01113] train_loss: 0.011358\n",
      "[411/01163] train_loss: 0.011314\n",
      "[411/01213] train_loss: 0.011562\n",
      "[412/00037] train_loss: 0.011777\n",
      "[412/00087] train_loss: 0.011657\n",
      "[412/00137] train_loss: 0.010785\n",
      "[412/00187] train_loss: 0.011718\n",
      "[412/00237] train_loss: 0.011972\n",
      "[412/00287] train_loss: 0.011370\n",
      "[412/00337] train_loss: 0.011679\n",
      "[412/00387] train_loss: 0.010987\n",
      "[412/00437] train_loss: 0.010983\n",
      "[412/00487] train_loss: 0.011571\n",
      "[412/00537] train_loss: 0.011541\n",
      "[412/00587] train_loss: 0.011088\n",
      "[412/00637] train_loss: 0.011567\n",
      "[412/00687] train_loss: 0.011656\n",
      "[412/00737] train_loss: 0.011710\n",
      "[412/00787] train_loss: 0.011651\n",
      "[412/00837] train_loss: 0.010575\n",
      "[412/00887] train_loss: 0.011853\n",
      "[412/00937] train_loss: 0.010758\n",
      "[412/00987] train_loss: 0.011176\n",
      "[412/01037] train_loss: 0.010664\n",
      "[412/01087] train_loss: 0.011021\n",
      "[412/01137] train_loss: 0.010952\n",
      "[412/01187] train_loss: 0.011037\n",
      "[413/00011] train_loss: 0.011734\n",
      "[413/00061] train_loss: 0.011873\n",
      "[413/00111] train_loss: 0.011868\n",
      "[413/00161] train_loss: 0.011466\n",
      "[413/00211] train_loss: 0.012260\n",
      "[413/00261] train_loss: 0.012149\n",
      "[413/00311] train_loss: 0.011554\n",
      "[413/00361] train_loss: 0.011695\n",
      "[413/00411] train_loss: 0.011192\n",
      "[413/00461] train_loss: 0.010582\n",
      "[413/00511] train_loss: 0.011642\n",
      "[413/00561] train_loss: 0.011367\n",
      "[413/00611] train_loss: 0.010764\n",
      "[413/00661] train_loss: 0.011459\n",
      "[413/00711] train_loss: 0.011145\n",
      "[413/00761] train_loss: 0.011430\n",
      "[413/00811] train_loss: 0.010995\n",
      "[413/00861] train_loss: 0.010635\n",
      "[413/00911] train_loss: 0.011869\n",
      "[413/00961] train_loss: 0.010547\n",
      "[413/01011] train_loss: 0.011425\n",
      "[413/01061] train_loss: 0.011676\n",
      "[413/01111] train_loss: 0.011345\n",
      "[413/01161] train_loss: 0.011431\n",
      "[413/01211] train_loss: 0.010856\n",
      "[414/00035] train_loss: 0.011925\n",
      "[414/00085] train_loss: 0.011551\n",
      "[414/00135] train_loss: 0.012203\n",
      "[414/00185] train_loss: 0.011794\n",
      "[414/00235] train_loss: 0.011428\n",
      "[414/00285] train_loss: 0.011308\n",
      "[414/00335] train_loss: 0.011367\n",
      "[414/00385] train_loss: 0.011894\n",
      "[414/00435] train_loss: 0.011766\n",
      "[414/00485] train_loss: 0.011393\n",
      "[414/00535] train_loss: 0.011071\n",
      "[414/00585] train_loss: 0.010690\n",
      "[414/00635] train_loss: 0.010824\n",
      "[414/00685] train_loss: 0.011542\n",
      "[414/00735] train_loss: 0.011117\n",
      "[414/00785] train_loss: 0.011367\n",
      "[414/00835] train_loss: 0.010823\n",
      "[414/00885] train_loss: 0.011761\n",
      "[414/00935] train_loss: 0.011429\n",
      "[414/00985] train_loss: 0.010536\n",
      "[414/01035] train_loss: 0.011136\n",
      "[414/01085] train_loss: 0.010585\n",
      "[414/01135] train_loss: 0.011063\n",
      "[414/01185] train_loss: 0.011701\n",
      "[415/00009] train_loss: 0.011536\n",
      "[415/00059] train_loss: 0.012062\n",
      "[415/00109] train_loss: 0.011613\n",
      "[415/00159] train_loss: 0.012293\n",
      "[415/00209] train_loss: 0.011040\n",
      "[415/00259] train_loss: 0.011773\n",
      "[415/00309] train_loss: 0.011864\n",
      "[415/00359] train_loss: 0.012184\n",
      "[415/00409] train_loss: 0.011696\n",
      "[415/00459] train_loss: 0.011132\n",
      "[415/00509] train_loss: 0.011869\n",
      "[415/00559] train_loss: 0.011754\n",
      "[415/00609] train_loss: 0.011580\n",
      "[415/00659] train_loss: 0.011344\n",
      "[415/00709] train_loss: 0.011078\n",
      "[415/00759] train_loss: 0.010938\n",
      "[415/00809] train_loss: 0.011163\n",
      "[415/00859] train_loss: 0.010962\n",
      "[415/00909] train_loss: 0.011171\n",
      "[415/00959] train_loss: 0.011493\n",
      "[415/01009] train_loss: 0.011259\n",
      "[415/01059] train_loss: 0.011019\n",
      "[415/01109] train_loss: 0.011490\n",
      "[415/01159] train_loss: 0.011038\n",
      "[415/01209] train_loss: 0.011029\n",
      "[416/00033] train_loss: 0.011882\n",
      "[416/00083] train_loss: 0.012332\n",
      "[416/00133] train_loss: 0.011615\n",
      "[416/00183] train_loss: 0.011531\n",
      "[416/00233] train_loss: 0.011316\n",
      "[416/00283] train_loss: 0.011512\n",
      "[416/00333] train_loss: 0.010849\n",
      "[416/00383] train_loss: 0.011555\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[416/00433] train_loss: 0.011384\n",
      "[416/00483] train_loss: 0.011069\n",
      "[416/00533] train_loss: 0.011048\n",
      "[416/00583] train_loss: 0.011267\n",
      "[416/00633] train_loss: 0.011063\n",
      "[416/00683] train_loss: 0.010558\n",
      "[416/00733] train_loss: 0.011312\n",
      "[416/00783] train_loss: 0.011137\n",
      "[416/00833] train_loss: 0.010956\n",
      "[416/00883] train_loss: 0.011270\n",
      "[416/00933] train_loss: 0.010896\n",
      "[416/00983] train_loss: 0.011318\n",
      "[416/01033] train_loss: 0.011580\n",
      "[416/01083] train_loss: 0.010813\n",
      "[416/01133] train_loss: 0.011514\n",
      "[416/01183] train_loss: 0.011035\n",
      "[417/00007] train_loss: 0.011847\n",
      "[417/00057] train_loss: 0.011628\n",
      "[417/00107] train_loss: 0.011915\n",
      "[417/00157] train_loss: 0.011626\n",
      "[417/00207] train_loss: 0.011667\n",
      "[417/00257] train_loss: 0.012496\n",
      "[417/00307] train_loss: 0.011273\n",
      "[417/00357] train_loss: 0.011514\n",
      "[417/00407] train_loss: 0.011178\n",
      "[417/00457] train_loss: 0.011179\n",
      "[417/00507] train_loss: 0.011440\n",
      "[417/00557] train_loss: 0.010666\n",
      "[417/00607] train_loss: 0.011485\n",
      "[417/00657] train_loss: 0.010697\n",
      "[417/00707] train_loss: 0.011449\n",
      "[417/00757] train_loss: 0.011590\n",
      "[417/00807] train_loss: 0.012091\n",
      "[417/00857] train_loss: 0.010421\n",
      "[417/00907] train_loss: 0.011121\n",
      "[417/00957] train_loss: 0.010822\n",
      "[417/01007] train_loss: 0.011139\n",
      "[417/01057] train_loss: 0.011965\n",
      "[417/01107] train_loss: 0.010849\n",
      "[417/01157] train_loss: 0.011506\n",
      "[417/01207] train_loss: 0.011662\n",
      "[418/00031] train_loss: 0.011082\n",
      "[418/00081] train_loss: 0.011554\n",
      "[418/00131] train_loss: 0.010864\n",
      "[418/00181] train_loss: 0.011116\n",
      "[418/00231] train_loss: 0.011511\n",
      "[418/00281] train_loss: 0.010886\n",
      "[418/00331] train_loss: 0.011652\n",
      "[418/00381] train_loss: 0.011600\n",
      "[418/00431] train_loss: 0.010937\n",
      "[418/00481] train_loss: 0.011496\n",
      "[418/00531] train_loss: 0.011146\n",
      "[418/00581] train_loss: 0.010917\n",
      "[418/00631] train_loss: 0.011171\n",
      "[418/00681] train_loss: 0.011002\n",
      "[418/00731] train_loss: 0.011044\n",
      "[418/00781] train_loss: 0.011005\n",
      "[418/00831] train_loss: 0.011117\n",
      "[418/00881] train_loss: 0.011518\n",
      "[418/00931] train_loss: 0.011227\n",
      "[418/00981] train_loss: 0.010748\n",
      "[418/01031] train_loss: 0.011540\n",
      "[418/01081] train_loss: 0.011367\n",
      "[418/01131] train_loss: 0.011346\n",
      "[418/01181] train_loss: 0.011644\n",
      "[419/00005] train_loss: 0.011169\n",
      "[419/00055] train_loss: 0.012109\n",
      "[419/00105] train_loss: 0.012036\n",
      "[419/00155] train_loss: 0.011575\n",
      "[419/00205] train_loss: 0.011041\n",
      "[419/00255] train_loss: 0.011728\n",
      "[419/00305] train_loss: 0.012229\n",
      "[419/00355] train_loss: 0.011171\n",
      "[419/00405] train_loss: 0.011922\n",
      "[419/00455] train_loss: 0.011416\n",
      "[419/00505] train_loss: 0.011847\n",
      "[419/00555] train_loss: 0.011187\n",
      "[419/00605] train_loss: 0.011307\n",
      "[419/00655] train_loss: 0.011310\n",
      "[419/00705] train_loss: 0.011261\n",
      "[419/00755] train_loss: 0.010731\n",
      "[419/00805] train_loss: 0.011766\n",
      "[419/00855] train_loss: 0.011104\n",
      "[419/00905] train_loss: 0.011211\n",
      "[419/00955] train_loss: 0.011307\n",
      "[419/01005] train_loss: 0.011107\n",
      "[419/01055] train_loss: 0.011078\n",
      "[419/01105] train_loss: 0.011706\n",
      "[419/01155] train_loss: 0.012055\n",
      "[419/01205] train_loss: 0.010638\n",
      "[420/00029] train_loss: 0.011680\n",
      "[420/00079] train_loss: 0.012035\n",
      "[420/00129] train_loss: 0.011739\n",
      "[420/00179] train_loss: 0.011385\n",
      "[420/00229] train_loss: 0.011730\n",
      "[420/00279] train_loss: 0.011567\n",
      "[420/00329] train_loss: 0.011417\n",
      "[420/00379] train_loss: 0.011383\n",
      "[420/00429] train_loss: 0.011628\n",
      "[420/00479] train_loss: 0.011529\n",
      "[420/00529] train_loss: 0.011699\n",
      "[420/00579] train_loss: 0.011048\n",
      "[420/00629] train_loss: 0.011010\n",
      "[420/00679] train_loss: 0.011160\n",
      "[420/00729] train_loss: 0.010817\n",
      "[420/00779] train_loss: 0.010885\n",
      "[420/00829] train_loss: 0.011362\n",
      "[420/00879] train_loss: 0.010858\n",
      "[420/00929] train_loss: 0.011348\n",
      "[420/00979] train_loss: 0.011481\n",
      "[420/01029] train_loss: 0.011269\n",
      "[420/01079] train_loss: 0.010920\n",
      "[420/01129] train_loss: 0.010592\n",
      "[420/01179] train_loss: 0.010863\n",
      "[421/00003] train_loss: 0.010735\n",
      "[421/00053] train_loss: 0.011485\n",
      "[421/00103] train_loss: 0.011869\n",
      "[421/00153] train_loss: 0.011401\n",
      "[421/00203] train_loss: 0.011063\n",
      "[421/00253] train_loss: 0.011334\n",
      "[421/00303] train_loss: 0.010790\n",
      "[421/00353] train_loss: 0.011681\n",
      "[421/00403] train_loss: 0.011306\n",
      "[421/00453] train_loss: 0.011723\n",
      "[421/00503] train_loss: 0.011864\n",
      "[421/00553] train_loss: 0.010894\n",
      "[421/00603] train_loss: 0.011417\n",
      "[421/00653] train_loss: 0.011267\n",
      "[421/00703] train_loss: 0.011154\n",
      "[421/00753] train_loss: 0.011820\n",
      "[421/00803] train_loss: 0.011840\n",
      "[421/00853] train_loss: 0.010951\n",
      "[421/00903] train_loss: 0.011173\n",
      "[421/00953] train_loss: 0.011339\n",
      "[421/01003] train_loss: 0.011503\n",
      "[421/01053] train_loss: 0.011214\n",
      "[421/01103] train_loss: 0.011213\n",
      "[421/01153] train_loss: 0.011011\n",
      "[421/01203] train_loss: 0.011553\n",
      "[422/00027] train_loss: 0.011184\n",
      "[422/00077] train_loss: 0.011549\n",
      "[422/00127] train_loss: 0.011491\n",
      "[422/00177] train_loss: 0.011086\n",
      "[422/00227] train_loss: 0.011533\n",
      "[422/00277] train_loss: 0.011150\n",
      "[422/00327] train_loss: 0.011389\n",
      "[422/00377] train_loss: 0.011206\n",
      "[422/00427] train_loss: 0.011594\n",
      "[422/00477] train_loss: 0.011089\n",
      "[422/00527] train_loss: 0.010751\n",
      "[422/00577] train_loss: 0.011803\n",
      "[422/00627] train_loss: 0.011702\n",
      "[422/00677] train_loss: 0.010909\n",
      "[422/00727] train_loss: 0.011187\n",
      "[422/00777] train_loss: 0.011018\n",
      "[422/00827] train_loss: 0.011445\n",
      "[422/00877] train_loss: 0.010753\n",
      "[422/00927] train_loss: 0.010719\n",
      "[422/00977] train_loss: 0.010649\n",
      "[422/01027] train_loss: 0.011612\n",
      "[422/01077] train_loss: 0.011164\n",
      "[422/01127] train_loss: 0.010953\n",
      "[422/01177] train_loss: 0.010930\n",
      "[423/00001] train_loss: 0.011244\n",
      "[423/00051] train_loss: 0.011552\n",
      "[423/00101] train_loss: 0.012771\n",
      "[423/00151] train_loss: 0.011733\n",
      "[423/00201] train_loss: 0.011709\n",
      "[423/00251] train_loss: 0.011840\n",
      "[423/00301] train_loss: 0.011168\n",
      "[423/00351] train_loss: 0.011370\n",
      "[423/00401] train_loss: 0.011022\n",
      "[423/00451] train_loss: 0.011724\n",
      "[423/00501] train_loss: 0.011510\n",
      "[423/00551] train_loss: 0.011528\n",
      "[423/00601] train_loss: 0.012177\n",
      "[423/00651] train_loss: 0.010953\n",
      "[423/00701] train_loss: 0.011131\n",
      "[423/00751] train_loss: 0.011925\n",
      "[423/00801] train_loss: 0.010923\n",
      "[423/00851] train_loss: 0.011451\n",
      "[423/00901] train_loss: 0.010799\n",
      "[423/00951] train_loss: 0.011268\n",
      "[423/01001] train_loss: 0.011062\n",
      "[423/01051] train_loss: 0.011271\n",
      "[423/01101] train_loss: 0.011211\n",
      "[423/01151] train_loss: 0.011241\n",
      "[423/01201] train_loss: 0.010840\n",
      "[424/00025] train_loss: 0.011059\n",
      "[424/00075] train_loss: 0.011686\n",
      "[424/00125] train_loss: 0.011133\n",
      "[424/00175] train_loss: 0.011474\n",
      "[424/00225] train_loss: 0.011783\n",
      "[424/00275] train_loss: 0.011504\n",
      "[424/00325] train_loss: 0.011085\n",
      "[424/00375] train_loss: 0.011391\n",
      "[424/00425] train_loss: 0.010858\n",
      "[424/00475] train_loss: 0.011587\n",
      "[424/00525] train_loss: 0.011345\n",
      "[424/00575] train_loss: 0.011433\n",
      "[424/00625] train_loss: 0.011123\n",
      "[424/00675] train_loss: 0.010665\n",
      "[424/00725] train_loss: 0.010939\n",
      "[424/00775] train_loss: 0.010814\n",
      "[424/00825] train_loss: 0.011514\n",
      "[424/00875] train_loss: 0.010957\n",
      "[424/00925] train_loss: 0.011613\n",
      "[424/00975] train_loss: 0.011211\n",
      "[424/01025] train_loss: 0.011576\n",
      "[424/01075] train_loss: 0.011530\n",
      "[424/01125] train_loss: 0.011016\n",
      "[424/01175] train_loss: 0.011070\n",
      "[424/01225] train_loss: 0.011554\n",
      "[425/00049] train_loss: 0.011515\n",
      "[425/00099] train_loss: 0.012052\n",
      "[425/00149] train_loss: 0.011519\n",
      "[425/00199] train_loss: 0.011665\n",
      "[425/00249] train_loss: 0.011041\n",
      "[425/00299] train_loss: 0.012204\n",
      "[425/00349] train_loss: 0.011336\n",
      "[425/00399] train_loss: 0.011170\n",
      "[425/00449] train_loss: 0.010980\n",
      "[425/00499] train_loss: 0.011421\n",
      "[425/00549] train_loss: 0.011128\n",
      "[425/00599] train_loss: 0.011502\n",
      "[425/00649] train_loss: 0.011349\n",
      "[425/00699] train_loss: 0.011387\n",
      "[425/00749] train_loss: 0.011603\n",
      "[425/00799] train_loss: 0.011421\n",
      "[425/00849] train_loss: 0.011046\n",
      "[425/00899] train_loss: 0.010817\n",
      "[425/00949] train_loss: 0.011413\n",
      "[425/00999] train_loss: 0.011104\n",
      "[425/01049] train_loss: 0.010844\n",
      "[425/01099] train_loss: 0.011329\n",
      "[425/01149] train_loss: 0.011564\n",
      "[425/01199] train_loss: 0.011360\n",
      "[426/00023] train_loss: 0.011636\n",
      "[426/00073] train_loss: 0.011876\n",
      "[426/00123] train_loss: 0.010943\n",
      "[426/00173] train_loss: 0.011444\n",
      "[426/00223] train_loss: 0.011848\n",
      "[426/00273] train_loss: 0.011488\n",
      "[426/00323] train_loss: 0.011453\n",
      "[426/00373] train_loss: 0.012093\n",
      "[426/00423] train_loss: 0.010577\n",
      "[426/00473] train_loss: 0.010401\n",
      "[426/00523] train_loss: 0.011165\n",
      "[426/00573] train_loss: 0.011675\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[426/00623] train_loss: 0.011159\n",
      "[426/00673] train_loss: 0.010827\n",
      "[426/00723] train_loss: 0.011475\n",
      "[426/00773] train_loss: 0.011923\n",
      "[426/00823] train_loss: 0.011610\n",
      "[426/00873] train_loss: 0.010910\n",
      "[426/00923] train_loss: 0.011134\n",
      "[426/00973] train_loss: 0.010390\n",
      "[426/01023] train_loss: 0.011074\n",
      "[426/01073] train_loss: 0.011357\n",
      "[426/01123] train_loss: 0.011451\n",
      "[426/01173] train_loss: 0.011259\n",
      "[426/01223] train_loss: 0.010995\n",
      "[427/00047] train_loss: 0.012732\n",
      "[427/00097] train_loss: 0.011988\n",
      "[427/00147] train_loss: 0.010963\n",
      "[427/00197] train_loss: 0.012368\n",
      "[427/00247] train_loss: 0.011981\n",
      "[427/00297] train_loss: 0.011623\n",
      "[427/00347] train_loss: 0.011397\n",
      "[427/00397] train_loss: 0.011526\n",
      "[427/00447] train_loss: 0.011770\n",
      "[427/00497] train_loss: 0.011542\n",
      "[427/00547] train_loss: 0.010880\n",
      "[427/00597] train_loss: 0.011524\n",
      "[427/00647] train_loss: 0.011019\n",
      "[427/00697] train_loss: 0.010621\n",
      "[427/00747] train_loss: 0.011159\n",
      "[427/00797] train_loss: 0.011179\n",
      "[427/00847] train_loss: 0.011054\n",
      "[427/00897] train_loss: 0.010897\n",
      "[427/00947] train_loss: 0.010871\n",
      "[427/00997] train_loss: 0.011653\n",
      "[427/01047] train_loss: 0.010989\n",
      "[427/01097] train_loss: 0.011260\n",
      "[427/01147] train_loss: 0.010919\n",
      "[427/01197] train_loss: 0.010597\n",
      "[428/00021] train_loss: 0.011706\n",
      "[428/00071] train_loss: 0.011646\n",
      "[428/00121] train_loss: 0.010964\n",
      "[428/00171] train_loss: 0.011513\n",
      "[428/00221] train_loss: 0.011411\n",
      "[428/00271] train_loss: 0.010862\n",
      "[428/00321] train_loss: 0.011319\n",
      "[428/00371] train_loss: 0.010961\n",
      "[428/00421] train_loss: 0.011435\n",
      "[428/00471] train_loss: 0.011363\n",
      "[428/00521] train_loss: 0.011148\n",
      "[428/00571] train_loss: 0.011084\n",
      "[428/00621] train_loss: 0.011467\n",
      "[428/00671] train_loss: 0.010765\n",
      "[428/00721] train_loss: 0.011548\n",
      "[428/00771] train_loss: 0.011797\n",
      "[428/00821] train_loss: 0.010816\n",
      "[428/00871] train_loss: 0.011099\n",
      "[428/00921] train_loss: 0.011324\n",
      "[428/00971] train_loss: 0.010642\n",
      "[428/01021] train_loss: 0.011419\n",
      "[428/01071] train_loss: 0.011514\n",
      "[428/01121] train_loss: 0.011310\n",
      "[428/01171] train_loss: 0.011375\n",
      "[428/01221] train_loss: 0.011250\n",
      "[429/00045] train_loss: 0.012156\n",
      "[429/00095] train_loss: 0.012413\n",
      "[429/00145] train_loss: 0.011251\n",
      "[429/00195] train_loss: 0.012167\n",
      "[429/00245] train_loss: 0.012051\n",
      "[429/00295] train_loss: 0.010928\n",
      "[429/00345] train_loss: 0.011816\n",
      "[429/00395] train_loss: 0.011292\n",
      "[429/00445] train_loss: 0.011576\n",
      "[429/00495] train_loss: 0.011688\n",
      "[429/00545] train_loss: 0.011993\n",
      "[429/00595] train_loss: 0.011145\n",
      "[429/00645] train_loss: 0.011247\n",
      "[429/00695] train_loss: 0.010918\n",
      "[429/00745] train_loss: 0.011612\n",
      "[429/00795] train_loss: 0.011527\n",
      "[429/00845] train_loss: 0.010846\n",
      "[429/00895] train_loss: 0.010663\n",
      "[429/00945] train_loss: 0.011005\n",
      "[429/00995] train_loss: 0.010809\n",
      "[429/01045] train_loss: 0.011687\n",
      "[429/01095] train_loss: 0.010389\n",
      "[429/01145] train_loss: 0.011490\n",
      "[429/01195] train_loss: 0.010851\n",
      "[430/00019] train_loss: 0.011349\n",
      "[430/00069] train_loss: 0.011762\n",
      "[430/00119] train_loss: 0.011818\n",
      "[430/00169] train_loss: 0.011172\n",
      "[430/00219] train_loss: 0.011121\n",
      "[430/00269] train_loss: 0.011219\n",
      "[430/00319] train_loss: 0.011398\n",
      "[430/00369] train_loss: 0.011016\n",
      "[430/00419] train_loss: 0.011236\n",
      "[430/00469] train_loss: 0.011529\n",
      "[430/00519] train_loss: 0.011717\n",
      "[430/00569] train_loss: 0.011088\n",
      "[430/00619] train_loss: 0.011132\n",
      "[430/00669] train_loss: 0.010341\n",
      "[430/00719] train_loss: 0.011677\n",
      "[430/00769] train_loss: 0.010860\n",
      "[430/00819] train_loss: 0.011279\n",
      "[430/00869] train_loss: 0.010758\n",
      "[430/00919] train_loss: 0.011716\n",
      "[430/00969] train_loss: 0.011219\n",
      "[430/01019] train_loss: 0.011212\n",
      "[430/01069] train_loss: 0.010892\n",
      "[430/01119] train_loss: 0.011180\n",
      "[430/01169] train_loss: 0.011372\n",
      "[430/01219] train_loss: 0.011025\n",
      "[431/00043] train_loss: 0.011193\n",
      "[431/00093] train_loss: 0.011587\n",
      "[431/00143] train_loss: 0.011135\n",
      "[431/00193] train_loss: 0.011260\n",
      "[431/00243] train_loss: 0.011762\n",
      "[431/00293] train_loss: 0.011523\n",
      "[431/00343] train_loss: 0.011654\n",
      "[431/00393] train_loss: 0.011421\n",
      "[431/00443] train_loss: 0.011913\n",
      "[431/00493] train_loss: 0.012093\n",
      "[431/00543] train_loss: 0.011350\n",
      "[431/00593] train_loss: 0.010847\n",
      "[431/00643] train_loss: 0.011694\n",
      "[431/00693] train_loss: 0.011102\n",
      "[431/00743] train_loss: 0.011986\n",
      "[431/00793] train_loss: 0.010713\n",
      "[431/00843] train_loss: 0.011145\n",
      "[431/00893] train_loss: 0.011805\n",
      "[431/00943] train_loss: 0.010875\n",
      "[431/00993] train_loss: 0.011448\n",
      "[431/01043] train_loss: 0.011033\n",
      "[431/01093] train_loss: 0.010786\n",
      "[431/01143] train_loss: 0.010499\n",
      "[431/01193] train_loss: 0.011240\n",
      "[432/00017] train_loss: 0.011281\n",
      "[432/00067] train_loss: 0.011668\n",
      "[432/00117] train_loss: 0.011556\n",
      "[432/00167] train_loss: 0.011605\n",
      "[432/00217] train_loss: 0.011341\n",
      "[432/00267] train_loss: 0.011300\n",
      "[432/00317] train_loss: 0.012073\n",
      "[432/00367] train_loss: 0.011091\n",
      "[432/00417] train_loss: 0.011746\n",
      "[432/00467] train_loss: 0.011945\n",
      "[432/00517] train_loss: 0.011338\n",
      "[432/00567] train_loss: 0.010444\n",
      "[432/00617] train_loss: 0.010745\n",
      "[432/00667] train_loss: 0.011211\n",
      "[432/00717] train_loss: 0.011189\n",
      "[432/00767] train_loss: 0.011105\n",
      "[432/00817] train_loss: 0.010828\n",
      "[432/00867] train_loss: 0.010803\n",
      "[432/00917] train_loss: 0.011556\n",
      "[432/00967] train_loss: 0.010620\n",
      "[432/01017] train_loss: 0.010719\n",
      "[432/01067] train_loss: 0.011185\n",
      "[432/01117] train_loss: 0.011287\n",
      "[432/01167] train_loss: 0.011512\n",
      "[432/01217] train_loss: 0.010798\n",
      "[433/00041] train_loss: 0.011885\n",
      "[433/00091] train_loss: 0.011270\n",
      "[433/00141] train_loss: 0.011315\n",
      "[433/00191] train_loss: 0.011679\n",
      "[433/00241] train_loss: 0.011082\n",
      "[433/00291] train_loss: 0.010978\n",
      "[433/00341] train_loss: 0.011335\n",
      "[433/00391] train_loss: 0.012209\n",
      "[433/00441] train_loss: 0.010920\n",
      "[433/00491] train_loss: 0.011624\n",
      "[433/00541] train_loss: 0.011237\n",
      "[433/00591] train_loss: 0.011581\n",
      "[433/00641] train_loss: 0.010803\n",
      "[433/00691] train_loss: 0.011503\n",
      "[433/00741] train_loss: 0.011419\n",
      "[433/00791] train_loss: 0.011686\n",
      "[433/00841] train_loss: 0.011191\n",
      "[433/00891] train_loss: 0.011119\n",
      "[433/00941] train_loss: 0.011271\n",
      "[433/00991] train_loss: 0.011386\n",
      "[433/01041] train_loss: 0.011250\n",
      "[433/01091] train_loss: 0.011024\n",
      "[433/01141] train_loss: 0.011178\n",
      "[433/01191] train_loss: 0.011063\n",
      "[434/00015] train_loss: 0.011420\n",
      "[434/00065] train_loss: 0.011527\n",
      "[434/00115] train_loss: 0.011095\n",
      "[434/00165] train_loss: 0.011125\n",
      "[434/00215] train_loss: 0.010527\n",
      "[434/00265] train_loss: 0.010950\n",
      "[434/00315] train_loss: 0.011439\n",
      "[434/00365] train_loss: 0.011854\n",
      "[434/00415] train_loss: 0.011490\n",
      "[434/00465] train_loss: 0.010779\n",
      "[434/00515] train_loss: 0.011705\n",
      "[434/00565] train_loss: 0.011109\n",
      "[434/00615] train_loss: 0.011121\n",
      "[434/00665] train_loss: 0.011319\n",
      "[434/00715] train_loss: 0.011313\n",
      "[434/00765] train_loss: 0.010913\n",
      "[434/00815] train_loss: 0.011282\n",
      "[434/00865] train_loss: 0.011156\n",
      "[434/00915] train_loss: 0.011086\n",
      "[434/00965] train_loss: 0.011442\n",
      "[434/01015] train_loss: 0.011781\n",
      "[434/01065] train_loss: 0.011327\n",
      "[434/01115] train_loss: 0.011068\n",
      "[434/01165] train_loss: 0.010627\n",
      "[434/01215] train_loss: 0.011119\n",
      "[435/00039] train_loss: 0.011463\n",
      "[435/00089] train_loss: 0.011988\n",
      "[435/00139] train_loss: 0.011304\n",
      "[435/00189] train_loss: 0.011789\n",
      "[435/00239] train_loss: 0.012359\n",
      "[435/00289] train_loss: 0.011404\n",
      "[435/00339] train_loss: 0.011378\n",
      "[435/00389] train_loss: 0.011579\n",
      "[435/00439] train_loss: 0.011392\n",
      "[435/00489] train_loss: 0.010516\n",
      "[435/00539] train_loss: 0.011256\n",
      "[435/00589] train_loss: 0.010897\n",
      "[435/00639] train_loss: 0.011820\n",
      "[435/00689] train_loss: 0.010824\n",
      "[435/00739] train_loss: 0.011745\n",
      "[435/00789] train_loss: 0.011651\n",
      "[435/00839] train_loss: 0.010839\n",
      "[435/00889] train_loss: 0.011038\n",
      "[435/00939] train_loss: 0.011481\n",
      "[435/00989] train_loss: 0.011681\n",
      "[435/01039] train_loss: 0.011184\n",
      "[435/01089] train_loss: 0.011168\n",
      "[435/01139] train_loss: 0.010902\n",
      "[435/01189] train_loss: 0.010613\n",
      "[436/00013] train_loss: 0.011633\n",
      "[436/00063] train_loss: 0.011427\n",
      "[436/00113] train_loss: 0.011456\n",
      "[436/00163] train_loss: 0.011182\n",
      "[436/00213] train_loss: 0.010963\n",
      "[436/00263] train_loss: 0.011588\n",
      "[436/00313] train_loss: 0.011629\n",
      "[436/00363] train_loss: 0.011488\n",
      "[436/00413] train_loss: 0.011722\n",
      "[436/00463] train_loss: 0.011091\n",
      "[436/00513] train_loss: 0.011343\n",
      "[436/00563] train_loss: 0.011316\n",
      "[436/00613] train_loss: 0.010821\n",
      "[436/00663] train_loss: 0.011638\n",
      "[436/00713] train_loss: 0.011144\n",
      "[436/00763] train_loss: 0.010834\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[436/00813] train_loss: 0.011398\n",
      "[436/00863] train_loss: 0.011499\n",
      "[436/00913] train_loss: 0.011235\n",
      "[436/00963] train_loss: 0.011161\n",
      "[436/01013] train_loss: 0.010931\n",
      "[436/01063] train_loss: 0.010837\n",
      "[436/01113] train_loss: 0.010440\n",
      "[436/01163] train_loss: 0.010654\n",
      "[436/01213] train_loss: 0.010463\n",
      "[437/00037] train_loss: 0.011525\n",
      "[437/00087] train_loss: 0.012285\n",
      "[437/00137] train_loss: 0.011318\n",
      "[437/00187] train_loss: 0.011162\n",
      "[437/00237] train_loss: 0.011396\n",
      "[437/00287] train_loss: 0.011626\n",
      "[437/00337] train_loss: 0.011584\n",
      "[437/00387] train_loss: 0.011453\n",
      "[437/00437] train_loss: 0.011141\n",
      "[437/00487] train_loss: 0.011731\n",
      "[437/00537] train_loss: 0.010860\n",
      "[437/00587] train_loss: 0.011590\n",
      "[437/00637] train_loss: 0.011106\n",
      "[437/00687] train_loss: 0.011358\n",
      "[437/00737] train_loss: 0.010509\n",
      "[437/00787] train_loss: 0.011324\n",
      "[437/00837] train_loss: 0.011118\n",
      "[437/00887] train_loss: 0.010845\n",
      "[437/00937] train_loss: 0.011696\n",
      "[437/00987] train_loss: 0.010933\n",
      "[437/01037] train_loss: 0.011640\n",
      "[437/01087] train_loss: 0.010968\n",
      "[437/01137] train_loss: 0.011044\n",
      "[437/01187] train_loss: 0.011159\n",
      "[438/00011] train_loss: 0.011464\n",
      "[438/00061] train_loss: 0.011665\n",
      "[438/00111] train_loss: 0.012012\n",
      "[438/00161] train_loss: 0.010924\n",
      "[438/00211] train_loss: 0.010898\n",
      "[438/00261] train_loss: 0.012213\n",
      "[438/00311] train_loss: 0.011392\n",
      "[438/00361] train_loss: 0.011628\n",
      "[438/00411] train_loss: 0.011581\n",
      "[438/00461] train_loss: 0.011243\n",
      "[438/00511] train_loss: 0.010607\n",
      "[438/00561] train_loss: 0.010936\n",
      "[438/00611] train_loss: 0.011126\n",
      "[438/00661] train_loss: 0.011367\n",
      "[438/00711] train_loss: 0.010411\n",
      "[438/00761] train_loss: 0.010864\n",
      "[438/00811] train_loss: 0.010606\n",
      "[438/00861] train_loss: 0.011126\n",
      "[438/00911] train_loss: 0.010566\n",
      "[438/00961] train_loss: 0.011340\n",
      "[438/01011] train_loss: 0.010803\n",
      "[438/01061] train_loss: 0.011191\n",
      "[438/01111] train_loss: 0.011898\n",
      "[438/01161] train_loss: 0.011683\n",
      "[438/01211] train_loss: 0.010839\n",
      "[439/00035] train_loss: 0.011766\n",
      "[439/00085] train_loss: 0.011624\n",
      "[439/00135] train_loss: 0.011789\n",
      "[439/00185] train_loss: 0.011554\n",
      "[439/00235] train_loss: 0.011465\n",
      "[439/00285] train_loss: 0.011699\n",
      "[439/00335] train_loss: 0.011548\n",
      "[439/00385] train_loss: 0.011301\n",
      "[439/00435] train_loss: 0.010752\n",
      "[439/00485] train_loss: 0.010711\n",
      "[439/00535] train_loss: 0.011500\n",
      "[439/00585] train_loss: 0.011490\n",
      "[439/00635] train_loss: 0.010830\n",
      "[439/00685] train_loss: 0.011885\n",
      "[439/00735] train_loss: 0.011260\n",
      "[439/00785] train_loss: 0.011444\n",
      "[439/00835] train_loss: 0.011430\n",
      "[439/00885] train_loss: 0.011819\n",
      "[439/00935] train_loss: 0.011079\n",
      "[439/00985] train_loss: 0.011392\n",
      "[439/01035] train_loss: 0.011209\n",
      "[439/01085] train_loss: 0.010436\n",
      "[439/01135] train_loss: 0.011151\n",
      "[439/01185] train_loss: 0.010322\n",
      "[440/00009] train_loss: 0.011764\n",
      "[440/00059] train_loss: 0.011738\n",
      "[440/00109] train_loss: 0.011237\n",
      "[440/00159] train_loss: 0.010940\n",
      "[440/00209] train_loss: 0.011588\n",
      "[440/00259] train_loss: 0.011238\n",
      "[440/00309] train_loss: 0.011060\n",
      "[440/00359] train_loss: 0.011422\n",
      "[440/00409] train_loss: 0.011303\n",
      "[440/00459] train_loss: 0.011015\n",
      "[440/00509] train_loss: 0.010987\n",
      "[440/00559] train_loss: 0.010919\n",
      "[440/00609] train_loss: 0.011043\n",
      "[440/00659] train_loss: 0.010720\n",
      "[440/00709] train_loss: 0.010913\n",
      "[440/00759] train_loss: 0.011260\n",
      "[440/00809] train_loss: 0.011409\n",
      "[440/00859] train_loss: 0.011123\n",
      "[440/00909] train_loss: 0.011224\n",
      "[440/00959] train_loss: 0.011548\n",
      "[440/01009] train_loss: 0.010784\n",
      "[440/01059] train_loss: 0.011377\n",
      "[440/01109] train_loss: 0.011228\n",
      "[440/01159] train_loss: 0.011316\n",
      "[440/01209] train_loss: 0.011318\n",
      "[441/00033] train_loss: 0.011264\n",
      "[441/00083] train_loss: 0.011729\n",
      "[441/00133] train_loss: 0.010945\n",
      "[441/00183] train_loss: 0.010713\n",
      "[441/00233] train_loss: 0.012000\n",
      "[441/00283] train_loss: 0.011784\n",
      "[441/00333] train_loss: 0.010989\n",
      "[441/00383] train_loss: 0.011503\n",
      "[441/00433] train_loss: 0.010952\n",
      "[441/00483] train_loss: 0.011465\n",
      "[441/00533] train_loss: 0.010513\n",
      "[441/00583] train_loss: 0.011013\n",
      "[441/00633] train_loss: 0.011557\n",
      "[441/00683] train_loss: 0.011745\n",
      "[441/00733] train_loss: 0.011868\n",
      "[441/00783] train_loss: 0.011566\n",
      "[441/00833] train_loss: 0.011942\n",
      "[441/00883] train_loss: 0.010706\n",
      "[441/00933] train_loss: 0.011396\n",
      "[441/00983] train_loss: 0.011430\n",
      "[441/01033] train_loss: 0.011303\n",
      "[441/01083] train_loss: 0.011042\n",
      "[441/01133] train_loss: 0.010585\n",
      "[441/01183] train_loss: 0.010914\n",
      "[442/00007] train_loss: 0.011476\n",
      "[442/00057] train_loss: 0.011264\n",
      "[442/00107] train_loss: 0.011885\n",
      "[442/00157] train_loss: 0.011748\n",
      "[442/00207] train_loss: 0.011465\n",
      "[442/00257] train_loss: 0.010538\n",
      "[442/00307] train_loss: 0.011308\n",
      "[442/00357] train_loss: 0.011481\n",
      "[442/00407] train_loss: 0.011347\n",
      "[442/00457] train_loss: 0.011197\n",
      "[442/00507] train_loss: 0.011766\n",
      "[442/00557] train_loss: 0.011040\n",
      "[442/00607] train_loss: 0.011728\n",
      "[442/00657] train_loss: 0.011368\n",
      "[442/00707] train_loss: 0.010265\n",
      "[442/00757] train_loss: 0.011018\n",
      "[442/00807] train_loss: 0.011188\n",
      "[442/00857] train_loss: 0.011004\n",
      "[442/00907] train_loss: 0.010881\n",
      "[442/00957] train_loss: 0.011873\n",
      "[442/01007] train_loss: 0.011208\n",
      "[442/01057] train_loss: 0.010430\n",
      "[442/01107] train_loss: 0.011449\n",
      "[442/01157] train_loss: 0.009954\n",
      "[442/01207] train_loss: 0.011360\n",
      "[443/00031] train_loss: 0.011390\n",
      "[443/00081] train_loss: 0.011936\n",
      "[443/00131] train_loss: 0.011637\n",
      "[443/00181] train_loss: 0.011702\n",
      "[443/00231] train_loss: 0.011259\n",
      "[443/00281] train_loss: 0.011302\n",
      "[443/00331] train_loss: 0.012197\n",
      "[443/00381] train_loss: 0.011279\n",
      "[443/00431] train_loss: 0.011391\n",
      "[443/00481] train_loss: 0.011418\n",
      "[443/00531] train_loss: 0.011215\n",
      "[443/00581] train_loss: 0.011010\n",
      "[443/00631] train_loss: 0.011569\n",
      "[443/00681] train_loss: 0.010701\n",
      "[443/00731] train_loss: 0.011569\n",
      "[443/00781] train_loss: 0.011280\n",
      "[443/00831] train_loss: 0.010460\n",
      "[443/00881] train_loss: 0.011210\n",
      "[443/00931] train_loss: 0.011329\n",
      "[443/00981] train_loss: 0.011717\n",
      "[443/01031] train_loss: 0.010528\n",
      "[443/01081] train_loss: 0.011348\n",
      "[443/01131] train_loss: 0.011257\n",
      "[443/01181] train_loss: 0.011061\n",
      "[444/00005] train_loss: 0.010912\n",
      "[444/00055] train_loss: 0.011178\n",
      "[444/00105] train_loss: 0.011288\n",
      "[444/00155] train_loss: 0.011040\n",
      "[444/00205] train_loss: 0.011303\n",
      "[444/00255] train_loss: 0.011029\n",
      "[444/00305] train_loss: 0.011448\n",
      "[444/00355] train_loss: 0.011405\n",
      "[444/00405] train_loss: 0.011038\n",
      "[444/00455] train_loss: 0.011270\n",
      "[444/00505] train_loss: 0.010898\n",
      "[444/00555] train_loss: 0.011256\n",
      "[444/00605] train_loss: 0.011331\n",
      "[444/00655] train_loss: 0.011643\n",
      "[444/00705] train_loss: 0.011258\n",
      "[444/00755] train_loss: 0.011164\n",
      "[444/00805] train_loss: 0.011230\n",
      "[444/00855] train_loss: 0.011040\n",
      "[444/00905] train_loss: 0.011101\n",
      "[444/00955] train_loss: 0.011715\n",
      "[444/01005] train_loss: 0.011484\n",
      "[444/01055] train_loss: 0.010849\n",
      "[444/01105] train_loss: 0.011381\n",
      "[444/01155] train_loss: 0.011071\n",
      "[444/01205] train_loss: 0.011788\n",
      "[445/00029] train_loss: 0.011802\n",
      "[445/00079] train_loss: 0.011473\n",
      "[445/00129] train_loss: 0.011443\n",
      "[445/00179] train_loss: 0.012309\n",
      "[445/00229] train_loss: 0.011383\n",
      "[445/00279] train_loss: 0.011963\n",
      "[445/00329] train_loss: 0.011491\n",
      "[445/00379] train_loss: 0.011014\n",
      "[445/00429] train_loss: 0.011165\n",
      "[445/00479] train_loss: 0.011108\n",
      "[445/00529] train_loss: 0.011399\n",
      "[445/00579] train_loss: 0.011647\n",
      "[445/00629] train_loss: 0.011316\n",
      "[445/00679] train_loss: 0.010580\n",
      "[445/00729] train_loss: 0.011856\n",
      "[445/00779] train_loss: 0.011909\n",
      "[445/00829] train_loss: 0.010761\n",
      "[445/00879] train_loss: 0.011225\n",
      "[445/00929] train_loss: 0.011251\n",
      "[445/00979] train_loss: 0.011293\n",
      "[445/01029] train_loss: 0.010792\n",
      "[445/01079] train_loss: 0.011202\n",
      "[445/01129] train_loss: 0.011052\n",
      "[445/01179] train_loss: 0.010512\n",
      "[446/00003] train_loss: 0.010676\n",
      "[446/00053] train_loss: 0.011025\n",
      "[446/00103] train_loss: 0.010903\n",
      "[446/00153] train_loss: 0.010887\n",
      "[446/00203] train_loss: 0.011383\n",
      "[446/00253] train_loss: 0.011126\n",
      "[446/00303] train_loss: 0.011371\n",
      "[446/00353] train_loss: 0.011430\n",
      "[446/00403] train_loss: 0.011941\n",
      "[446/00453] train_loss: 0.011982\n",
      "[446/00503] train_loss: 0.011423\n",
      "[446/00553] train_loss: 0.011156\n",
      "[446/00603] train_loss: 0.011487\n",
      "[446/00653] train_loss: 0.010353\n",
      "[446/00703] train_loss: 0.011482\n",
      "[446/00753] train_loss: 0.010056\n",
      "[446/00803] train_loss: 0.010788\n",
      "[446/00853] train_loss: 0.011670\n",
      "[446/00903] train_loss: 0.010740\n",
      "[446/00953] train_loss: 0.010632\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[446/01003] train_loss: 0.010494\n",
      "[446/01053] train_loss: 0.010912\n",
      "[446/01103] train_loss: 0.011301\n",
      "[446/01153] train_loss: 0.011102\n",
      "[446/01203] train_loss: 0.011277\n",
      "[447/00027] train_loss: 0.011390\n",
      "[447/00077] train_loss: 0.011638\n",
      "[447/00127] train_loss: 0.011536\n",
      "[447/00177] train_loss: 0.011662\n",
      "[447/00227] train_loss: 0.011576\n",
      "[447/00277] train_loss: 0.011400\n",
      "[447/00327] train_loss: 0.011879\n",
      "[447/00377] train_loss: 0.011029\n",
      "[447/00427] train_loss: 0.011387\n",
      "[447/00477] train_loss: 0.010764\n",
      "[447/00527] train_loss: 0.011547\n",
      "[447/00577] train_loss: 0.011728\n",
      "[447/00627] train_loss: 0.011683\n",
      "[447/00677] train_loss: 0.011387\n",
      "[447/00727] train_loss: 0.011382\n",
      "[447/00777] train_loss: 0.011449\n",
      "[447/00827] train_loss: 0.011480\n",
      "[447/00877] train_loss: 0.011063\n",
      "[447/00927] train_loss: 0.010634\n",
      "[447/00977] train_loss: 0.011530\n",
      "[447/01027] train_loss: 0.011661\n",
      "[447/01077] train_loss: 0.010587\n",
      "[447/01127] train_loss: 0.011213\n",
      "[447/01177] train_loss: 0.010841\n",
      "[448/00001] train_loss: 0.010877\n",
      "[448/00051] train_loss: 0.011393\n",
      "[448/00101] train_loss: 0.011522\n",
      "[448/00151] train_loss: 0.011614\n",
      "[448/00201] train_loss: 0.011056\n",
      "[448/00251] train_loss: 0.011991\n",
      "[448/00301] train_loss: 0.010687\n",
      "[448/00351] train_loss: 0.011352\n",
      "[448/00401] train_loss: 0.011964\n",
      "[448/00451] train_loss: 0.011147\n",
      "[448/00501] train_loss: 0.010498\n",
      "[448/00551] train_loss: 0.011150\n",
      "[448/00601] train_loss: 0.010925\n",
      "[448/00651] train_loss: 0.011049\n",
      "[448/00701] train_loss: 0.010978\n",
      "[448/00751] train_loss: 0.011050\n",
      "[448/00801] train_loss: 0.010858\n",
      "[448/00851] train_loss: 0.011481\n",
      "[448/00901] train_loss: 0.011460\n",
      "[448/00951] train_loss: 0.010772\n",
      "[448/01001] train_loss: 0.011496\n",
      "[448/01051] train_loss: 0.010279\n",
      "[448/01101] train_loss: 0.011299\n",
      "[448/01151] train_loss: 0.010776\n",
      "[448/01201] train_loss: 0.010952\n",
      "[449/00025] train_loss: 0.011202\n",
      "[449/00075] train_loss: 0.011612\n",
      "[449/00125] train_loss: 0.011131\n",
      "[449/00175] train_loss: 0.011431\n",
      "[449/00225] train_loss: 0.011415\n",
      "[449/00275] train_loss: 0.011448\n",
      "[449/00325] train_loss: 0.011880\n",
      "[449/00375] train_loss: 0.011268\n",
      "[449/00425] train_loss: 0.011111\n",
      "[449/00475] train_loss: 0.010792\n",
      "[449/00525] train_loss: 0.011077\n",
      "[449/00575] train_loss: 0.011714\n",
      "[449/00625] train_loss: 0.011107\n",
      "[449/00675] train_loss: 0.010784\n",
      "[449/00725] train_loss: 0.011085\n",
      "[449/00775] train_loss: 0.011070\n",
      "[449/00825] train_loss: 0.011225\n",
      "[449/00875] train_loss: 0.010552\n",
      "[449/00925] train_loss: 0.011479\n",
      "[449/00975] train_loss: 0.011840\n",
      "[449/01025] train_loss: 0.011231\n",
      "[449/01075] train_loss: 0.011240\n",
      "[449/01125] train_loss: 0.011699\n",
      "[449/01175] train_loss: 0.011104\n",
      "[449/01225] train_loss: 0.011234\n",
      "[450/00049] train_loss: 0.011597\n",
      "[450/00099] train_loss: 0.011822\n",
      "[450/00149] train_loss: 0.011223\n",
      "[450/00199] train_loss: 0.011451\n",
      "[450/00249] train_loss: 0.010888\n",
      "[450/00299] train_loss: 0.010816\n",
      "[450/00349] train_loss: 0.011450\n",
      "[450/00399] train_loss: 0.011637\n",
      "[450/00449] train_loss: 0.011183\n",
      "[450/00499] train_loss: 0.011114\n",
      "[450/00549] train_loss: 0.011513\n",
      "[450/00599] train_loss: 0.010831\n",
      "[450/00649] train_loss: 0.011304\n",
      "[450/00699] train_loss: 0.011281\n",
      "[450/00749] train_loss: 0.010636\n",
      "[450/00799] train_loss: 0.010995\n",
      "[450/00849] train_loss: 0.011201\n",
      "[450/00899] train_loss: 0.010657\n",
      "[450/00949] train_loss: 0.010605\n",
      "[450/00999] train_loss: 0.010866\n",
      "[450/01049] train_loss: 0.011343\n",
      "[450/01099] train_loss: 0.011077\n",
      "[450/01149] train_loss: 0.010649\n",
      "[450/01199] train_loss: 0.011273\n",
      "[451/00023] train_loss: 0.011446\n",
      "[451/00073] train_loss: 0.011416\n",
      "[451/00123] train_loss: 0.011156\n",
      "[451/00173] train_loss: 0.010835\n",
      "[451/00223] train_loss: 0.011234\n",
      "[451/00273] train_loss: 0.011408\n",
      "[451/00323] train_loss: 0.011556\n",
      "[451/00373] train_loss: 0.010754\n",
      "[451/00423] train_loss: 0.010976\n",
      "[451/00473] train_loss: 0.011599\n",
      "[451/00523] train_loss: 0.011264\n",
      "[451/00573] train_loss: 0.011880\n",
      "[451/00623] train_loss: 0.012437\n",
      "[451/00673] train_loss: 0.010948\n",
      "[451/00723] train_loss: 0.010955\n",
      "[451/00773] train_loss: 0.010812\n",
      "[451/00823] train_loss: 0.011904\n",
      "[451/00873] train_loss: 0.010839\n",
      "[451/00923] train_loss: 0.011144\n",
      "[451/00973] train_loss: 0.010398\n",
      "[451/01023] train_loss: 0.011564\n",
      "[451/01073] train_loss: 0.010548\n",
      "[451/01123] train_loss: 0.011621\n",
      "[451/01173] train_loss: 0.011779\n",
      "[451/01223] train_loss: 0.011318\n",
      "[452/00047] train_loss: 0.011194\n",
      "[452/00097] train_loss: 0.011107\n",
      "[452/00147] train_loss: 0.011807\n",
      "[452/00197] train_loss: 0.011047\n",
      "[452/00247] train_loss: 0.011688\n",
      "[452/00297] train_loss: 0.011344\n",
      "[452/00347] train_loss: 0.010841\n",
      "[452/00397] train_loss: 0.010994\n",
      "[452/00447] train_loss: 0.011015\n",
      "[452/00497] train_loss: 0.010626\n",
      "[452/00547] train_loss: 0.011456\n",
      "[452/00597] train_loss: 0.011592\n",
      "[452/00647] train_loss: 0.011250\n",
      "[452/00697] train_loss: 0.010912\n",
      "[452/00747] train_loss: 0.010967\n",
      "[452/00797] train_loss: 0.010143\n",
      "[452/00847] train_loss: 0.010658\n",
      "[452/00897] train_loss: 0.010706\n",
      "[452/00947] train_loss: 0.010722\n",
      "[452/00997] train_loss: 0.011988\n",
      "[452/01047] train_loss: 0.011643\n",
      "[452/01097] train_loss: 0.010345\n",
      "[452/01147] train_loss: 0.011499\n",
      "[452/01197] train_loss: 0.011011\n",
      "[453/00021] train_loss: 0.010782\n",
      "[453/00071] train_loss: 0.011413\n",
      "[453/00121] train_loss: 0.011327\n",
      "[453/00171] train_loss: 0.011691\n",
      "[453/00221] train_loss: 0.011844\n",
      "[453/00271] train_loss: 0.011443\n",
      "[453/00321] train_loss: 0.010559\n",
      "[453/00371] train_loss: 0.011430\n",
      "[453/00421] train_loss: 0.011049\n",
      "[453/00471] train_loss: 0.011780\n",
      "[453/00521] train_loss: 0.011241\n",
      "[453/00571] train_loss: 0.011298\n",
      "[453/00621] train_loss: 0.010799\n",
      "[453/00671] train_loss: 0.011227\n",
      "[453/00721] train_loss: 0.011354\n",
      "[453/00771] train_loss: 0.011315\n",
      "[453/00821] train_loss: 0.011482\n",
      "[453/00871] train_loss: 0.011103\n",
      "[453/00921] train_loss: 0.011244\n",
      "[453/00971] train_loss: 0.010746\n",
      "[453/01021] train_loss: 0.011434\n",
      "[453/01071] train_loss: 0.010693\n",
      "[453/01121] train_loss: 0.011157\n",
      "[453/01171] train_loss: 0.011731\n",
      "[453/01221] train_loss: 0.011355\n",
      "[454/00045] train_loss: 0.011736\n",
      "[454/00095] train_loss: 0.012112\n",
      "[454/00145] train_loss: 0.010797\n",
      "[454/00195] train_loss: 0.011447\n",
      "[454/00245] train_loss: 0.011258\n",
      "[454/00295] train_loss: 0.010526\n",
      "[454/00345] train_loss: 0.010457\n",
      "[454/00395] train_loss: 0.010619\n",
      "[454/00445] train_loss: 0.010911\n",
      "[454/00495] train_loss: 0.011291\n",
      "[454/00545] train_loss: 0.010942\n",
      "[454/00595] train_loss: 0.011315\n",
      "[454/00645] train_loss: 0.011093\n",
      "[454/00695] train_loss: 0.010950\n",
      "[454/00745] train_loss: 0.011221\n",
      "[454/00795] train_loss: 0.011140\n",
      "[454/00845] train_loss: 0.011440\n",
      "[454/00895] train_loss: 0.011275\n",
      "[454/00945] train_loss: 0.011655\n",
      "[454/00995] train_loss: 0.010885\n",
      "[454/01045] train_loss: 0.010934\n",
      "[454/01095] train_loss: 0.011537\n",
      "[454/01145] train_loss: 0.010728\n",
      "[454/01195] train_loss: 0.011295\n",
      "[455/00019] train_loss: 0.011069\n",
      "[455/00069] train_loss: 0.011474\n",
      "[455/00119] train_loss: 0.011885\n",
      "[455/00169] train_loss: 0.011490\n",
      "[455/00219] train_loss: 0.011240\n",
      "[455/00269] train_loss: 0.011618\n",
      "[455/00319] train_loss: 0.011682\n",
      "[455/00369] train_loss: 0.011571\n",
      "[455/00419] train_loss: 0.010846\n",
      "[455/00469] train_loss: 0.011396\n",
      "[455/00519] train_loss: 0.011449\n",
      "[455/00569] train_loss: 0.011330\n",
      "[455/00619] train_loss: 0.011028\n",
      "[455/00669] train_loss: 0.011141\n",
      "[455/00719] train_loss: 0.011879\n",
      "[455/00769] train_loss: 0.010495\n",
      "[455/00819] train_loss: 0.010834\n",
      "[455/00869] train_loss: 0.011400\n",
      "[455/00919] train_loss: 0.011022\n",
      "[455/00969] train_loss: 0.011351\n",
      "[455/01019] train_loss: 0.011442\n",
      "[455/01069] train_loss: 0.011054\n",
      "[455/01119] train_loss: 0.010656\n",
      "[455/01169] train_loss: 0.011395\n",
      "[455/01219] train_loss: 0.011365\n",
      "[456/00043] train_loss: 0.011099\n",
      "[456/00093] train_loss: 0.011107\n",
      "[456/00143] train_loss: 0.011732\n",
      "[456/00193] train_loss: 0.011726\n",
      "[456/00243] train_loss: 0.011071\n",
      "[456/00293] train_loss: 0.011706\n",
      "[456/00343] train_loss: 0.011000\n",
      "[456/00393] train_loss: 0.011164\n",
      "[456/00443] train_loss: 0.011197\n",
      "[456/00493] train_loss: 0.011046\n",
      "[456/00543] train_loss: 0.010652\n",
      "[456/00593] train_loss: 0.011075\n",
      "[456/00643] train_loss: 0.010717\n",
      "[456/00693] train_loss: 0.011030\n",
      "[456/00743] train_loss: 0.010693\n",
      "[456/00793] train_loss: 0.010124\n",
      "[456/00843] train_loss: 0.011410\n",
      "[456/00893] train_loss: 0.011089\n",
      "[456/00943] train_loss: 0.011413\n",
      "[456/00993] train_loss: 0.011451\n",
      "[456/01043] train_loss: 0.011124\n",
      "[456/01093] train_loss: 0.011467\n",
      "[456/01143] train_loss: 0.011125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[456/01193] train_loss: 0.011810\n",
      "[457/00017] train_loss: 0.010930\n",
      "[457/00067] train_loss: 0.012185\n",
      "[457/00117] train_loss: 0.011229\n",
      "[457/00167] train_loss: 0.011413\n",
      "[457/00217] train_loss: 0.011111\n",
      "[457/00267] train_loss: 0.012152\n",
      "[457/00317] train_loss: 0.011583\n",
      "[457/00367] train_loss: 0.011407\n",
      "[457/00417] train_loss: 0.011641\n",
      "[457/00467] train_loss: 0.010993\n",
      "[457/00517] train_loss: 0.011098\n",
      "[457/00567] train_loss: 0.010707\n",
      "[457/00617] train_loss: 0.011469\n",
      "[457/00667] train_loss: 0.010483\n",
      "[457/00717] train_loss: 0.010748\n",
      "[457/00767] train_loss: 0.011347\n",
      "[457/00817] train_loss: 0.011360\n",
      "[457/00867] train_loss: 0.010904\n",
      "[457/00917] train_loss: 0.011449\n",
      "[457/00967] train_loss: 0.011451\n",
      "[457/01017] train_loss: 0.011317\n",
      "[457/01067] train_loss: 0.011306\n",
      "[457/01117] train_loss: 0.010789\n",
      "[457/01167] train_loss: 0.010905\n",
      "[457/01217] train_loss: 0.011064\n",
      "[458/00041] train_loss: 0.011853\n",
      "[458/00091] train_loss: 0.011725\n",
      "[458/00141] train_loss: 0.011050\n",
      "[458/00191] train_loss: 0.011335\n",
      "[458/00241] train_loss: 0.011186\n",
      "[458/00291] train_loss: 0.011014\n",
      "[458/00341] train_loss: 0.010800\n",
      "[458/00391] train_loss: 0.010700\n",
      "[458/00441] train_loss: 0.011774\n",
      "[458/00491] train_loss: 0.010862\n",
      "[458/00541] train_loss: 0.011192\n",
      "[458/00591] train_loss: 0.010507\n",
      "[458/00641] train_loss: 0.010855\n",
      "[458/00691] train_loss: 0.010345\n",
      "[458/00741] train_loss: 0.010390\n",
      "[458/00791] train_loss: 0.010836\n",
      "[458/00841] train_loss: 0.011259\n",
      "[458/00891] train_loss: 0.011974\n",
      "[458/00941] train_loss: 0.011122\n",
      "[458/00991] train_loss: 0.010983\n",
      "[458/01041] train_loss: 0.011929\n",
      "[458/01091] train_loss: 0.011395\n",
      "[458/01141] train_loss: 0.010927\n",
      "[458/01191] train_loss: 0.011458\n",
      "[459/00015] train_loss: 0.011155\n",
      "[459/00065] train_loss: 0.011043\n",
      "[459/00115] train_loss: 0.011925\n",
      "[459/00165] train_loss: 0.012215\n",
      "[459/00215] train_loss: 0.010908\n",
      "[459/00265] train_loss: 0.010681\n",
      "[459/00315] train_loss: 0.010899\n",
      "[459/00365] train_loss: 0.011132\n",
      "[459/00415] train_loss: 0.011080\n",
      "[459/00465] train_loss: 0.011180\n",
      "[459/00515] train_loss: 0.011058\n",
      "[459/00565] train_loss: 0.011167\n",
      "[459/00615] train_loss: 0.011073\n",
      "[459/00665] train_loss: 0.010733\n",
      "[459/00715] train_loss: 0.010914\n",
      "[459/00765] train_loss: 0.011534\n",
      "[459/00815] train_loss: 0.011544\n",
      "[459/00865] train_loss: 0.011210\n",
      "[459/00915] train_loss: 0.011087\n",
      "[459/00965] train_loss: 0.011158\n",
      "[459/01015] train_loss: 0.011462\n",
      "[459/01065] train_loss: 0.011428\n",
      "[459/01115] train_loss: 0.011512\n",
      "[459/01165] train_loss: 0.011107\n",
      "[459/01215] train_loss: 0.010784\n",
      "[460/00039] train_loss: 0.011533\n",
      "[460/00089] train_loss: 0.010638\n",
      "[460/00139] train_loss: 0.010976\n",
      "[460/00189] train_loss: 0.011155\n",
      "[460/00239] train_loss: 0.011372\n",
      "[460/00289] train_loss: 0.011112\n",
      "[460/00339] train_loss: 0.010935\n",
      "[460/00389] train_loss: 0.011535\n",
      "[460/00439] train_loss: 0.011652\n",
      "[460/00489] train_loss: 0.011417\n",
      "[460/00539] train_loss: 0.011000\n",
      "[460/00589] train_loss: 0.010625\n",
      "[460/00639] train_loss: 0.011505\n",
      "[460/00689] train_loss: 0.011936\n",
      "[460/00739] train_loss: 0.010371\n",
      "[460/00789] train_loss: 0.010589\n",
      "[460/00839] train_loss: 0.011004\n",
      "[460/00889] train_loss: 0.011165\n",
      "[460/00939] train_loss: 0.011007\n",
      "[460/00989] train_loss: 0.011279\n",
      "[460/01039] train_loss: 0.010582\n",
      "[460/01089] train_loss: 0.011764\n",
      "[460/01139] train_loss: 0.010277\n",
      "[460/01189] train_loss: 0.011351\n",
      "[461/00013] train_loss: 0.010485\n",
      "[461/00063] train_loss: 0.011588\n",
      "[461/00113] train_loss: 0.011609\n",
      "[461/00163] train_loss: 0.011719\n",
      "[461/00213] train_loss: 0.011520\n",
      "[461/00263] train_loss: 0.011203\n",
      "[461/00313] train_loss: 0.011416\n",
      "[461/00363] train_loss: 0.011662\n",
      "[461/00413] train_loss: 0.011065\n",
      "[461/00463] train_loss: 0.011641\n",
      "[461/00513] train_loss: 0.011156\n",
      "[461/00563] train_loss: 0.011103\n",
      "[461/00613] train_loss: 0.011293\n",
      "[461/00663] train_loss: 0.011001\n",
      "[461/00713] train_loss: 0.010742\n",
      "[461/00763] train_loss: 0.010719\n",
      "[461/00813] train_loss: 0.011228\n",
      "[461/00863] train_loss: 0.010878\n",
      "[461/00913] train_loss: 0.011666\n",
      "[461/00963] train_loss: 0.011306\n",
      "[461/01013] train_loss: 0.011342\n",
      "[461/01063] train_loss: 0.011215\n",
      "[461/01113] train_loss: 0.010478\n",
      "[461/01163] train_loss: 0.010573\n",
      "[461/01213] train_loss: 0.011678\n",
      "[462/00037] train_loss: 0.011726\n",
      "[462/00087] train_loss: 0.010531\n",
      "[462/00137] train_loss: 0.011023\n",
      "[462/00187] train_loss: 0.010859\n",
      "[462/00237] train_loss: 0.011581\n",
      "[462/00287] train_loss: 0.010794\n",
      "[462/00337] train_loss: 0.010396\n",
      "[462/00387] train_loss: 0.011161\n",
      "[462/00437] train_loss: 0.011179\n",
      "[462/00487] train_loss: 0.010858\n",
      "[462/00537] train_loss: 0.011746\n",
      "[462/00587] train_loss: 0.011267\n",
      "[462/00637] train_loss: 0.011573\n",
      "[462/00687] train_loss: 0.011055\n",
      "[462/00737] train_loss: 0.011091\n",
      "[462/00787] train_loss: 0.010787\n",
      "[462/00837] train_loss: 0.011385\n",
      "[462/00887] train_loss: 0.011637\n",
      "[462/00937] train_loss: 0.011282\n",
      "[462/00987] train_loss: 0.010701\n",
      "[462/01037] train_loss: 0.011065\n",
      "[462/01087] train_loss: 0.010865\n",
      "[462/01137] train_loss: 0.011015\n",
      "[462/01187] train_loss: 0.011593\n",
      "[463/00011] train_loss: 0.010840\n",
      "[463/00061] train_loss: 0.011582\n",
      "[463/00111] train_loss: 0.012010\n",
      "[463/00161] train_loss: 0.011322\n",
      "[463/00211] train_loss: 0.012140\n",
      "[463/00261] train_loss: 0.011030\n",
      "[463/00311] train_loss: 0.011387\n",
      "[463/00361] train_loss: 0.011162\n",
      "[463/00411] train_loss: 0.010955\n",
      "[463/00461] train_loss: 0.011046\n",
      "[463/00511] train_loss: 0.011324\n",
      "[463/00561] train_loss: 0.011815\n",
      "[463/00611] train_loss: 0.010887\n",
      "[463/00661] train_loss: 0.010533\n",
      "[463/00711] train_loss: 0.010631\n",
      "[463/00761] train_loss: 0.010937\n",
      "[463/00811] train_loss: 0.010987\n",
      "[463/00861] train_loss: 0.011398\n",
      "[463/00911] train_loss: 0.011338\n",
      "[463/00961] train_loss: 0.011292\n",
      "[463/01011] train_loss: 0.010958\n",
      "[463/01061] train_loss: 0.010582\n",
      "[463/01111] train_loss: 0.012010\n",
      "[463/01161] train_loss: 0.011517\n",
      "[463/01211] train_loss: 0.011123\n",
      "[464/00035] train_loss: 0.011353\n",
      "[464/00085] train_loss: 0.011269\n",
      "[464/00135] train_loss: 0.011681\n",
      "[464/00185] train_loss: 0.010804\n",
      "[464/00235] train_loss: 0.011484\n",
      "[464/00285] train_loss: 0.011764\n",
      "[464/00335] train_loss: 0.011362\n",
      "[464/00385] train_loss: 0.010232\n",
      "[464/00435] train_loss: 0.010874\n",
      "[464/00485] train_loss: 0.010649\n",
      "[464/00535] train_loss: 0.011211\n",
      "[464/00585] train_loss: 0.011817\n",
      "[464/00635] train_loss: 0.011191\n",
      "[464/00685] train_loss: 0.011227\n",
      "[464/00735] train_loss: 0.011285\n",
      "[464/00785] train_loss: 0.010854\n",
      "[464/00835] train_loss: 0.011126\n",
      "[464/00885] train_loss: 0.010584\n",
      "[464/00935] train_loss: 0.010790\n",
      "[464/00985] train_loss: 0.010520\n",
      "[464/01035] train_loss: 0.011138\n",
      "[464/01085] train_loss: 0.010987\n",
      "[464/01135] train_loss: 0.010860\n",
      "[464/01185] train_loss: 0.011533\n",
      "[465/00009] train_loss: 0.011271\n",
      "[465/00059] train_loss: 0.011483\n",
      "[465/00109] train_loss: 0.011135\n",
      "[465/00159] train_loss: 0.011691\n",
      "[465/00209] train_loss: 0.011768\n",
      "[465/00259] train_loss: 0.010981\n",
      "[465/00309] train_loss: 0.011519\n",
      "[465/00359] train_loss: 0.011112\n",
      "[465/00409] train_loss: 0.011476\n",
      "[465/00459] train_loss: 0.011570\n",
      "[465/00509] train_loss: 0.011658\n",
      "[465/00559] train_loss: 0.011257\n",
      "[465/00609] train_loss: 0.011740\n",
      "[465/00659] train_loss: 0.010728\n",
      "[465/00709] train_loss: 0.011289\n",
      "[465/00759] train_loss: 0.010866\n",
      "[465/00809] train_loss: 0.011164\n",
      "[465/00859] train_loss: 0.010685\n",
      "[465/00909] train_loss: 0.011058\n",
      "[465/00959] train_loss: 0.010335\n",
      "[465/01009] train_loss: 0.010538\n",
      "[465/01059] train_loss: 0.011547\n",
      "[465/01109] train_loss: 0.010710\n",
      "[465/01159] train_loss: 0.012040\n",
      "[465/01209] train_loss: 0.010827\n",
      "[466/00033] train_loss: 0.011362\n",
      "[466/00083] train_loss: 0.010947\n",
      "[466/00133] train_loss: 0.011082\n",
      "[466/00183] train_loss: 0.011343\n",
      "[466/00233] train_loss: 0.011143\n",
      "[466/00283] train_loss: 0.010895\n",
      "[466/00333] train_loss: 0.011094\n",
      "[466/00383] train_loss: 0.011761\n",
      "[466/00433] train_loss: 0.011356\n",
      "[466/00483] train_loss: 0.010633\n",
      "[466/00533] train_loss: 0.011473\n",
      "[466/00583] train_loss: 0.011755\n",
      "[466/00633] train_loss: 0.011717\n",
      "[466/00683] train_loss: 0.011017\n",
      "[466/00733] train_loss: 0.010968\n",
      "[466/00783] train_loss: 0.011030\n",
      "[466/00833] train_loss: 0.011182\n",
      "[466/00883] train_loss: 0.010616\n",
      "[466/00933] train_loss: 0.010507\n",
      "[466/00983] train_loss: 0.010547\n",
      "[466/01033] train_loss: 0.010842\n",
      "[466/01083] train_loss: 0.010964\n",
      "[466/01133] train_loss: 0.011683\n",
      "[466/01183] train_loss: 0.010703\n",
      "[467/00007] train_loss: 0.011785\n",
      "[467/00057] train_loss: 0.011844\n",
      "[467/00107] train_loss: 0.010893\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[467/00157] train_loss: 0.011277\n",
      "[467/00207] train_loss: 0.012138\n",
      "[467/00257] train_loss: 0.011239\n",
      "[467/00307] train_loss: 0.011117\n",
      "[467/00357] train_loss: 0.012635\n",
      "[467/00407] train_loss: 0.011667\n",
      "[467/00457] train_loss: 0.011450\n",
      "[467/00507] train_loss: 0.011120\n",
      "[467/00557] train_loss: 0.011593\n",
      "[467/00607] train_loss: 0.010987\n",
      "[467/00657] train_loss: 0.011702\n",
      "[467/00707] train_loss: 0.010374\n",
      "[467/00757] train_loss: 0.010580\n",
      "[467/00807] train_loss: 0.010926\n",
      "[467/00857] train_loss: 0.010682\n",
      "[467/00907] train_loss: 0.010958\n",
      "[467/00957] train_loss: 0.010412\n",
      "[467/01007] train_loss: 0.011180\n",
      "[467/01057] train_loss: 0.010913\n",
      "[467/01107] train_loss: 0.010939\n",
      "[467/01157] train_loss: 0.011800\n",
      "[467/01207] train_loss: 0.011242\n",
      "[468/00031] train_loss: 0.011746\n",
      "[468/00081] train_loss: 0.010688\n",
      "[468/00131] train_loss: 0.011151\n",
      "[468/00181] train_loss: 0.011494\n",
      "[468/00231] train_loss: 0.011580\n",
      "[468/00281] train_loss: 0.011635\n",
      "[468/00331] train_loss: 0.010693\n",
      "[468/00381] train_loss: 0.010996\n",
      "[468/00431] train_loss: 0.010603\n",
      "[468/00481] train_loss: 0.010737\n",
      "[468/00531] train_loss: 0.010509\n",
      "[468/00581] train_loss: 0.010719\n",
      "[468/00631] train_loss: 0.011329\n",
      "[468/00681] train_loss: 0.011079\n",
      "[468/00731] train_loss: 0.010720\n",
      "[468/00781] train_loss: 0.010724\n",
      "[468/00831] train_loss: 0.011477\n",
      "[468/00881] train_loss: 0.011536\n",
      "[468/00931] train_loss: 0.010376\n",
      "[468/00981] train_loss: 0.011043\n",
      "[468/01031] train_loss: 0.010791\n",
      "[468/01081] train_loss: 0.011384\n",
      "[468/01131] train_loss: 0.011121\n",
      "[468/01181] train_loss: 0.012002\n",
      "[469/00005] train_loss: 0.010977\n",
      "[469/00055] train_loss: 0.011666\n",
      "[469/00105] train_loss: 0.012522\n",
      "[469/00155] train_loss: 0.011433\n",
      "[469/00205] train_loss: 0.011559\n",
      "[469/00255] train_loss: 0.010896\n",
      "[469/00305] train_loss: 0.011273\n",
      "[469/00355] train_loss: 0.011175\n",
      "[469/00405] train_loss: 0.010591\n",
      "[469/00455] train_loss: 0.011087\n",
      "[469/00505] train_loss: 0.010599\n",
      "[469/00555] train_loss: 0.010597\n",
      "[469/00605] train_loss: 0.011166\n",
      "[469/00655] train_loss: 0.010799\n",
      "[469/00705] train_loss: 0.010935\n",
      "[469/00755] train_loss: 0.011478\n",
      "[469/00805] train_loss: 0.010242\n",
      "[469/00855] train_loss: 0.010725\n",
      "[469/00905] train_loss: 0.011183\n",
      "[469/00955] train_loss: 0.011719\n",
      "[469/01005] train_loss: 0.011133\n",
      "[469/01055] train_loss: 0.011765\n",
      "[469/01105] train_loss: 0.011256\n",
      "[469/01155] train_loss: 0.011143\n",
      "[469/01205] train_loss: 0.011210\n",
      "[470/00029] train_loss: 0.011758\n",
      "[470/00079] train_loss: 0.011999\n",
      "[470/00129] train_loss: 0.011000\n",
      "[470/00179] train_loss: 0.010493\n",
      "[470/00229] train_loss: 0.011360\n",
      "[470/00279] train_loss: 0.011304\n",
      "[470/00329] train_loss: 0.011288\n",
      "[470/00379] train_loss: 0.010563\n",
      "[470/00429] train_loss: 0.011064\n",
      "[470/00479] train_loss: 0.010879\n",
      "[470/00529] train_loss: 0.010829\n",
      "[470/00579] train_loss: 0.011105\n",
      "[470/00629] train_loss: 0.011331\n",
      "[470/00679] train_loss: 0.011185\n",
      "[470/00729] train_loss: 0.011578\n",
      "[470/00779] train_loss: 0.011522\n",
      "[470/00829] train_loss: 0.010731\n",
      "[470/00879] train_loss: 0.010392\n",
      "[470/00929] train_loss: 0.010925\n",
      "[470/00979] train_loss: 0.010871\n",
      "[470/01029] train_loss: 0.010387\n",
      "[470/01079] train_loss: 0.010754\n",
      "[470/01129] train_loss: 0.011808\n",
      "[470/01179] train_loss: 0.010483\n",
      "[471/00003] train_loss: 0.010680\n",
      "[471/00053] train_loss: 0.010917\n",
      "[471/00103] train_loss: 0.011854\n",
      "[471/00153] train_loss: 0.011246\n",
      "[471/00203] train_loss: 0.011091\n",
      "[471/00253] train_loss: 0.010941\n",
      "[471/00303] train_loss: 0.011292\n",
      "[471/00353] train_loss: 0.011184\n",
      "[471/00403] train_loss: 0.010855\n",
      "[471/00453] train_loss: 0.011749\n",
      "[471/00503] train_loss: 0.010814\n",
      "[471/00553] train_loss: 0.012007\n",
      "[471/00603] train_loss: 0.011334\n",
      "[471/00653] train_loss: 0.011312\n",
      "[471/00703] train_loss: 0.011721\n",
      "[471/00753] train_loss: 0.011375\n",
      "[471/00803] train_loss: 0.011352\n",
      "[471/00853] train_loss: 0.011954\n",
      "[471/00903] train_loss: 0.010832\n",
      "[471/00953] train_loss: 0.011095\n",
      "[471/01003] train_loss: 0.011091\n",
      "[471/01053] train_loss: 0.010900\n",
      "[471/01103] train_loss: 0.011307\n",
      "[471/01153] train_loss: 0.010664\n",
      "[471/01203] train_loss: 0.010032\n",
      "[472/00027] train_loss: 0.011721\n",
      "[472/00077] train_loss: 0.010739\n",
      "[472/00127] train_loss: 0.011260\n",
      "[472/00177] train_loss: 0.011022\n",
      "[472/00227] train_loss: 0.011635\n",
      "[472/00277] train_loss: 0.010962\n",
      "[472/00327] train_loss: 0.011324\n",
      "[472/00377] train_loss: 0.010896\n",
      "[472/00427] train_loss: 0.011283\n",
      "[472/00477] train_loss: 0.011163\n",
      "[472/00527] train_loss: 0.011309\n",
      "[472/00577] train_loss: 0.010818\n",
      "[472/00627] train_loss: 0.010526\n",
      "[472/00677] train_loss: 0.010867\n",
      "[472/00727] train_loss: 0.011046\n",
      "[472/00777] train_loss: 0.010756\n",
      "[472/00827] train_loss: 0.011285\n",
      "[472/00877] train_loss: 0.011437\n",
      "[472/00927] train_loss: 0.010245\n",
      "[472/00977] train_loss: 0.012038\n",
      "[472/01027] train_loss: 0.010300\n",
      "[472/01077] train_loss: 0.011019\n",
      "[472/01127] train_loss: 0.011061\n",
      "[472/01177] train_loss: 0.010841\n",
      "[473/00001] train_loss: 0.010964\n",
      "[473/00051] train_loss: 0.011477\n",
      "[473/00101] train_loss: 0.011918\n",
      "[473/00151] train_loss: 0.011666\n",
      "[473/00201] train_loss: 0.011698\n",
      "[473/00251] train_loss: 0.011273\n",
      "[473/00301] train_loss: 0.011514\n",
      "[473/00351] train_loss: 0.011473\n",
      "[473/00401] train_loss: 0.010830\n",
      "[473/00451] train_loss: 0.011443\n",
      "[473/00501] train_loss: 0.010914\n",
      "[473/00551] train_loss: 0.010971\n",
      "[473/00601] train_loss: 0.010730\n",
      "[473/00651] train_loss: 0.010930\n",
      "[473/00701] train_loss: 0.010933\n",
      "[473/00751] train_loss: 0.011086\n",
      "[473/00801] train_loss: 0.011022\n",
      "[473/00851] train_loss: 0.010943\n",
      "[473/00901] train_loss: 0.011155\n",
      "[473/00951] train_loss: 0.010690\n",
      "[473/01001] train_loss: 0.011050\n",
      "[473/01051] train_loss: 0.011412\n",
      "[473/01101] train_loss: 0.010841\n",
      "[473/01151] train_loss: 0.011369\n",
      "[473/01201] train_loss: 0.010503\n",
      "[474/00025] train_loss: 0.010754\n",
      "[474/00075] train_loss: 0.011467\n",
      "[474/00125] train_loss: 0.011380\n",
      "[474/00175] train_loss: 0.010824\n",
      "[474/00225] train_loss: 0.011288\n",
      "[474/00275] train_loss: 0.011637\n",
      "[474/00325] train_loss: 0.010637\n",
      "[474/00375] train_loss: 0.011061\n",
      "[474/00425] train_loss: 0.011274\n",
      "[474/00475] train_loss: 0.010977\n",
      "[474/00525] train_loss: 0.010806\n",
      "[474/00575] train_loss: 0.010929\n",
      "[474/00625] train_loss: 0.010493\n",
      "[474/00675] train_loss: 0.011419\n",
      "[474/00725] train_loss: 0.011034\n",
      "[474/00775] train_loss: 0.011233\n",
      "[474/00825] train_loss: 0.010454\n",
      "[474/00875] train_loss: 0.010523\n",
      "[474/00925] train_loss: 0.011194\n",
      "[474/00975] train_loss: 0.010516\n",
      "[474/01025] train_loss: 0.010827\n",
      "[474/01075] train_loss: 0.010761\n",
      "[474/01125] train_loss: 0.011349\n",
      "[474/01175] train_loss: 0.011700\n",
      "[474/01225] train_loss: 0.011028\n",
      "[475/00049] train_loss: 0.011772\n",
      "[475/00099] train_loss: 0.011478\n",
      "[475/00149] train_loss: 0.011203\n",
      "[475/00199] train_loss: 0.010830\n",
      "[475/00249] train_loss: 0.011088\n",
      "[475/00299] train_loss: 0.011059\n",
      "[475/00349] train_loss: 0.011344\n",
      "[475/00399] train_loss: 0.011462\n",
      "[475/00449] train_loss: 0.010954\n",
      "[475/00499] train_loss: 0.011292\n",
      "[475/00549] train_loss: 0.010444\n",
      "[475/00599] train_loss: 0.011132\n",
      "[475/00649] train_loss: 0.010944\n",
      "[475/00699] train_loss: 0.011746\n",
      "[475/00749] train_loss: 0.011405\n",
      "[475/00799] train_loss: 0.011387\n",
      "[475/00849] train_loss: 0.010878\n",
      "[475/00899] train_loss: 0.011385\n",
      "[475/00949] train_loss: 0.011039\n",
      "[475/00999] train_loss: 0.011179\n",
      "[475/01049] train_loss: 0.010850\n",
      "[475/01099] train_loss: 0.010873\n",
      "[475/01149] train_loss: 0.010868\n",
      "[475/01199] train_loss: 0.011511\n",
      "[476/00023] train_loss: 0.011670\n",
      "[476/00073] train_loss: 0.011423\n",
      "[476/00123] train_loss: 0.011916\n",
      "[476/00173] train_loss: 0.011122\n",
      "[476/00223] train_loss: 0.011702\n",
      "[476/00273] train_loss: 0.011154\n",
      "[476/00323] train_loss: 0.011104\n",
      "[476/00373] train_loss: 0.010579\n",
      "[476/00423] train_loss: 0.010579\n",
      "[476/00473] train_loss: 0.010960\n",
      "[476/00523] train_loss: 0.010560\n",
      "[476/00573] train_loss: 0.011908\n",
      "[476/00623] train_loss: 0.010986\n",
      "[476/00673] train_loss: 0.011406\n",
      "[476/00723] train_loss: 0.010717\n",
      "[476/00773] train_loss: 0.010396\n",
      "[476/00823] train_loss: 0.011308\n",
      "[476/00873] train_loss: 0.011092\n",
      "[476/00923] train_loss: 0.010909\n",
      "[476/00973] train_loss: 0.011220\n",
      "[476/01023] train_loss: 0.010827\n",
      "[476/01073] train_loss: 0.011211\n",
      "[476/01123] train_loss: 0.010804\n",
      "[476/01173] train_loss: 0.010818\n",
      "[476/01223] train_loss: 0.011135\n",
      "[477/00047] train_loss: 0.012131\n",
      "[477/00097] train_loss: 0.011545\n",
      "[477/00147] train_loss: 0.011463\n",
      "[477/00197] train_loss: 0.011706\n",
      "[477/00247] train_loss: 0.011741\n",
      "[477/00297] train_loss: 0.011499\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[477/00347] train_loss: 0.011418\n",
      "[477/00397] train_loss: 0.011703\n",
      "[477/00447] train_loss: 0.011345\n",
      "[477/00497] train_loss: 0.010932\n",
      "[477/00547] train_loss: 0.011501\n",
      "[477/00597] train_loss: 0.010870\n",
      "[477/00647] train_loss: 0.010883\n",
      "[477/00697] train_loss: 0.011411\n",
      "[477/00747] train_loss: 0.010827\n",
      "[477/00797] train_loss: 0.011238\n",
      "[477/00847] train_loss: 0.010769\n",
      "[477/00897] train_loss: 0.010902\n",
      "[477/00947] train_loss: 0.011110\n",
      "[477/00997] train_loss: 0.011422\n",
      "[477/01047] train_loss: 0.010598\n",
      "[477/01097] train_loss: 0.010753\n",
      "[477/01147] train_loss: 0.010409\n",
      "[477/01197] train_loss: 0.010867\n",
      "[478/00021] train_loss: 0.011188\n",
      "[478/00071] train_loss: 0.010963\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: CUBLAS_STATUS_EXECUTION_FAILED when calling `cublasLtMatmul( ltHandle, computeDesc.descriptor(), &alpha_val, mat1_ptr, Adesc.descriptor(), mat2_ptr, Bdesc.descriptor(), &beta_val, result_ptr, Cdesc.descriptor(), result_ptr, Cdesc.descriptor(), &heuristicResult.algo, workspace.data_ptr(), workspaceSize, at::cuda::getCurrentCUDAStream())`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4414/2712520192.py\u001b[0m in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m }\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mtrain_deepsdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgeneralization_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/tum/geometric-machine-learning/exercises/ex-3/exercise_3/training/train_deepsdf.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;31m# Start training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatent_vectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/tum/geometric-machine-learning/exercises/ex-3/exercise_3/training/train_deepsdf.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, latent_vectors, train_dataloader, device, config)\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0;31m# TODO: perform forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_latent_vectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpoints\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             \u001b[0mpredicted_sdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# TODO: truncate predicted sdf between -0.1 and 0.1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/machine_learning_for_3d_geometry-_2t5S_p_-py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/tum/geometric-machine-learning/exercises/ex-3/exercise_3/model/deepsdf.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x_in)\u001b[0m\n\u001b[1;32m     32\u001b[0m         \"\"\"\n\u001b[1;32m     33\u001b[0m         \u001b[0;31m# TODO: implement forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwnl1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwnl2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwnl3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/machine_learning_for_3d_geometry-_2t5S_p_-py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1206\u001b[0m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbw_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_input_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1208\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_global_forward_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1210\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/machine_learning_for_3d_geometry-_2t5S_p_-py3.10/lib/python3.10/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: CUBLAS_STATUS_EXECUTION_FAILED when calling `cublasLtMatmul( ltHandle, computeDesc.descriptor(), &alpha_val, mat1_ptr, Adesc.descriptor(), mat2_ptr, Bdesc.descriptor(), &beta_val, result_ptr, Cdesc.descriptor(), result_ptr, Cdesc.descriptor(), &heuristicResult.algo, workspace.data_ptr(), workspaceSize, at::cuda::getCurrentCUDAStream())`"
     ]
    }
   ],
   "source": [
    "from exercise_3.training import train_deepsdf\n",
    "\n",
    "generalization_config = {\n",
    "    'experiment_name': '3_2_deepsdf_generalization',\n",
    "    'device': 'cuda:0',  # run this on a gpu for a reasonable training time\n",
    "    'is_overfit': False,\n",
    "    'num_sample_points': 4096, # you can adjust this such that the model fits on your gpu\n",
    "    'latent_code_length': 256,\n",
    "    'batch_size': 1,\n",
    "    'resume_ckpt': None,\n",
    "    'learning_rate_model': 0.0005,\n",
    "    'learning_rate_code': 0.001,\n",
    "    'lambda_code_regularization': 0.0001,\n",
    "    'max_epochs': 500,  # not necessary to run for 2000 epochs if you're short on time, at 500 epochs you should start to see reasonable results\n",
    "    'print_every_n': 50,\n",
    "    'visualize_every_n': 5000,\n",
    "}\n",
    "\n",
    "train_deepsdf.main(generalization_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (f) Inference using the trained model on observed SDF values\n",
    "\n",
    "Fill in the inference script `exercise_3/inference/infer_deepsdf.py`. Note that it's not simply a forward pass, but an optimization of the latent code such that we have lowest error on observed SDF values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from exercise_3.inference.infer_deepsdf import InferenceHandlerDeepSDF\n",
    "\n",
    "device = torch.device('cuda:0')  # change this to cpu if you're not using a gpu\n",
    "\n",
    "inference_handler = InferenceHandlerDeepSDF(256, \"exercise_3/runs/3_2_deepsdf_generalization\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we try inference on a shape from validation set, for which we have a complete observation of sdf values. This is an easier problem as compared to shape completion,\n",
    "since we have all the information already in the input.\n",
    "\n",
    "Let's visualize the observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observations with negative SDF (inside)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55110b56d95d4313a66c1baed2c67257",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observations with positive SDF (outside)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c62183eedb0943029fa64d572c10bc7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get observed data\n",
    "points, sdf = ShapeImplicit.get_all_sdf_samples(\"b351e06f5826444c19fb4103277a6b93\")\n",
    "\n",
    "inside_points = points[sdf[:, 0] < 0, :].numpy()\n",
    "outside_points = points[sdf[:, 0] > 0, :].numpy()\n",
    "\n",
    "# visualize observed points; you'll observe that the observations are very complete\n",
    "print('Observations with negative SDF (inside)')\n",
    "visualize_pointcloud(inside_points, 0.025, flip_axes=True)\n",
    "print('Observations with positive SDF (outside)')\n",
    "visualize_pointcloud(outside_points, 0.025, flip_axes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Reconstruction on these observations with the trained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00000] optim_loss: 0.034338\n",
      "[00050] optim_loss: 0.028915\n",
      "[00100] optim_loss: 0.028603\n",
      "[00150] optim_loss: 0.029490\n",
      "[00200] optim_loss: 0.029218\n",
      "[00250] optim_loss: 0.027984\n",
      "[00300] optim_loss: 0.028638\n",
      "[00350] optim_loss: 0.029182\n",
      "[00400] optim_loss: 0.028715\n",
      "[00450] optim_loss: 0.029203\n",
      "[00500] optim_loss: 0.028290\n",
      "[00550] optim_loss: 0.028844\n",
      "[00600] optim_loss: 0.028844\n",
      "[00650] optim_loss: 0.028291\n",
      "[00700] optim_loss: 0.029019\n",
      "[00750] optim_loss: 0.028621\n",
      "Optimization complete.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0a1ed0e2e4448bba0f2b2be76cf4fb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# reconstruct\n",
    "vertices, faces = inference_handler.reconstruct(points, sdf, 800)\n",
    "# visualize\n",
    "visualize_mesh(vertices, faces, flip_axes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can try the shape completion task, i.e., inference on a shape from validation set, for which we do not have a complete observation of sdf values. The observed points are visualized below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observations with negative SDF (inside)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7793645d1f6f4645adfa7bf3bfb9a387",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observations with positive SDF (outside)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9366cfeed28740efbeb36b3eef9010cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get observed data\n",
    "points, sdf = ShapeImplicit.get_all_sdf_samples(\"b351e06f5826444c19fb4103277a6b93_incomplete\")\n",
    "\n",
    "inside_points = points[sdf[:, 0] < 0, :].numpy()\n",
    "outside_points = points[sdf[:, 0] > 0, :].numpy()\n",
    "\n",
    "# visualize observed points; you'll observe that the observations are incomplete\n",
    "# making this is a shape completion task\n",
    "print('Observations with negative SDF (inside)')\n",
    "visualize_pointcloud(inside_points, 0.025, flip_axes=True)\n",
    "print('Observations with positive SDF (outside)')\n",
    "visualize_pointcloud(outside_points, 0.025, flip_axes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shape completion using the trained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00000] optim_loss: 0.019164\n",
      "[00050] optim_loss: 0.013110\n",
      "[00100] optim_loss: 0.012693\n",
      "[00150] optim_loss: 0.012431\n",
      "[00200] optim_loss: 0.013025\n",
      "[00250] optim_loss: 0.012496\n",
      "[00300] optim_loss: 0.011881\n",
      "[00350] optim_loss: 0.011923\n",
      "[00400] optim_loss: 0.011902\n",
      "[00450] optim_loss: 0.011821\n",
      "[00500] optim_loss: 0.011983\n",
      "[00550] optim_loss: 0.011991\n",
      "[00600] optim_loss: 0.011815\n",
      "[00650] optim_loss: 0.011830\n",
      "[00700] optim_loss: 0.011621\n",
      "[00750] optim_loss: 0.011960\n",
      "Optimization complete.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96cfe46b50fb4bf0939fae0c3569c2b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# reconstruct\n",
    "vertices, faces = inference_handler.reconstruct(points, sdf, 800)\n",
    "# visualize\n",
    "visualize_mesh(vertices, faces, flip_axes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (g) Latent space interpolation\n",
    "\n",
    "The latent space learned by DeepSDF is interpolatable, meaning that decoding latent codes from this space produced meaningful shapes. Given two latent codes, a linearly interpolatable latent space will decode\n",
    "each of the intermediate codes to some valid shape. Let's see if this holds for our trained model.\n",
    "\n",
    "We'll pick two shapes from the train set as visualized below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GT Shape A\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2a12879d0d44140a91c0ceb850755af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GT Shape B\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e2ba07bfdb94e37ae5e019271bae2dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from exercise_3.data.shape_implicit import ShapeImplicit\n",
    "from exercise_3.util.visualization import visualize_mesh\n",
    "\n",
    "mesh = ShapeImplicit.get_mesh(\"494fe53da65650b8c358765b76c296\")\n",
    "print('GT Shape A')\n",
    "visualize_mesh(mesh.vertices, mesh.faces, flip_axes=True)\n",
    "\n",
    "mesh = ShapeImplicit.get_mesh(\"5ca1ef55ff5f68501921e7a85cf9da35\")\n",
    "print('GT Shape B')\n",
    "visualize_mesh(mesh.vertices, mesh.faces, flip_axes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Implement the missing parts in `exercise_3/inference/infer_deepsdf.py` such that it interpolates two given latent vectors, and run the code fragement below once done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from exercise_3.inference.infer_deepsdf import InferenceHandlerDeepSDF\n",
    "\n",
    "inference_handler = InferenceHandlerDeepSDF(256, \"exercise_3/runs/3_2_deepsdf_generalization\", torch.device('cuda:0'))\n",
    "# interpolate; also exports interpolated meshes to disk\n",
    "inference_handler.interpolate('494fe53da65650b8c358765b76c296', '5ca1ef55ff5f68501921e7a85cf9da35', 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the interpolation below. If everything works out correctly, you should see a smooth transformation between the shapes, with all intermediate shapes being valid sofas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "visualizing:   0%|                                                                         | 0/122 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualization failed The _imagingft C module is not installed\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'exercise_3/runs/3_2_deepsdf_generalization/latent_interp.gif'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_6861/1256056307.py\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# create a visualization of the interpolation process\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mmeshes_to_gif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmesh_paths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"exercise_3/runs/3_2_deepsdf_generalization/latent_interp.gif\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mshow_gif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"exercise_3/runs/3_2_deepsdf_generalization/latent_interp.gif\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/tum/geometric-machine-learning/exercises/ex-3/exercise_3/util/misc.py\u001b[0m in \u001b[0;36mshow_gif\u001b[0;34m(fname)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mbase64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0mb64\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase64\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb64encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ascii'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHTML\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'<img src=\"data:image/gif;base64,{b64}\" />'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'exercise_3/runs/3_2_deepsdf_generalization/latent_interp.gif'"
     ]
    }
   ],
   "source": [
    "from exercise_3.util.mesh_collection_to_gif import  meshes_to_gif\n",
    "from exercise_3.util.misc import show_gif\n",
    "\n",
    "# create list of meshes (just exported) to be visualized\n",
    "mesh_paths = sorted([x for x in Path(\"exercise_3/runs/3_2_deepsdf_generalization/interpolation\").iterdir() if int(x.name.split('.')[0].split(\"_\")[1]) == 0], key=lambda x: int(x.name.split('.')[0].split(\"_\")[0]))\n",
    "mesh_paths = mesh_paths + mesh_paths[::-1]\n",
    "\n",
    "# create a visualization of the interpolation process\n",
    "meshes_to_gif(mesh_paths, \"exercise_3/runs/3_2_deepsdf_generalization/latent_interp.gif\", 20)\n",
    "# show_gif(\"exercise_3/runs/3_2_deepsdf_generalization/latent_interp.gif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Submission\n",
    "\n",
    "This is the end of exercise 3 🙂. Please create a zip containing all files we provided, everything you modified, your visualization images/gif (no need to submit generated OBJs), including your checkpoints. Name it with your matriculation number(s) as described in exercise 1. Make sure this notebook can be run without problems. Then, submit via Moodle.\n",
    "\n",
    "**Note**: The maximum submission file size limit for Moodle is 100M. You do not need to submit your overfitting checkpoints; however, the generalization checkpoint will be >200M. The easiest way to still be able to submit that one is to split it with zip like this: `zip -s 100M model_best.ckpt.zip model_best.ckpt` which creates a `.zip` and a `.z01`. You can then submit both files alongside another zip containing all your code and outputs.\n",
    "\n",
    "**Submission Deadline**: 07.12.2022, 23:55"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## References\n",
    "\n",
    "[1] Dai, Angela, Charles Ruizhongtai Qi, and Matthias Nießner. \"Shape completion using 3d-encoder-predictor cnns and shape synthesis.\" Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2017.\n",
    "\n",
    "[2] Park, Jeong Joon, et al. \"Deepsdf: Learning continuous signed distance functions for shape representation.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2019."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
